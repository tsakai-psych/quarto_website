[
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  psych,\n  microbenchmark\n)"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#demo-data",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#demo-data",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "demo data",
    "text": "demo data\npsych::bfiがちょうどいいので、逆転処理などをしておいてデモデータにします。\n\n\nCode\ndf_bfi &lt;- \n  psych::bfi |&gt;\n  relocate(\n    gender, education, age\n  ) |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  mutate(\n    across(\n      .cols = all_of(\n        unlist(bfi.keys) |&gt;\n          str_subset(pattern = \"^-\") |&gt; # extract reverse item\n          str_remove(pattern = \"^-\")\n      ),\n      .fns = \\(x) {7 - x} # six point scale, so subtract from seven.\n    )\n  ) |&gt;\n1  rename(\n    q_education = education,\n    q_age = age,\n    q_gender = gender\n  ) |&gt;\n  as_tibble() # for better printing\n\n\n\n1\n\nわざわざ列名を変えなくてもいいのですが、tidyselect::starts_with()でいちいちignore.case = TRUEとしないとageとA*、educationとE*の区別をつけてもらえなくてめんどくさいので、変えておきます。\n\n\n\n\n\ndf_bfi\n\n# A tibble: 2,800 × 29\n   id    q_gender q_education q_age    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 61617        1          NA    16     5     4     3     4     4     2     3     3     3     3     4     4\n 2 61618        2          NA    18     5     4     5     2     5     5     4     4     4     3     6     6\n 3 61620        2          NA    17     2     4     5     4     4     4     5     4     5     2     5     3\n 4 61621        2          NA    17     3     4     6     5     5     4     4     3     2     2     2     4\n 5 61622        1          NA    17     5     3     3     4     5     4     4     5     4     5     5     5\n 6 61623        2           3    21     1     6     5     6     5     6     6     6     6     4     5     6\n 7 61624        1          NA    18     5     5     5     3     5     5     4     4     5     4     3     4\n 8 61629        1           2    19     3     3     1     5     1     3     2     4     5     3     4     1\n 9 61630        1           1    19     3     3     6     3     3     6     6     3     3     2     2     4\n10 61633        2          NA    17     5     5     6     6     5     6     5     6     5     6     5     5\n# ℹ 2,790 more rows\n# ℹ 13 more variables: E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;,\n#   O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#rowwise",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#rowwise",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "rowwise()",
    "text": "rowwise()\n心理学のデータ処理で因子分析とα係数の確認を終えたら、次に行うのは尺度得点の計算だと思います。参加者ごとに下位尺度の得点を算出していくのですが、その場合は調べてみるとたいていはdplyr::rowise()1とdplyr::c_across()2を使った処理に行き着くと思います。つまり、rowwise()で行ごとにグルーピングして、c_acrossでまとめたい要素をまとめて、それで処理するという方法です。\n\ndf_bfi |&gt;\n  rowwise() |&gt;\n  mutate(\n    score_A = mean(c_across(starts_with(\"A\"))),\n    score_C = mean(c_across(starts_with(\"C\"))),\n    score_E = mean(c_across(starts_with(\"E\"))),\n    score_N = mean(c_across(starts_with(\"N\"))),\n    score_O = mean(c_across(starts_with(\"O\"))),\n    .after = q_age\n  ) |&gt;\n  ungroup()\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれで処理できるのでそれはそれでいいのですが、いくつか気になる点があります。\n\ndplyr::ungroup()が必要\nungroup()を忘れるとrowwiseグループが維持されるので、その後の処理で面倒なことが起こる可能性があります。\n行数によっては遅い\n行数が少なければ気にならないと思いますが、行数が多いデータだとrowwise()処理は体感できるレベルで遅いです。今回の2800行のデータだと4秒程度かかります。\n\n\nmicrobenchmark::microbenchmark(\n  \"rowwise\" = {\n    df_bfi |&gt;\n      rowwise() |&gt;\n      mutate(\n        score_A = mean(c_across(starts_with(\"A\"))),\n        score_C = mean(c_across(starts_with(\"C\"))),\n        score_E = mean(c_across(starts_with(\"E\"))),\n        score_N = mean(c_across(starts_with(\"N\"))),\n        score_O = mean(c_across(starts_with(\"O\"))),\n        .after = q_age\n      ) |&gt;\n      ungroup()\n  },\n1  times = 5L\n)\n\n\n1\n\nレンダリングの際にこの処理だけであまりにも時間がかかるので、ベンチマークの反復はデフォルトの100回から5回に減らしました。\n\n\n\n\nUnit: seconds\n    expr      min       lq     mean   median       uq      max neval\n rowwise 4.379653 4.452662 4.606178 4.665492 4.692358 4.840726     5\n\n\nというわけで、別のやり方がないか模索したわけです。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#by-argument",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#by-argument",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": ".by argument",
    "text": ".by argument\nグルーピングに.by3引数を用いるやり方です。dplyr::group_by()やrowwiseは関数としてパイプフローに組み込んで、処理の後もグルーピングを維持するのに対して.by = .../by = ...は処理の関数の引数で設定し、その処理限りのグルーピングを行います。戻ってくるdataframeはグループ化されていないので、個人的にはその後の処理がやりやすい感じがしてよく使っています。変数選択にはtidy-selectの文法が使えます。\nなお、.by引数に突っ込めるrowwise()ってある？という質問がPosit Communityのforumに投げられているのですが、悲しいことに回答なしでclosedになっています。おそらく現時点ではそのようなものは実装されてないみたいなので、質問者の方が提示しているように.byには一意のID列を設定すればいいのだと思います。 そして、このID列を.byに入れたときは実質rowwise処理になるので、なんとc_across()がちゃんと動きます。\n\ndf_bfi |&gt;\n  mutate(\n      score_A = mean(c_across(starts_with(\"A\"))),\n      score_C = mean(c_across(starts_with(\"C\"))),\n      score_E = mean(c_across(starts_with(\"E\"))),\n      score_N = mean(c_across(starts_with(\"N\"))),\n      score_O = mean(c_across(starts_with(\"O\"))),\n      .after = q_age,\n      .by = id\n    )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\n先ほどのrowwise()の書き方と全く同じ結果が返ってきています。ID列をちゃんと作ってあれば、.by引数にそれを入れることでもできるわけです。 ただし、実はこの方法も時間がかかる処理で、rowwise()のときと同じくらいの処理時間がかかります。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \".by\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = mean(c_across(starts_with(\"A\"))),\n        score_C = mean(c_across(starts_with(\"C\"))),\n        score_E = mean(c_across(starts_with(\"E\"))),\n        score_N = mean(c_across(starts_with(\"N\"))),\n        score_O = mean(c_across(starts_with(\"O\"))),\n        .after = q_age,\n        .by = id\n      )\n  },\n  times = 5L # for saving time\n)\n\n\nUnit: seconds\n expr      min       lq     mean   median       uq     max neval\n  .by 4.452133 4.503107 4.554612 4.581422 4.597017 4.63938     5"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#rowmeanspick...",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#rowmeanspick...",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "rowMeans(pick(...))",
    "text": "rowMeans(pick(...))\n上記の.by引数に突っ込めるrowwise()みたいな関数ってないのかな～と探していたときに、たまたまこんな記事を見つけました。\n\nRow-wise means in dplyr\n\nbase::rowMeans()にdplyr::pick()4で列を選択して入れるという技です。pick()はmutate()やdplyr::summrise()のような関数の中でtidy-selectの文法を使ってdataframe列を選択できる関数です。pick()の戻り値がdataframeであること、rowMeans()は引数にdataframeも入れられること、rowMeans()の戻り値は各行の値の平均値を収めたベクトルであることを利用して、rowwise()を使わずに実質的にrowwise処理をしてしまおうというわけですね。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = rowMeans(pick(starts_with(\"A\"))),\n    score_C = rowMeans(pick(starts_with(\"C\"))),\n    score_E = rowMeans(pick(starts_with(\"E\"))),\n    score_N = rowMeans(pick(starts_with(\"N\"))),\n    score_O = rowMeans(pick(starts_with(\"O\"))),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれもまた今までの書き方と同じ結果が返ってきています。mean()がrowMeansに、c_across()がpick()に変わっただけなので、コードの可読性も悪くない気がします。\nそして処理速度ですが、上記２つに比べてとても速いです。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = rowMeans(pick(starts_with(\"A\"))),\n        score_C = rowMeans(pick(starts_with(\"C\"))),\n        score_E = rowMeans(pick(starts_with(\"E\"))),\n        score_N = rowMeans(pick(starts_with(\"N\"))),\n        score_O = rowMeans(pick(starts_with(\"O\"))),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n           expr    min    lq     mean median    uq    max neval\n base::rowMeans 4.3955 4.775 5.446678 5.0946 5.985 9.3616   100\n\n\n上記2つは単位が秒だったのに、こちらの単位はミリ秒です。つまり、4ミリ秒程度で処理が終わっています。\nちなみに、もし各項目の合計得点が尺度得点である場合は、base::rowSums()を使えばいいです。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#applymargin-1",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#applymargin-1",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "apply(MARGIN = 1)",
    "text": "apply(MARGIN = 1)\nそういえば、行での計算はbase::apply(MARGIN = 1)でもできるのを思い出しました。引数には同じくpick()で選んだ列を入れて、関数にmean()を選択すれば同じ結果が得られるはずです。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = apply(\n      pick(starts_with(\"A\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_C = apply(\n      pick(starts_with(\"C\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_E = apply(\n      pick(starts_with(\"E\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_N = apply(\n      pick(starts_with(\"N\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_O = apply(\n      pick(starts_with(\"O\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれまでの方法と同じ結果が返ってきました。apply()は引数MARGINに入れるのって0or1だっけ1or2だっけ？どっちが行でどっちが列だっけ？となるので、あんまり使ってないです。（覚えればいいんですけど指定は1or2で、1が行に対して、2が列に対しての計算です。）\n処理速度に関してはどうでしょうか。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"apply_margin1\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = apply(\n          pick(starts_with(\"A\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_C = apply(\n          pick(starts_with(\"C\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_E = apply(\n          pick(starts_with(\"E\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_N = apply(\n          pick(starts_with(\"N\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_O = apply(\n          pick(starts_with(\"O\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n          expr     min       lq    mean median      uq      max neval\n apply_margin1 69.1538 72.07505 75.1989 73.648 76.3195 178.5305   100\n\n\nbfiデータだと70ミリ秒くらいで済むみたいです。rowMeans()よりは遅いですが、rowwise()と.byよりは速いみたいですね。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#comparison",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#comparison",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "Comparison",
    "text": "Comparison\nせっかくなので処理速度を一度に比べてみます。ただし、このままbfiデータでベンチマークするととんでもない時間がかかるので、もっと数が少ないirisデータで比較します。\n\ndf_iris &lt;- iris |&gt;\n  as_tibble() |&gt; # for better printing\n  rowid_to_column() # for equal results\n\nhead(df_iris)\n\n# A tibble: 6 × 6\n  rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n  &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1     1          5.1         3.5          1.4         0.2 setosa \n2     2          4.9         3            1.4         0.2 setosa \n3     3          4.7         3.2          1.3         0.2 setosa \n4     4          4.6         3.1          1.5         0.2 setosa \n5     5          5           3.6          1.4         0.2 setosa \n6     6          5.4         3.9          1.7         0.4 setosa \n\n\n\n\nCode\nres &lt;- microbenchmark::microbenchmark(\n  \"rowwise\" = {\n    df_iris |&gt;\n      rowwise() |&gt;\n      mutate(\n        sepal = mean(c_across(starts_with(\"Sepal\"))),\n        petal = mean(c_across(starts_with(\"Petal\")))\n      ) |&gt;\n      ungroup()\n  },\n  \".by\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = mean(c_across(starts_with(\"Sepal\"))),\n        petal = mean(c_across(starts_with(\"Petal\"))),\n        .by = rowid\n      )\n  },\n  \"rowMeans\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = rowMeans(pick(starts_with(\"Sepal\"))),\n        petal = rowMeans(pick(starts_with(\"Petal\")))\n      )\n  },\n  \"apply_margin1\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = apply(\n          pick(starts_with(\"Sepal\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        petal = apply(\n          pick(starts_with(\"Petal\")),\n          MARGIN = 1,\n          FUN = mean\n        )\n      )\n  },\n  check = \"equal\"\n)\n\nres\n\n\nUnit: milliseconds\n          expr     min       lq      mean  median       uq      max neval cld\n       rowwise 83.6199 87.56705 92.048610 90.2374 93.21040 198.1570   100 a  \n           .by 81.9493 85.52320 89.220028 87.6496 90.60810 108.6058   100  b \n      rowMeans  1.4587  1.56730  1.722859  1.6520  1.73630   4.0320   100   c\n apply_margin1  2.8520  2.99130  3.212039  3.0942  3.26395   5.5633   100   c\n\n\n\nautoplot(res)\n\n\n\n\n\n\n\n\n圧倒的にrowMeans()が速いです。桁が違います。元記事の比較データだと、10000行のデータでもrowMeans()を使う方法が圧倒的に速いです。dplyrのarticle（Row-wise operations）にも書いてありましたが、速さを求めるならこっちと言うのもわかります。base関数はやはり侮れません。apply()も行数が少なければ高速っぽいです。\n\n\n\n\n\n\n番外編：Rfast::rowmeans()\n\n\n\n\n\n同じノリでRfast::rowmeans()もイケるか…？と思って試してみました。Rfast5パッケージの関数は基本はmatrixかつNAなしじゃないと使えない（内部で使ってるCppの都合でNAがあると良くないとのこと）のですが、Helpを見る限りrowmeans()はdataframeでも計算してくれそうな感じっぽいので試してみました。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = Rfast::rowmeans(pick(starts_with(\"A\")))\n  )\n\nError in `mutate()`:\nℹ In argument: `score_A = Rfast::rowmeans(pick(starts_with(\"A\")))`.\nCaused by error:\n! Not compatible with requested type: [type=list; target=double].\n\n\nダメでした。ということで、無理やりmatrixにして再挑戦します。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = pick(starts_with(\"A\")) |&gt;\n      as.matrix() |&gt;\n      Rfast::rowmeans(),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 30\n   id    q_gender q_education q_age score_A    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 61617        1          NA    16     4       5     4     3     4     4     2     3     3     3     3     4\n 2 61618        2          NA    18     4.2     5     4     5     2     5     5     4     4     4     3     6\n 3 61620        2          NA    17     3.8     2     4     5     4     4     4     5     4     5     2     5\n 4 61621        2          NA    17     4.6     3     4     6     5     5     4     4     3     2     2     2\n 5 61622        1          NA    17     4       5     3     3     4     5     4     4     5     4     5     5\n 6 61623        2           3    21     4.6     1     6     5     6     5     6     6     6     6     4     5\n 7 61624        1          NA    18     4.6     5     5     5     3     5     5     4     4     5     4     3\n 8 61629        1           2    19     2.6     3     3     1     5     1     3     2     4     5     3     4\n 9 61630        1           1    19     3.6     3     3     6     3     3     6     6     3     3     2     2\n10 61633        2          NA    17     5.4     5     5     6     6     5     6     5     6     5     6     5\n# ℹ 2,790 more rows\n# ℹ 14 more variables: E2 &lt;dbl&gt;, E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;,\n#   N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nできました。でも、わざわざmatrixにしなきゃいけいないのはめんどくさいです。\nちなみに、bfiデータの場合、rowMeans()の方法と処理時間には差がつきません。同じくらい速いです。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = rowMeans(pick(starts_with(\"A\"))),\n        score_C = rowMeans(pick(starts_with(\"C\"))),\n        score_E = rowMeans(pick(starts_with(\"E\"))),\n        score_N = rowMeans(pick(starts_with(\"N\"))),\n        score_O = rowMeans(pick(starts_with(\"O\"))),\n        .after = q_age\n      )\n  },\n  \"Rfast::rowmeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = pick(starts_with(\"A\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_C = pick(starts_with(\"C\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_E = pick(starts_with(\"E\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_N = pick(starts_with(\"N\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_O = pick(starts_with(\"O\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n            expr    min      lq     mean median     uq    max neval cld\n  base::rowMeans 4.2664 4.34500 4.555280 4.4065 4.5439 8.7891   100   a\n Rfast::rowmeans 4.2167 4.28615 4.673185 4.3685 4.5172 8.2862   100   a\n\n\nggplot2::diamondsは約54000行あるんですが、計算させてみるとrowMeans()よりも微妙に速かったりします。ただし1ミリ秒も差がつかないので体感できないです。\n\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    diamonds |&gt;\n      mutate(\n        res = pick(depth:z) |&gt;\n          rowMeans()\n      )\n  },\n  \"Rfast::rowmeans\" = {\n    diamonds |&gt;\n      mutate(\n        res = pick(depth:z) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans()\n      )\n  },\n  check = \"equal\"\n)\n\nUnit: milliseconds\n            expr    min      lq     mean  median      uq    max neval cld\n  base::rowMeans 2.2124 2.37665 2.779288 2.51615 2.70125 5.3981   100  a \n Rfast::rowmeans 1.5674 1.70840 2.082682 1.83450 2.08915 6.0799   100   b\n\n\nものすごくデータ数が多いときなら、選択肢に入る方法かもしれません。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#footnotes",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#footnotes",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://dplyr.tidyverse.org/reference/rowwise.html↩︎\nhttps://dplyr.tidyverse.org/reference/c_across.html↩︎\nhttps://dplyr.tidyverse.org/reference/dplyr_by.html↩︎\nhttps://dplyr.tidyverse.org/reference/pick.html↩︎\nhttps://github.com/RfastOfficial/Rfast?tab=readme-ov-file#readme↩︎"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html",
    "href": "posts/20251025_map_alpha/index.html",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  psych\n)"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすいデータの場合",
    "text": "やりやすいデータの場合\nまず、処理しやすいようにデータをlong型にします。ついでにtibble::rownames_to_column()でrownameをid列に変えておきます。id列がないとtidyr::pivot_wider()出来なくなるので、少なくともその処理の前にはやっておきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n1    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  )\n\n\n1\n\n引数colsはtidy-selectの文法なので-c(id, gender, education, age)でもいいです。longにしたい列名が「文字1文字数字1文字」なのがわかっているので、dplyr::matches()で正規表現を使って絞りました。\n\n\n\n\n# A tibble: 70,000 × 6\n   id    gender education   age items value\n   &lt;chr&gt;  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n 1 61617      1        NA    16 A1        2\n 2 61617      1        NA    16 A2        4\n 3 61617      1        NA    16 A3        3\n 4 61617      1        NA    16 A4        4\n 5 61617      1        NA    16 A5        4\n 6 61617      1        NA    16 C1        2\n 7 61617      1        NA    16 C2        3\n 8 61617      1        NA    16 C3        3\n 9 61617      1        NA    16 C4        4\n10 61617      1        NA    16 C5        4\n# ℹ 69,990 more rows\n\n\n次に、nest用の列を作ってnestします。今回は列名に下位尺度が入っているので、それを抽出すれば簡単にnest用の列が完成です。やりやすいデータとはこのことを言っています。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n1    nest_key = str_extract(items, \"^.\") |&gt;\n2      tolower()\n  ) |&gt;\n3  nest(.by = nest_key)\n\n\n1\n\nitems（列名のベクトル）の各要素から最初の一文字だけ抜ければいいので、stringr::str_extract()でいいです。\n\n2\n\nあとでbfi.keysから抽出しやすくすために、小文字にします。\n\n3\n\nnestに使った列以外の残りは、引数.keysを指定しなければdata列にまとまります。\n\n\n\n\n# A tibble: 5 × 2\n  nest_key data                 \n  &lt;chr&gt;    &lt;list&gt;               \n1 a        &lt;tibble [14,000 × 6]&gt;\n2 c        &lt;tibble [14,000 × 6]&gt;\n3 e        &lt;tibble [14,000 × 6]&gt;\n4 n        &lt;tibble [14,000 × 6]&gt;\n5 o        &lt;tibble [14,000 × 6]&gt;\n\n\nnestされたデータを処理します。data列はlistなので、中の各要素を処理したいときはpurrr::map()が使えます。data列の各要素はデータフレームで、しかもaの行はAで始まる項目だけ、cの行はCで始まる項目だけ、eの行は…のlongデータになっています。そのため、map()の中でwide型に直して必要列以外取り除き、psych::alpha()に入れてあげればいいわけです。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n1    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n2            keys = bfi.keys[[bfi_key_name]] |&gt;\n3              str_subset(pattern = \"^-\") |&gt;\n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n4      set_names(bfi_key_name),\n5    .by = nest_key\n  )\n\n\n1\n\nalpha()の中でbfi.keysの要素名を使えるようにしたいので、ここで抜き出しておきます。\n\n2\n\nまさに上の処理で抜き出した要素名をここで使います。\n\n3\n\n引数keysは、逆転する項目の項目名を文字列ベクトルで入れるか、逆転する項目は-1、そのままの項目は1にした数値ベクトルを入れます。今回は前者で入れるので、bfi.keysの各要素（文字列ベクトル）のうち、stringr::str_subset()を使って-で始まる項目だけを抽出して、str_remove()で-を取り除きます。\n\n4\n\nmap()の戻り値のlistに名前を付けたいのでmap() |&gt; purrr::set_names()とします。nestデータのほかの列の要素を使えるので、dplyr::group_map() |&gt; set_names()よりもいい気がします。\n\n5\n\n引数.byにnest_key列を指定して、実質rowwise処理をします。これを指定することで、(2)の処理で[[bfi_key_name]]のところにbfi_key_name列の要素が1つだけ入ります。これがないと、(2)の処理で要素5の文字列ベクトルc（\"agree\", \"conscientious\", ...）が[[の中に入ってしまうのでエラーになります。\n\n\n\n\n# A tibble: 5 × 4\n  nest_key data                  bfi_key_name  res_alpha   \n  &lt;chr&gt;    &lt;list&gt;                &lt;chr&gt;         &lt;named list&gt;\n1 a        &lt;tibble [14,000 × 6]&gt; agree         &lt;psych&gt;     \n2 c        &lt;tibble [14,000 × 6]&gt; conscientious &lt;psych&gt;     \n3 e        &lt;tibble [14,000 × 6]&gt; extraversion  &lt;psych&gt;     \n4 n        &lt;tibble [14,000 × 6]&gt; neuroticism   &lt;psych&gt;     \n5 o        &lt;tibble [14,000 × 6]&gt; openness      &lt;psych&gt;     \n\n\n最後に結果が詰まったres_alpha列だけ取り出します。データフレームからある1列の要素を取り出すときはdplyr::pull()が使えます。res_alpha列はmap()を使って作ったので、各下位尺度のα係数が入ったlistが返ってきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n            keys = bfi.keys[[bfi_key_name]] |&gt; \n              str_subset(pattern = \"^-\") |&gt; \n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(bfi_key_name),\n    .by = nest_key\n  ) |&gt;\n1  pull(res_alpha)\n\n\n1\n\npullは最後に作られた列を返すので、実はpull()だけでもres_alpha列を引っ張ってこれます。\n\n\n\n\n$agree\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nA1-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nA2       0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\nA3       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nA4       0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nA5       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.59   0.49  4.6 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\n$conscientious\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nC1       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nC2       0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nC3       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nC4-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\nC5-      0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nC1  2779  0.65  0.67  0.54   0.45  4.5 1.2\nC2  2776  0.70  0.71  0.60   0.50  4.4 1.3\nC3  2780  0.66  0.67  0.54   0.46  4.3 1.3\nC4- 2774  0.74  0.73  0.64   0.55  4.4 1.4\nC5- 2784  0.72  0.68  0.57   0.48  3.7 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nC1 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nC2 0.03 0.09 0.11 0.23 0.35 0.20 0.01\nC3 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nC4 0.28 0.29 0.17 0.16 0.08 0.02 0.01\nC5 0.18 0.20 0.12 0.22 0.17 0.10 0.01\n\n$extraversion\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nE1-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nE2-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nE3       0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nE4       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\nE5       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nE1- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nE2- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nE3  2775  0.68  0.70  0.58   0.50  4.0 1.4\nE4  2791  0.75  0.75  0.66   0.58  4.4 1.5\nE5  2779  0.64  0.66  0.52   0.45  4.4 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nE1 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nE2 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nE3 0.05 0.11 0.15 0.30 0.27 0.13 0.01\nE4 0.05 0.09 0.10 0.16 0.34 0.26 0.00\nE5 0.03 0.08 0.10 0.22 0.34 0.22 0.01\n\n$neuroticism\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nN1      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nN2      0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nN3      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nN4      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nN5      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nN1 2778  0.80  0.80  0.76   0.67  2.9 1.6\nN2 2779  0.79  0.79  0.75   0.65  3.5 1.5\nN3 2789  0.81  0.81  0.74   0.67  3.2 1.6\nN4 2764  0.72  0.71  0.60   0.54  3.2 1.6\nN5 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nN1 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nN2 0.12 0.19 0.15 0.26 0.18 0.10 0.01\nN3 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nN4 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nN5 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$openness\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nO2-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\nO5-      0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nO1  2778  0.62  0.65  0.52   0.39  4.8 1.1\nO2- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nO3  2772  0.67  0.69  0.59   0.45  4.4 1.2\nO4  2786  0.50  0.52  0.29   0.22  4.9 1.2\nO5- 2780  0.67  0.66  0.52   0.42  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nO1 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nO2 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nO3 0.03 0.05 0.11 0.28 0.34 0.20 0.01\nO4 0.02 0.04 0.06 0.17 0.32 0.39 0.01\nO5 0.27 0.32 0.19 0.13 0.07 0.03 0.01\n\n\nということで、元のdfからmapを使って一気にα係数を求めることができました。一つのnamed listに下位尺度5つ分の出力が詰まっているので、オブジェクトに入れた際にGrobal Env.がオブジェクトだらけにならないというのはいい点かもしれないです。"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすくなさそうなデータの場合",
    "text": "やりやすくなさそうなデータの場合\nbfiデータは列名に下位尺度の分類が含まれていたのでやりやすかったのですが、実際に扱うデータだとそうはいかない場合もあると思います。というわけで、bfiを少しいじってこんなデータを用意してみました。\n\n\n\n\n\n\n改造の処理はこちら\n\n\n\n\n\nbfiの列をデモグラフィック列（26-28）以外ランダムに並べ替えるために、列番号をシャッフルします。\n\nset.seed(20251025)\n\n(vec_col_order &lt;- c(sample(1:25), 26:28))\n\n [1] 23  7 13 25  1  3  5 10 17  8 21 22 15 16  4 19  6 18  9 20 12 11  2 14 24\n[26] 26 27 28\n\n\n後でkeyを作る用のベクトルを作成。チェック用に元のbfiの列名を名前に付けておきます。\n\nvec_names_df_test &lt;- c(\n  paste0(\"q1_x\", 1:25),\n  paste0(\"q2_x\", 1:3)\n) |&gt;\n  setNames(\n    colnames(bfi[, vec_col_order])\n  )\n\nvec_names_df_test\n\n       O3        C2        E3        O5        A1        A3        A5        C5 \n  \"q1_x1\"   \"q1_x2\"   \"q1_x3\"   \"q1_x4\"   \"q1_x5\"   \"q1_x6\"   \"q1_x7\"   \"q1_x8\" \n       N2        C3        O1        O2        E5        N1        A4        N4 \n  \"q1_x9\"  \"q1_x10\"  \"q1_x11\"  \"q1_x12\"  \"q1_x13\"  \"q1_x14\"  \"q1_x15\"  \"q1_x16\" \n       C1        N3        C4        N5        E2        E1        A2        E4 \n \"q1_x17\"  \"q1_x18\"  \"q1_x19\"  \"q1_x20\"  \"q1_x21\"  \"q1_x22\"  \"q1_x23\"  \"q1_x24\" \n       O4    gender education       age \n \"q1_x25\"   \"q2_x1\"   \"q2_x2\"   \"q2_x3\" \n\n\n列番号をシャッフルしたベクトルを使って、列を並び替えたdfを作ります。ついでにid列もつけておきます。\n\ndf_test &lt;- bfi[, vec_col_order] |&gt;\n  `colnames&lt;-`(vec_names_df_test) |&gt;\n  rownames_to_column(var = \"id\")\n\ncolnames(df_test)\n\n [1] \"id\"     \"q1_x1\"  \"q1_x2\"  \"q1_x3\"  \"q1_x4\"  \"q1_x5\"  \"q1_x6\"  \"q1_x7\" \n [9] \"q1_x8\"  \"q1_x9\"  \"q1_x10\" \"q1_x11\" \"q1_x12\" \"q1_x13\" \"q1_x14\" \"q1_x15\"\n[17] \"q1_x16\" \"q1_x17\" \"q1_x18\" \"q1_x19\" \"q1_x20\" \"q1_x21\" \"q1_x22\" \"q1_x23\"\n[25] \"q1_x24\" \"q1_x25\" \"q2_x1\"  \"q2_x2\"  \"q2_x3\" \n\n\n構成要素の分類を示すkeyを作ります。実際の場合はnamed listを自力で作成すればいいと思います（それか、psych::make.keys()あたりを使うか）。今回は手入力したくないのでゴリ押します。\n\nlist_test_keys &lt;-\n  names(bfi.keys) |&gt;\n  str_extract(pattern = \"^.\") |&gt;\n  toupper() |&gt;\n  set_names() |&gt;\n  map(\n    .f = \\(x){\n      temp_vec &lt;- vec_names_df_test[1:25]\n      temp_vec[str_starts(names(temp_vec), x)]\n    }\n  ) |&gt;\n  map(\n    .f = \\(x) {\n      if_else(\n        names(x) %in% unlist(bfi.keys),\n        x,\n        paste0(\"-\", x) |&gt;\n          set_names(names(x))\n      )\n    }\n  )\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\n手入力の方がむしろ省コードだし楽だろ！！という指摘は今回はなかったことにします。\nちなみに逆転項目の設定はちゃんとできています。\n\ndata.frame(\n  enframe(\n    unlist(list_test_keys)\n  ) |&gt;\n    arrange(name),\n  enframe(\n    unlist(bfi.keys)\n  )\n)\n\n   name   value         name.1 value.1\n1  A.A1  -q1_x5         agree1     -A1\n2  A.A2  q1_x23         agree2      A2\n3  A.A3   q1_x6         agree3      A3\n4  A.A4  q1_x15         agree4      A4\n5  A.A5   q1_x7         agree5      A5\n6  C.C1  q1_x17 conscientious1      C1\n7  C.C2   q1_x2 conscientious2      C2\n8  C.C3  q1_x10 conscientious3      C3\n9  C.C4 -q1_x19 conscientious4     -C4\n10 C.C5  -q1_x8 conscientious5     -C5\n11 E.E1 -q1_x22  extraversion1     -E1\n12 E.E2 -q1_x21  extraversion2     -E2\n13 E.E3   q1_x3  extraversion3      E3\n14 E.E4  q1_x24  extraversion4      E4\n15 E.E5  q1_x13  extraversion5      E5\n16 N.N1  q1_x14   neuroticism1      N1\n17 N.N2   q1_x9   neuroticism2      N2\n18 N.N3  q1_x18   neuroticism3      N3\n19 N.N4  q1_x16   neuroticism4      N4\n20 N.N5  q1_x20   neuroticism5      N5\n21 O.O1  q1_x11      openness1      O1\n22 O.O2 -q1_x12      openness2     -O2\n23 O.O3   q1_x1      openness3      O3\n24 O.O4  q1_x25      openness4      O4\n25 O.O5  -q1_x4      openness5     -O5\n\n\n\n\n\n元のデータと行数列数は一緒ですが、列名と順番が変わりました。コードブックの作成が必須ですね。\n\nstr(df_test)\n\n'data.frame':   2800 obs. of  29 variables:\n $ id    : chr  \"61617\" \"61618\" \"61620\" \"61621\" ...\n $ q1_x1 : int  3 4 5 4 4 5 5 4 6 5 ...\n $ q1_x2 : int  3 4 5 4 4 6 4 2 6 5 ...\n $ q1_x3 : int  3 6 4 4 5 6 4 4 NA 4 ...\n $ q1_x4 : int  3 3 2 5 3 1 1 3 1 2 ...\n $ q1_x5 : int  2 2 5 4 2 6 2 4 4 2 ...\n $ q1_x6 : int  3 5 5 6 3 5 5 1 6 6 ...\n $ q1_x7 : int  4 5 4 5 5 5 5 1 3 5 ...\n $ q1_x8 : int  4 4 5 5 2 3 3 4 5 1 ...\n $ q1_x9 : int  4 3 5 5 3 5 2 3 5 5 ...\n $ q1_x10: int  3 4 4 3 5 6 4 4 3 6 ...\n $ q1_x11: int  3 4 4 3 3 4 5 3 6 5 ...\n $ q1_x12: int  6 2 2 3 3 3 2 2 6 1 ...\n $ q1_x13: int  4 3 5 4 5 6 5 1 3 5 ...\n $ q1_x14: int  3 3 4 2 2 3 1 6 5 5 ...\n $ q1_x15: int  4 2 4 5 4 6 3 5 3 6 ...\n $ q1_x16: int  2 5 2 4 4 2 1 6 3 2 ...\n $ q1_x17: int  2 5 4 4 4 6 5 3 6 6 ...\n $ q1_x18: int  2 3 4 2 4 2 2 2 2 5 ...\n $ q1_x19: int  4 3 2 5 3 1 2 2 4 2 ...\n $ q1_x20: int  3 5 3 1 3 3 1 4 3 4 ...\n $ q1_x21: int  3 1 4 3 2 1 3 6 3 2 ...\n $ q1_x22: int  3 1 2 5 2 2 4 3 5 2 ...\n $ q1_x23: int  4 4 4 4 3 6 5 3 3 5 ...\n $ q1_x24: int  4 4 4 4 4 5 5 2 4 5 ...\n $ q1_x25: int  4 3 5 3 3 6 6 5 6 5 ...\n $ q2_x1 : int  1 2 2 2 1 2 1 1 1 2 ...\n $ q2_x2 : int  NA NA NA NA NA 3 NA 2 1 NA ...\n $ q2_x3 : int  16 18 17 17 17 21 18 19 19 17 ...\n\n\nそして構成要素の分類のkeyも用意しました。要素のベクトルに名前がついているのはチェックのためで、本来はないと思ってください。QualtricsとかMicrosoft Formsとかには項目をランダムに提示する機能があるので、それを使えば列をこんなにシャッフルする必要はないんですが、「よくわからなかったので、尺度の項目の順番を自分で頑張ってシャッフルしちゃいました…」という事案はあると思います。\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\nというわけで、こちらのデータでもmap()でalpha()を一気に処理してみようと思います。先ほどはデータセットの列名に下位尺度の分類が入っていたので処理が少なく済みましたが、今回はそうではないので処理が少し増えます。\nまずlong型にした後、新たにマッチ用の列を作って逆転項目に-をつけます。（別にわざわざ新しい列を作らなくてもいいのですが、alpha()の出力のときに項目名の前後に-がつくのが気になるので、あえてマッチ用の列を作っています。）\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    )\n  )\n\n# A tibble: 70,000 × 7\n   id    q2_x1 q2_x2 q2_x3 items  value match_items\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;      \n 1 61617     1    NA    16 q1_x1      3 q1_x1      \n 2 61617     1    NA    16 q1_x2      3 q1_x2      \n 3 61617     1    NA    16 q1_x3      3 q1_x3      \n 4 61617     1    NA    16 q1_x4      3 -q1_x4     \n 5 61617     1    NA    16 q1_x5      2 -q1_x5     \n 6 61617     1    NA    16 q1_x6      3 q1_x6      \n 7 61617     1    NA    16 q1_x7      4 q1_x7      \n 8 61617     1    NA    16 q1_x8      4 -q1_x8     \n 9 61617     1    NA    16 q1_x9      4 q1_x9      \n10 61617     1    NA    16 q1_x10     3 q1_x10     \n# ℹ 69,990 more rows\n\n\n次に、マッチ用の列をdplyr::case_match()に入れて、分類用のリストと照合します。case_match()は、第一引数.xにマッチさせたいベクトル、以降はtwo-sided formula形式で処理を書いていきます。左側（LHS）はマッチさせたい要素、右側（RHS）にはマッチしたものに対する出力を入れます。このformulaの部分だけは頑張って書かないといけません。今回は分類リストがnamed listでその要素は文字列ベクトルなので、各要素にマッチしたらその要素のリスト名（＝下位尺度の分類）を返すようにします。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\",\n    )\n  )\n\n# A tibble: 70,000 × 8\n   id    q2_x1 q2_x2 q2_x3 items  value match_items nest_key\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 61617     1    NA    16 q1_x1      3 q1_x1       O       \n 2 61617     1    NA    16 q1_x2      3 q1_x2       C       \n 3 61617     1    NA    16 q1_x3      3 q1_x3       E       \n 4 61617     1    NA    16 q1_x4      3 -q1_x4      O       \n 5 61617     1    NA    16 q1_x5      2 -q1_x5      A       \n 6 61617     1    NA    16 q1_x6      3 q1_x6       A       \n 7 61617     1    NA    16 q1_x7      4 q1_x7       A       \n 8 61617     1    NA    16 q1_x8      4 -q1_x8      C       \n 9 61617     1    NA    16 q1_x9      4 q1_x9       N       \n10 61617     1    NA    16 q1_x10     3 q1_x10      C       \n# ℹ 69,990 more rows\n\n\n後の処理は先ほどと変わりません。nestしたうえでmap処理をしていけばいいです。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\"\n    )\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n1  arrange(nest_key) |&gt;\n  mutate(\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n        x |&gt;\n2          select(-match_items) |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(starts_with(\"q1\")) |&gt;\n          psych::alpha(\n            keys = list_test_keys[[nest_key]] |&gt;\n              str_subset(\"^-\") |&gt;\n              str_remove(\"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(nest_key),\n    .by = nest_key\n  ) |&gt;\n  pull(res_alpha)\n\n\n1\n\ndplyrとかで実装されている引数.byはデータを出現順に並べるので、データによっては出力がきれいな順番（A, B, C, … ）にならないです。なので見やすくするためにdplyr::arrange()でソートしておきます。\n\n2\n\nマッチ用の列は残しておくとwide型にしたときにデータがズレるので外しておきます。マッチ用の列を作っていなかったら省略してOKです。\n\n\n\n\n$A\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x5-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nq1_x6       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nq1_x7       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\nq1_x15      0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nq1_x23      0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x5- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nq1_x6  2774  0.76  0.77  0.71   0.59  4.6 1.3\nq1_x7  2784  0.69  0.70  0.59   0.49  4.6 1.3\nq1_x15 2781  0.65  0.63  0.47   0.39  4.7 1.5\nq1_x23 2773  0.73  0.75  0.67   0.56  4.8 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x5  0.33 0.29 0.14 0.12 0.08 0.03 0.01\nq1_x6  0.03 0.06 0.07 0.20 0.36 0.27 0.01\nq1_x7  0.02 0.07 0.09 0.22 0.35 0.25 0.01\nq1_x15 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nq1_x23 0.02 0.05 0.05 0.20 0.37 0.31 0.01\n\n$C\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x2        0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nq1_x8-       0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\nq1_x10       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nq1_x17       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nq1_x19-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x2   2776  0.70  0.71  0.60   0.50  4.4 1.3\nq1_x8-  2784  0.72  0.68  0.57   0.48  3.7 1.6\nq1_x10  2780  0.66  0.67  0.54   0.46  4.3 1.3\nq1_x17  2779  0.65  0.67  0.54   0.45  4.5 1.2\nq1_x19- 2774  0.74  0.73  0.64   0.55  4.4 1.4\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x2  0.03 0.09 0.11 0.23 0.35 0.20 0.01\nq1_x8  0.18 0.20 0.12 0.22 0.17 0.10 0.01\nq1_x10 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nq1_x17 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nq1_x19 0.28 0.29 0.17 0.16 0.08 0.02 0.01\n\n$E\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x3        0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nq1_x13       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\nq1_x21-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nq1_x22-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nq1_x24       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x3   2775  0.68  0.70  0.58   0.50  4.0 1.4\nq1_x13  2779  0.64  0.66  0.52   0.45  4.4 1.3\nq1_x21- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nq1_x22- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nq1_x24  2791  0.75  0.75  0.66   0.58  4.4 1.5\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x3  0.05 0.11 0.15 0.30 0.27 0.13 0.01\nq1_x13 0.03 0.08 0.10 0.22 0.34 0.22 0.01\nq1_x21 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nq1_x22 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nq1_x24 0.05 0.09 0.10 0.16 0.34 0.26 0.00\n\n$N\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x9       0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nq1_x14      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nq1_x16      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nq1_x18      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nq1_x20      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x9  2779  0.79  0.79  0.75   0.65  3.5 1.5\nq1_x14 2778  0.80  0.80  0.76   0.67  2.9 1.6\nq1_x16 2764  0.72  0.71  0.60   0.54  3.2 1.6\nq1_x18 2789  0.81  0.81  0.74   0.67  3.2 1.6\nq1_x20 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x9  0.12 0.19 0.15 0.26 0.18 0.10 0.01\nq1_x14 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nq1_x16 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nq1_x18 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nq1_x20 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$O\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x1        0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nq1_x4-       0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\nq1_x11       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nq1_x12-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nq1_x25       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x1   2772  0.67  0.69  0.59   0.45  4.4 1.2\nq1_x4-  2780  0.67  0.66  0.52   0.42  4.5 1.3\nq1_x11  2778  0.62  0.65  0.52   0.39  4.8 1.1\nq1_x12- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nq1_x25  2786  0.50  0.52  0.29   0.22  4.9 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x1  0.03 0.05 0.11 0.28 0.34 0.20 0.01\nq1_x4  0.27 0.32 0.19 0.13 0.07 0.03 0.01\nq1_x11 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nq1_x12 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nq1_x25 0.02 0.04 0.06 0.17 0.32 0.39 0.01\n\n\nこちらのデータでもうまくできました。"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "LICENSE",
    "section": "",
    "text": "MIT License\nCopyright 2025 Takuto SAKAI\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content",
    "section": "",
    "text": "大体の投稿がRについての個人的な備忘録とかメモです。\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\ndplyr::rowwiseを使わないで個人の尺度得点を算出する\n\n\n\nR memo\n\npreprocess\n\n\n\ndplyr::rowwise()を使わずに、個人の尺度得点を算出する方法を試してみた。rowMeans(pick(...))が速くていいかも。\n\n\n\n\n\n2025-11-02\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける\n\n\n\nR memo\n\nvisualization\n\n\n\nggplot2でもExcelみたいに片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつけてみた。\n\n\n\n\n\n2025-10-26\n\n\n\n\n\n\n\n\n\n\n\n\npurrr::mapを使ってpsych::alphaを一気に処理する\n\n\n\nR memo\n\npreprocess\n\nanalysis\n\n\n\npurrr::map()の中でpsych::alpha()を使って、元のdfから一気にα係数の処理をしてみた。\n\n\n\n\n\n2025-10-25\n\n\n\n\n\n\n\n\n\n\n\n\n個人ウェブサイトを立ち上げてみました\n\n\n\nannouncement\n\n\n\nQuartoの機能を使って、個人のウェブサイトを作ってみました。\n\n\n\n\n\n2025-10-19\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "名前\n酒井拓人 (SAKAI, Takuto)\n\n\n現所属\n\n愛知学院大学心理学部 実験助手（教員）\n\n（教員）までが正式な職位です。\n\n\n\n\n研究領域\n\n言い方や言葉遣いが印象にどのような影響を与えるかについて興味があります。\n\n自分は心理学科で育ったので心理学の人だと思っているのですが、やっている内容はなんかそうじゃない気もします。\n\n\n\n\nLinks\n\nreseachmap.jp\n\n業績などはresearchmapにまとめてあります。学会発表のポスター（pdf）もなるべくアップロードするようにしています。\n\n  https://orcid.org/0009-0004-0736-5229 \n\nE-mailアドレスはORCIDに掲載しています。\n\nOSF\n\n一部の発表資料はOSFに置いています。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "さかいの備忘録",
    "section": "",
    "text": "Welcom to my website!\n社会心理学者・社会言語学者（？）の酒井拓人のウェブサイトです。\nX (Twitter)で発信できる量にも限界がある（特にRのコード）ので、ウェブサイトを作ってみました。\n\nAbout：私について\nContent：大体の投稿がRの個人的なメモです。"
  },
  {
    "objectID": "posts/20251019_start_website/index.html",
    "href": "posts/20251019_start_website/index.html",
    "title": "個人ウェブサイトを立ち上げてみました",
    "section": "",
    "text": "Quartoの機能を使って個人のウェブサイトを作ってみました。\nRのメモについては、今までX（Twitter）にコードをスクショしたものを上げていたのですが、それもちょっと不便に感じてきたので、これからはコードについてはこのウェブサイトで公開しようと思います。"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Problem",
    "text": "Problem\nまず、データをテキトーに用意します。実験データを読み込んで集計した後だと思ってください。\n\ntest_df &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(3, 3, 4, 5),\n    sd = rep(1.5, 4),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c              3   1.5   1.5   4.5\n2 a          d              3   1.5   1.5   4.5\n3 b          c              4   1.5   2.5   5.5\n4 b          d              5   1.5   3.5   6.5\n\n\nyminは誤差範囲の下側、ymaxは誤差範囲の上側の数値です。実際のデータではdplyr::reframe()やdplyr::summarise()の中で、ggplot2::mean_sdl(..., mult = 1)とすれば、平均値と平均値±1SDの値が一発で得られます。\n\n\n\n\n\n\nggplot2::mean_sdl()の例\n\n\n\n\n\n\niris |&gt;\n  reframe(\n    check_mean = mean(Sepal.Length),\n    check_sd = sd(Sepal.Length),\n    ggplot2::mean_sdl(Sepal.Length, mult = 1),\n    .by = Species\n  )\n\n     Species check_mean  check_sd     y     ymin     ymax\n1     setosa      5.006 0.3524897 5.006 4.653510 5.358490\n2 versicolor      5.936 0.5161711 5.936 5.419829 6.452171\n3  virginica      6.588 0.6358796 6.588 5.952120 7.223880\n\n\nmean_sdl()のHelpを見るとわかりますが、もともとはHmisc::smean.sdl()なのでその引数（mult、na.rm）が使えます。引数multは標準偏差を何倍して平均値に加えるかなので1でいいです。\n\n\n\n普通にggplotで作るならこんな感じでしょうか。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5,\n    position = position_dodge(.2)\n  ) +\n  geom_line(position = position_dodge(.2)) +\n  geom_point(\n    aes(shape = group_line),\n    position = position_dodge(.2),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  )\n\n\n\n\n\n\n\n\n普段の自分で使う用のグラフならこれでいいです。ただ、誤差範囲が両側なのと、Excelでは（簡単に）できない位置ずらしがあるので、答え合わせ用としてはよくないですね。というわけで、位置ずらしせず誤差範囲を片側にしたグラフをggplotでも作りたいわけです。"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Solution",
    "text": "Solution\nまず、ggplot2::geom_errorbar()は要素としてyminとymaxが必須です。なので、例えば上下のどちらかを消そうとしてaes()の中で省いてしまうとエラーになります。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  geom_errorbar(\n    aes(ymax = ymax)\n  )\n\nError in `geom_errorbar()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_errorbar()` requires the following missing aesthetics: ymin or\n  xmin and xmax.\n\n\nということで、yminとymaxに適切な値を設定する必要があります。\nまず、横軸の水準ごとに値が小さい方がTRUEを返す列を作り、値が小さい方はymaxがNAに、値が大きい方はyminがNAになるようにします。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n1    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  )\n\n\n1\n\n横軸の水準ごとになので、ここの.byは必須。それかgroup_by() |&gt; mutate() |&gt; ungroup()。\n\n\n\n\n# A tibble: 4 × 7\n  group_line group_axis     y    sd  ymin  ymax is_smaller\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;     \n1 a          c              3   1.5   1.5  NA   TRUE      \n2 a          d              3   1.5   1.5  NA   TRUE      \n3 b          c              4   1.5  NA     5.5 FALSE     \n4 b          d              5   1.5  NA     6.5 FALSE     \n\n\nこの状態でgeom_errorbar()を使うとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n片側だけにひげができていますが、線分が消えてしまっています。ではNAの代わりにyの値を入れるとどうなるでしょうか。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n線分の表示が戻りました。一見よさそうに見えるのですが、これをマーカーと合わせるとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nマーカーにひげが重なってしまい見栄えが悪いです。ということで、geom::errorbar()にはyを渡すよりはNAを渡す方が作りたいグラフに近いことになります。では、同じことをgeom_linerange()でやるとどうなるでしょう。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin, ymax = ymax),\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nこちらは線分が表示されていますね。\nということは、geom_errorbar()にNAを入れて片方のひげだけ作り、geom_linerange()にyを入れて線分を描いてもらえば解決しそうな気がします。というわけで、geom_errorbar()用の参照列とgeom_linerange()用の参照列を作ってグラフを描いてもらいます。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n求めていたものができました。後はいろいろ調整すれば完成です。\n\n\nCode\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 8),\n    expand = expansion()\n  )\n\n\n\n\n\n\n\n\n\nこんな風に微妙にクロスしているデータでもうまくいきます。\n\ntest_df_cross &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(2.1, 2, 2, 4),\n    sd = c(1.7, 1, .5, .2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_cross\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c            2.1   1.7   0.4   3.8\n2 a          d            2     1     1     3  \n3 b          c            2     0.5   1.5   2.5\n4 b          d            4     0.2   3.8   4.2\n\n\n\n\nCode\ntest_df_cross |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcelで同じことをする場合\n\n\n\n\n\nちなみに、Excelで誤差範囲を片側にしているときにデータがクロスしている場合、誤差範囲の値の片方を負の値にする必要があります。\n\n\n\n平均値\n水準c\n水準d\n\n\n\n\n条件A\n2.1\n2\n\n\n条件B\n2\n4\n\n\n\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n1.7\n1\n\n\n条件B\n0.5\n0.2\n\n\n\nこんな感じでグラフ作成用にデータを配置していた場合、以下のように標準偏差の方のどちらかの水準の列を負の値にすると、誤差範囲の向きが反転します。\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n-1.7\n1\n\n\n条件B\n-0.5\n0.2\n\n\n\n\n\n\n横軸の要因が2要因以上あっても大丈夫です。\n\nset.seed(20251026)\n\ntest_df_2x4 &lt;- \n  tibble(\n    group_line = rep(letters[1:2], each = 4),\n    group_axis = rep(LETTERS[1:4], times = 2),\n    y = runif(8, min = 3, max = 5),\n    sd = runif(8, min = .5, max = 2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_2x4\n\n# A tibble: 8 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          A           3.65 1.89   1.76  5.54\n2 a          B           3.88 1.77   2.11  5.65\n3 a          C           4.90 0.816  4.08  5.71\n4 a          D           4.51 1.38   3.13  5.88\n5 b          A           3.98 1.25   2.73  5.22\n6 b          B           4.79 1.03   3.76  5.82\n7 b          C           3.30 1.39   1.91  4.70\n8 b          D           4.50 0.681  3.82  5.18\n\n\n\n\nCode\ntest_df_2x4 |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )"
  }
]