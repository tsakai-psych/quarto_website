[
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Problem",
    "text": "Problem\nまず、データをテキトーに用意します。実験データを読み込んで集計した後だと思ってください。\n\ntest_df &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(3, 3, 4, 5),\n    sd = rep(1.5, 4),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c              3   1.5   1.5   4.5\n2 a          d              3   1.5   1.5   4.5\n3 b          c              4   1.5   2.5   5.5\n4 b          d              5   1.5   3.5   6.5\n\n\nyminは誤差範囲の下側、ymaxは誤差範囲の上側の数値です。実際のデータではdplyr::reframe()やdplyr::summarise()の中で、ggplot2::mean_sdl(..., mult = 1)とすれば、平均値と平均値±1SDの値が一発で得られます。\n\n\n\n\n\n\nggplot2::mean_sdl()の例\n\n\n\n\n\n\niris |&gt;\n  reframe(\n    check_mean = mean(Sepal.Length),\n    check_sd = sd(Sepal.Length),\n    ggplot2::mean_sdl(Sepal.Length, mult = 1),\n    .by = Species\n  )\n\n     Species check_mean  check_sd     y     ymin     ymax\n1     setosa      5.006 0.3524897 5.006 4.653510 5.358490\n2 versicolor      5.936 0.5161711 5.936 5.419829 6.452171\n3  virginica      6.588 0.6358796 6.588 5.952120 7.223880\n\n\nmean_sdl()のHelpを見るとわかりますが、もともとはHmisc::smean.sdl()なのでその引数（mult、na.rm）が使えます。引数multは標準偏差を何倍して平均値に加えるかなので1でいいです。\n\n\n\n普通にggplotで作るならこんな感じでしょうか。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5,\n    position = position_dodge(.2)\n  ) +\n  geom_line(position = position_dodge(.2)) +\n  geom_point(\n    aes(shape = group_line),\n    position = position_dodge(.2),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  )\n\n\n\n\n\n\n\n\n普段の自分で使う用のグラフならこれでいいです。ただ、誤差範囲が両側なのと、Excelでは（簡単に）できない位置ずらしがあるので、答え合わせ用としてはよくないですね。というわけで、位置ずらしせず誤差範囲を片側にしたグラフをggplotでも作りたいわけです。"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Solution",
    "text": "Solution\nまず、ggplot2::geom_errorbar()は要素としてyminとymaxが必須です。なので、例えば上下のどちらかを消そうとしてaes()の中で省いてしまうとエラーになります。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  geom_errorbar(\n    aes(ymax = ymax)\n  )\n\nError in `geom_errorbar()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_errorbar()` requires the following missing aesthetics: ymin or\n  xmin and xmax.\n\n\nということで、yminとymaxに適切な値を設定する必要があります。\nまず、横軸の水準ごとに値が小さい方がTRUEを返す列を作り、値が小さい方はymaxがNAに、値が大きい方はyminがNAになるようにします。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n1    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  )\n\n\n1\n\n横軸の水準ごとになので、ここの.byは必須。それかgroup_by() |&gt; mutate() |&gt; ungroup()。\n\n\n\n\n# A tibble: 4 × 7\n  group_line group_axis     y    sd  ymin  ymax is_smaller\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;     \n1 a          c              3   1.5   1.5  NA   TRUE      \n2 a          d              3   1.5   1.5  NA   TRUE      \n3 b          c              4   1.5  NA     5.5 FALSE     \n4 b          d              5   1.5  NA     6.5 FALSE     \n\n\nこの状態でgeom_errorbar()を使うとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n片側だけにひげができていますが、線分が消えてしまっています。ではNAの代わりにyの値を入れるとどうなるでしょうか。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n線分の表示が戻りました。一見よさそうに見えるのですが、これをマーカーと合わせるとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nマーカーにひげが重なってしまい見栄えが悪いです。ということで、geom::errorbar()にはyを渡すよりはNAを渡す方が作りたいグラフに近いことになります。では、同じことをgeom_linerange()でやるとどうなるでしょう。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin, ymax = ymax),\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nこちらは線分が表示されていますね。\nということは、geom_errorbar()にNAを入れて片方のひげだけ作り、geom_linerange()にyを入れて線分を描いてもらえば解決しそうな気がします。というわけで、geom_errorbar()用の参照列とgeom_linerange()用の参照列を作ってグラフを描いてもらいます。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n求めていたものができました。後はいろいろ調整すれば完成です。\n\n\nCode\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 8),\n    expand = expansion()\n  )\n\n\n\n\n\n\n\n\n\nこんな風に微妙にクロスしているデータでもうまくいきます。\n\ntest_df_cross &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(2.1, 2, 2, 4),\n    sd = c(1.7, 1, .5, .2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_cross\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c            2.1   1.7   0.4   3.8\n2 a          d            2     1     1     3  \n3 b          c            2     0.5   1.5   2.5\n4 b          d            4     0.2   3.8   4.2\n\n\n\n\nCode\ntest_df_cross |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcelで同じことをする場合\n\n\n\n\n\nちなみに、Excelで誤差範囲を片側にしているときにデータがクロスしている場合、誤差範囲の値の片方を負の値にする必要があります。\n\n\n\n平均値\n水準c\n水準d\n\n\n\n\n条件A\n2.1\n2\n\n\n条件B\n2\n4\n\n\n\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n1.7\n1\n\n\n条件B\n0.5\n0.2\n\n\n\nこんな感じでグラフ作成用にデータを配置していた場合、以下のように標準偏差の方のどちらかの水準の列を負の値にすると、誤差範囲の向きが反転します。\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n-1.7\n1\n\n\n条件B\n-0.5\n0.2\n\n\n\n\n\n\n横軸の要因が2要因以上あっても大丈夫です。\n\nset.seed(20251026)\n\ntest_df_2x4 &lt;- \n  tibble(\n    group_line = rep(letters[1:2], each = 4),\n    group_axis = rep(LETTERS[1:4], times = 2),\n    y = runif(8, min = 3, max = 5),\n    sd = runif(8, min = .5, max = 2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_2x4\n\n# A tibble: 8 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          A           3.65 1.89   1.76  5.54\n2 a          B           3.88 1.77   2.11  5.65\n3 a          C           4.90 0.816  4.08  5.71\n4 a          D           4.51 1.38   3.13  5.88\n5 b          A           3.98 1.25   2.73  5.22\n6 b          B           4.79 1.03   3.76  5.82\n7 b          C           3.30 1.39   1.91  4.70\n8 b          D           4.50 0.681  3.82  5.18\n\n\n\n\nCode\ntest_df_2x4 |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )"
  },
  {
    "objectID": "posts/20251019_start_website/index.html",
    "href": "posts/20251019_start_website/index.html",
    "title": "個人ウェブサイトを立ち上げてみました",
    "section": "",
    "text": "Quartoの機能を使って個人のウェブサイトを作ってみました。\nRのメモについては、今までX（Twitter）にコードをスクショしたものを上げていたのですが、それもちょっと不便に感じてきたので、これからはコードについてはこのウェブサイトで公開しようと思います。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "さかいの備忘録",
    "section": "",
    "text": "Welcom to my website!\n社会心理学者・社会言語学者（？）の酒井拓人のウェブサイトです。\nX (Twitter)で発信できる量にも限界がある（特にRのコード）ので、ウェブサイトを作ってみました。\n\nAbout：私について\nContent：大体の投稿がRの個人的なメモです。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "名前\n酒井拓人 (SAKAI, Takuto)\n\n\n現所属\n\n愛知学院大学心理学部 実験助手（教員）\n\n（教員）までが正式な職位です。\n\n\n\n\n研究領域\n\n言い方や言葉遣いが印象にどのような影響を与えるかについて興味があります。\n\n自分は心理学科で育ったので心理学の人だと思っているのですが、やっている内容はなんかそうじゃない気もします。\n\n\n\n\nLinks\n\nreseachmap.jp\n\n業績などはresearchmapにまとめてあります。学会発表のポスター（pdf）もなるべくアップロードするようにしています。\n\n  https://orcid.org/0009-0004-0736-5229 \n\nE-mailアドレスはORCIDに掲載しています。\n\nOSF\n\n一部の発表資料はOSFに置いています。"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content",
    "section": "",
    "text": "大体の投稿がRについての個人的な備忘録とかメモです。\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける\n\n\n\nR memo\n\nvisualization\n\n\n\nggplot2でもExcelみたいに片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつけてみた。\n\n\n\n\n\n2025-10-26\n\n\n\n\n\n\n\n\n\n\n\n\npurrr::mapを使ってpsych::alphaを一気に処理する\n\n\n\nR memo\n\npreprocess\n\nanalysis\n\n\n\npurrr::map()の中でpsych::alpha()を使って、元のdfから一気にα係数の処理をしてみた。\n\n\n\n\n\n2025-10-25\n\n\n\n\n\n\n\n\n\n\n\n\n個人ウェブサイトを立ち上げてみました\n\n\n\nannouncement\n\n\n\nQuartoの機能を使って、個人のウェブサイトを作ってみました。\n\n\n\n\n\n2025-10-19\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "LICENSE",
    "section": "",
    "text": "MIT License\nCopyright 2025 Takuto SAKAI\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html",
    "href": "posts/20251025_map_alpha/index.html",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  psych\n)"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすいデータの場合",
    "text": "やりやすいデータの場合\nまず、処理しやすいようにデータをlong型にします。ついでにtibble::rownames_to_column()でrownameをid列に変えておきます。id列がないとtidyr::pivot_wider()出来なくなるので、少なくともその処理の前にはやっておきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n1    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  )\n\n\n1\n\n引数colsはtidy-selectの文法なので-c(id, gender, education, age)でもいいです。longにしたい列名が「文字1文字数字1文字」なのがわかっているので、dplyr::matches()で正規表現を使って絞りました。\n\n\n\n\n# A tibble: 70,000 × 6\n   id    gender education   age items value\n   &lt;chr&gt;  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n 1 61617      1        NA    16 A1        2\n 2 61617      1        NA    16 A2        4\n 3 61617      1        NA    16 A3        3\n 4 61617      1        NA    16 A4        4\n 5 61617      1        NA    16 A5        4\n 6 61617      1        NA    16 C1        2\n 7 61617      1        NA    16 C2        3\n 8 61617      1        NA    16 C3        3\n 9 61617      1        NA    16 C4        4\n10 61617      1        NA    16 C5        4\n# ℹ 69,990 more rows\n\n\n次に、nest用の列を作ってnestします。今回は列名に下位尺度が入っているので、それを抽出すれば簡単にnest用の列が完成です。やりやすいデータとはこのことを言っています。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n1    nest_key = str_extract(items, \"^.\") |&gt;\n2      tolower()\n  ) |&gt;\n3  nest(.by = nest_key)\n\n\n1\n\nitems（列名のベクトル）の各要素から最初の一文字だけ抜ければいいので、stringr::str_extract()でいいです。\n\n2\n\nあとでbfi.keysから抽出しやすくすために、小文字にします。\n\n3\n\nnestに使った列以外の残りは、引数.keysを指定しなければdata列にまとまります。\n\n\n\n\n# A tibble: 5 × 2\n  nest_key data                 \n  &lt;chr&gt;    &lt;list&gt;               \n1 a        &lt;tibble [14,000 × 6]&gt;\n2 c        &lt;tibble [14,000 × 6]&gt;\n3 e        &lt;tibble [14,000 × 6]&gt;\n4 n        &lt;tibble [14,000 × 6]&gt;\n5 o        &lt;tibble [14,000 × 6]&gt;\n\n\nnestされたデータを処理します。data列はlistなので、中の各要素を処理したいときはpurrr::map()が使えます。data列の各要素はデータフレームで、しかもaの行はAで始まる項目だけ、cの行はCで始まる項目だけ、eの行は…のlongデータになっています。そのため、map()の中でwide型に直して必要列以外取り除き、psych::alpha()に入れてあげればいいわけです。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n1    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n2            keys = bfi.keys[[bfi_key_name]] |&gt;\n3              str_subset(pattern = \"^-\") |&gt;\n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n4      set_names(bfi_key_name),\n5    .by = nest_key\n  )\n\n\n1\n\nalpha()の中でbfi.keysの要素名を使えるようにしたいので、ここで抜き出しておきます。\n\n2\n\nまさに上の処理で抜き出した要素名をここで使います。\n\n3\n\n引数keysは、逆転する項目の項目名を文字列ベクトルで入れるか、逆転する項目は-1、そのままの項目は1にした数値ベクトルを入れます。今回は前者で入れるので、bfi.keysの各要素（文字列ベクトル）のうち、stringr::str_subset()を使って-で始まる項目だけを抽出して、str_remove()で-を取り除きます。\n\n4\n\nmap()の戻り値のlistに名前を付けたいのでmap() |&gt; purrr::set_names()とします。nestデータのほかの列の要素を使えるので、dplyr::group_map() |&gt; set_names()よりもいい気がします。\n\n5\n\n引数.byにnest_key列を指定して、実質rowwise処理をします。これを指定することで、(2)の処理で[[bfi_key_name]]のところにbfi_key_name列の要素が1つだけ入ります。これがないと、(2)の処理で要素5の文字列ベクトルc（\"agree\", \"conscientious\", ...）が[[の中に入ってしまうのでエラーになります。\n\n\n\n\n# A tibble: 5 × 4\n  nest_key data                  bfi_key_name  res_alpha   \n  &lt;chr&gt;    &lt;list&gt;                &lt;chr&gt;         &lt;named list&gt;\n1 a        &lt;tibble [14,000 × 6]&gt; agree         &lt;psych&gt;     \n2 c        &lt;tibble [14,000 × 6]&gt; conscientious &lt;psych&gt;     \n3 e        &lt;tibble [14,000 × 6]&gt; extraversion  &lt;psych&gt;     \n4 n        &lt;tibble [14,000 × 6]&gt; neuroticism   &lt;psych&gt;     \n5 o        &lt;tibble [14,000 × 6]&gt; openness      &lt;psych&gt;     \n\n\n最後に結果が詰まったres_alpha列だけ取り出します。データフレームからある1列の要素を取り出すときはdplyr::pull()が使えます。res_alpha列はmap()を使って作ったので、各下位尺度のα係数が入ったlistが返ってきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n            keys = bfi.keys[[bfi_key_name]] |&gt; \n              str_subset(pattern = \"^-\") |&gt; \n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(bfi_key_name),\n    .by = nest_key\n  ) |&gt;\n1  pull(res_alpha)\n\n\n1\n\npullは最後に作られた列を返すので、実はpull()だけでもres_alpha列を引っ張ってこれます。\n\n\n\n\n$agree\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nA1-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nA2       0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\nA3       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nA4       0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nA5       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.59   0.49  4.6 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\n$conscientious\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nC1       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nC2       0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nC3       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nC4-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\nC5-      0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nC1  2779  0.65  0.67  0.54   0.45  4.5 1.2\nC2  2776  0.70  0.71  0.60   0.50  4.4 1.3\nC3  2780  0.66  0.67  0.54   0.46  4.3 1.3\nC4- 2774  0.74  0.73  0.64   0.55  4.4 1.4\nC5- 2784  0.72  0.68  0.57   0.48  3.7 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nC1 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nC2 0.03 0.09 0.11 0.23 0.35 0.20 0.01\nC3 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nC4 0.28 0.29 0.17 0.16 0.08 0.02 0.01\nC5 0.18 0.20 0.12 0.22 0.17 0.10 0.01\n\n$extraversion\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nE1-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nE2-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nE3       0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nE4       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\nE5       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nE1- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nE2- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nE3  2775  0.68  0.70  0.58   0.50  4.0 1.4\nE4  2791  0.75  0.75  0.66   0.58  4.4 1.5\nE5  2779  0.64  0.66  0.52   0.45  4.4 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nE1 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nE2 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nE3 0.05 0.11 0.15 0.30 0.27 0.13 0.01\nE4 0.05 0.09 0.10 0.16 0.34 0.26 0.00\nE5 0.03 0.08 0.10 0.22 0.34 0.22 0.01\n\n$neuroticism\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nN1      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nN2      0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nN3      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nN4      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nN5      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nN1 2778  0.80  0.80  0.76   0.67  2.9 1.6\nN2 2779  0.79  0.79  0.75   0.65  3.5 1.5\nN3 2789  0.81  0.81  0.74   0.67  3.2 1.6\nN4 2764  0.72  0.71  0.60   0.54  3.2 1.6\nN5 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nN1 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nN2 0.12 0.19 0.15 0.26 0.18 0.10 0.01\nN3 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nN4 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nN5 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$openness\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nO2-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\nO5-      0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nO1  2778  0.62  0.65  0.52   0.39  4.8 1.1\nO2- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nO3  2772  0.67  0.69  0.59   0.45  4.4 1.2\nO4  2786  0.50  0.52  0.29   0.22  4.9 1.2\nO5- 2780  0.67  0.66  0.52   0.42  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nO1 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nO2 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nO3 0.03 0.05 0.11 0.28 0.34 0.20 0.01\nO4 0.02 0.04 0.06 0.17 0.32 0.39 0.01\nO5 0.27 0.32 0.19 0.13 0.07 0.03 0.01\n\n\nということで、元のdfからmapを使って一気にα係数を求めることができました。一つのnamed listに下位尺度5つ分の出力が詰まっているので、オブジェクトに入れた際にGrobal Env.がオブジェクトだらけにならないというのはいい点かもしれないです。"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすくなさそうなデータの場合",
    "text": "やりやすくなさそうなデータの場合\nbfiデータは列名に下位尺度の分類が含まれていたのでやりやすかったのですが、実際に扱うデータだとそうはいかない場合もあると思います。というわけで、bfiを少しいじってこんなデータを用意してみました。\n\n\n\n\n\n\n改造の処理はこちら\n\n\n\n\n\nbfiの列をデモグラフィック列（26-28）以外ランダムに並べ替えるために、列番号をシャッフルします。\n\nset.seed(20251025)\n\n(vec_col_order &lt;- c(sample(1:25), 26:28))\n\n [1] 23  7 13 25  1  3  5 10 17  8 21 22 15 16  4 19  6 18  9 20 12 11  2 14 24\n[26] 26 27 28\n\n\n後でkeyを作る用のベクトルを作成。チェック用に元のbfiの列名を名前に付けておきます。\n\nvec_names_df_test &lt;- c(\n  paste0(\"q1_x\", 1:25),\n  paste0(\"q2_x\", 1:3)\n) |&gt;\n  setNames(\n    colnames(bfi[, vec_col_order])\n  )\n\nvec_names_df_test\n\n       O3        C2        E3        O5        A1        A3        A5        C5 \n  \"q1_x1\"   \"q1_x2\"   \"q1_x3\"   \"q1_x4\"   \"q1_x5\"   \"q1_x6\"   \"q1_x7\"   \"q1_x8\" \n       N2        C3        O1        O2        E5        N1        A4        N4 \n  \"q1_x9\"  \"q1_x10\"  \"q1_x11\"  \"q1_x12\"  \"q1_x13\"  \"q1_x14\"  \"q1_x15\"  \"q1_x16\" \n       C1        N3        C4        N5        E2        E1        A2        E4 \n \"q1_x17\"  \"q1_x18\"  \"q1_x19\"  \"q1_x20\"  \"q1_x21\"  \"q1_x22\"  \"q1_x23\"  \"q1_x24\" \n       O4    gender education       age \n \"q1_x25\"   \"q2_x1\"   \"q2_x2\"   \"q2_x3\" \n\n\n列番号をシャッフルしたベクトルを使って、列を並び替えたdfを作ります。ついでにid列もつけておきます。\n\ndf_test &lt;- bfi[, vec_col_order] |&gt;\n  `colnames&lt;-`(vec_names_df_test) |&gt;\n  rownames_to_column(var = \"id\")\n\ncolnames(df_test)\n\n [1] \"id\"     \"q1_x1\"  \"q1_x2\"  \"q1_x3\"  \"q1_x4\"  \"q1_x5\"  \"q1_x6\"  \"q1_x7\" \n [9] \"q1_x8\"  \"q1_x9\"  \"q1_x10\" \"q1_x11\" \"q1_x12\" \"q1_x13\" \"q1_x14\" \"q1_x15\"\n[17] \"q1_x16\" \"q1_x17\" \"q1_x18\" \"q1_x19\" \"q1_x20\" \"q1_x21\" \"q1_x22\" \"q1_x23\"\n[25] \"q1_x24\" \"q1_x25\" \"q2_x1\"  \"q2_x2\"  \"q2_x3\" \n\n\n構成要素の分類を示すkeyを作ります。実際の場合はnamed listを自力で作成すればいいと思います（それか、psych::make.keys()あたりを使うか）。今回は手入力したくないのでゴリ押します。\n\nlist_test_keys &lt;-\n  names(bfi.keys) |&gt;\n  str_extract(pattern = \"^.\") |&gt;\n  toupper() |&gt;\n  set_names() |&gt;\n  map(\n    .f = \\(x){\n      temp_vec &lt;- vec_names_df_test[1:25]\n      temp_vec[str_starts(names(temp_vec), x)]\n    }\n  ) |&gt;\n  map(\n    .f = \\(x) {\n      if_else(\n        names(x) %in% unlist(bfi.keys),\n        x,\n        paste0(\"-\", x) |&gt;\n          set_names(names(x))\n      )\n    }\n  )\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\n手入力の方がむしろ省コードだし楽だろ！！という指摘は今回はなかったことにします。\nちなみに逆転項目の設定はちゃんとできています。\n\ndata.frame(\n  enframe(\n    unlist(list_test_keys)\n  ) |&gt;\n    arrange(name),\n  enframe(\n    unlist(bfi.keys)\n  )\n)\n\n   name   value         name.1 value.1\n1  A.A1  -q1_x5         agree1     -A1\n2  A.A2  q1_x23         agree2      A2\n3  A.A3   q1_x6         agree3      A3\n4  A.A4  q1_x15         agree4      A4\n5  A.A5   q1_x7         agree5      A5\n6  C.C1  q1_x17 conscientious1      C1\n7  C.C2   q1_x2 conscientious2      C2\n8  C.C3  q1_x10 conscientious3      C3\n9  C.C4 -q1_x19 conscientious4     -C4\n10 C.C5  -q1_x8 conscientious5     -C5\n11 E.E1 -q1_x22  extraversion1     -E1\n12 E.E2 -q1_x21  extraversion2     -E2\n13 E.E3   q1_x3  extraversion3      E3\n14 E.E4  q1_x24  extraversion4      E4\n15 E.E5  q1_x13  extraversion5      E5\n16 N.N1  q1_x14   neuroticism1      N1\n17 N.N2   q1_x9   neuroticism2      N2\n18 N.N3  q1_x18   neuroticism3      N3\n19 N.N4  q1_x16   neuroticism4      N4\n20 N.N5  q1_x20   neuroticism5      N5\n21 O.O1  q1_x11      openness1      O1\n22 O.O2 -q1_x12      openness2     -O2\n23 O.O3   q1_x1      openness3      O3\n24 O.O4  q1_x25      openness4      O4\n25 O.O5  -q1_x4      openness5     -O5\n\n\n\n\n\n元のデータと行数列数は一緒ですが、列名と順番が変わりました。コードブックの作成が必須ですね。\n\nstr(df_test)\n\n'data.frame':   2800 obs. of  29 variables:\n $ id    : chr  \"61617\" \"61618\" \"61620\" \"61621\" ...\n $ q1_x1 : int  3 4 5 4 4 5 5 4 6 5 ...\n $ q1_x2 : int  3 4 5 4 4 6 4 2 6 5 ...\n $ q1_x3 : int  3 6 4 4 5 6 4 4 NA 4 ...\n $ q1_x4 : int  3 3 2 5 3 1 1 3 1 2 ...\n $ q1_x5 : int  2 2 5 4 2 6 2 4 4 2 ...\n $ q1_x6 : int  3 5 5 6 3 5 5 1 6 6 ...\n $ q1_x7 : int  4 5 4 5 5 5 5 1 3 5 ...\n $ q1_x8 : int  4 4 5 5 2 3 3 4 5 1 ...\n $ q1_x9 : int  4 3 5 5 3 5 2 3 5 5 ...\n $ q1_x10: int  3 4 4 3 5 6 4 4 3 6 ...\n $ q1_x11: int  3 4 4 3 3 4 5 3 6 5 ...\n $ q1_x12: int  6 2 2 3 3 3 2 2 6 1 ...\n $ q1_x13: int  4 3 5 4 5 6 5 1 3 5 ...\n $ q1_x14: int  3 3 4 2 2 3 1 6 5 5 ...\n $ q1_x15: int  4 2 4 5 4 6 3 5 3 6 ...\n $ q1_x16: int  2 5 2 4 4 2 1 6 3 2 ...\n $ q1_x17: int  2 5 4 4 4 6 5 3 6 6 ...\n $ q1_x18: int  2 3 4 2 4 2 2 2 2 5 ...\n $ q1_x19: int  4 3 2 5 3 1 2 2 4 2 ...\n $ q1_x20: int  3 5 3 1 3 3 1 4 3 4 ...\n $ q1_x21: int  3 1 4 3 2 1 3 6 3 2 ...\n $ q1_x22: int  3 1 2 5 2 2 4 3 5 2 ...\n $ q1_x23: int  4 4 4 4 3 6 5 3 3 5 ...\n $ q1_x24: int  4 4 4 4 4 5 5 2 4 5 ...\n $ q1_x25: int  4 3 5 3 3 6 6 5 6 5 ...\n $ q2_x1 : int  1 2 2 2 1 2 1 1 1 2 ...\n $ q2_x2 : int  NA NA NA NA NA 3 NA 2 1 NA ...\n $ q2_x3 : int  16 18 17 17 17 21 18 19 19 17 ...\n\n\nそして構成要素の分類のkeyも用意しました。要素のベクトルに名前がついているのはチェックのためで、本来はないと思ってください。QualtricsとかMicrosoft Formsとかには項目をランダムに提示する機能があるので、それを使えば列をこんなにシャッフルする必要はないんですが、「よくわからなかったので、尺度の項目の順番を自分で頑張ってシャッフルしちゃいました…」という事案はあると思います。\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\nというわけで、こちらのデータでもmap()でalpha()を一気に処理してみようと思います。先ほどはデータセットの列名に下位尺度の分類が入っていたので処理が少なく済みましたが、今回はそうではないので処理が少し増えます。\nまずlong型にした後、新たにマッチ用の列を作って逆転項目に-をつけます。（別にわざわざ新しい列を作らなくてもいいのですが、alpha()の出力のときに項目名の前後に-がつくのが気になるので、あえてマッチ用の列を作っています。）\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    )\n  )\n\n# A tibble: 70,000 × 7\n   id    q2_x1 q2_x2 q2_x3 items  value match_items\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;      \n 1 61617     1    NA    16 q1_x1      3 q1_x1      \n 2 61617     1    NA    16 q1_x2      3 q1_x2      \n 3 61617     1    NA    16 q1_x3      3 q1_x3      \n 4 61617     1    NA    16 q1_x4      3 -q1_x4     \n 5 61617     1    NA    16 q1_x5      2 -q1_x5     \n 6 61617     1    NA    16 q1_x6      3 q1_x6      \n 7 61617     1    NA    16 q1_x7      4 q1_x7      \n 8 61617     1    NA    16 q1_x8      4 -q1_x8     \n 9 61617     1    NA    16 q1_x9      4 q1_x9      \n10 61617     1    NA    16 q1_x10     3 q1_x10     \n# ℹ 69,990 more rows\n\n\n次に、マッチ用の列をdplyr::case_match()に入れて、分類用のリストと照合します。case_match()は、第一引数.xにマッチさせたいベクトル、以降はtwo-sided formula形式で処理を書いていきます。左側（LHS）はマッチさせたい要素、右側（RHS）にはマッチしたものに対する出力を入れます。このformulaの部分だけは頑張って書かないといけません。今回は分類リストがnamed listでその要素は文字列ベクトルなので、各要素にマッチしたらその要素のリスト名（＝下位尺度の分類）を返すようにします。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\",\n    )\n  )\n\n# A tibble: 70,000 × 8\n   id    q2_x1 q2_x2 q2_x3 items  value match_items nest_key\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 61617     1    NA    16 q1_x1      3 q1_x1       O       \n 2 61617     1    NA    16 q1_x2      3 q1_x2       C       \n 3 61617     1    NA    16 q1_x3      3 q1_x3       E       \n 4 61617     1    NA    16 q1_x4      3 -q1_x4      O       \n 5 61617     1    NA    16 q1_x5      2 -q1_x5      A       \n 6 61617     1    NA    16 q1_x6      3 q1_x6       A       \n 7 61617     1    NA    16 q1_x7      4 q1_x7       A       \n 8 61617     1    NA    16 q1_x8      4 -q1_x8      C       \n 9 61617     1    NA    16 q1_x9      4 q1_x9       N       \n10 61617     1    NA    16 q1_x10     3 q1_x10      C       \n# ℹ 69,990 more rows\n\n\n後の処理は先ほどと変わりません。nestしたうえでmap処理をしていけばいいです。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\"\n    )\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n1  arrange(nest_key) |&gt;\n  mutate(\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n        x |&gt;\n2          select(-match_items) |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(starts_with(\"q1\")) |&gt;\n          psych::alpha(\n            keys = list_test_keys[[nest_key]] |&gt;\n              str_subset(\"^-\") |&gt;\n              str_remove(\"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(nest_key),\n    .by = nest_key\n  ) |&gt;\n  pull(res_alpha)\n\n\n1\n\ndplyrとかで実装されている引数.byはデータを出現順に並べるので、データによっては出力がきれいな順番（A, B, C, … ）にならないです。なので見やすくするためにdplyr::arrange()でソートしておきます。\n\n2\n\nマッチ用の列は残しておくとwide型にしたときにデータがズレるので外しておきます。マッチ用の列を作っていなかったら省略してOKです。\n\n\n\n\n$A\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x5-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nq1_x6       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nq1_x7       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\nq1_x15      0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nq1_x23      0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x5- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nq1_x6  2774  0.76  0.77  0.71   0.59  4.6 1.3\nq1_x7  2784  0.69  0.70  0.59   0.49  4.6 1.3\nq1_x15 2781  0.65  0.63  0.47   0.39  4.7 1.5\nq1_x23 2773  0.73  0.75  0.67   0.56  4.8 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x5  0.33 0.29 0.14 0.12 0.08 0.03 0.01\nq1_x6  0.03 0.06 0.07 0.20 0.36 0.27 0.01\nq1_x7  0.02 0.07 0.09 0.22 0.35 0.25 0.01\nq1_x15 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nq1_x23 0.02 0.05 0.05 0.20 0.37 0.31 0.01\n\n$C\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x2        0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nq1_x8-       0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\nq1_x10       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nq1_x17       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nq1_x19-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x2   2776  0.70  0.71  0.60   0.50  4.4 1.3\nq1_x8-  2784  0.72  0.68  0.57   0.48  3.7 1.6\nq1_x10  2780  0.66  0.67  0.54   0.46  4.3 1.3\nq1_x17  2779  0.65  0.67  0.54   0.45  4.5 1.2\nq1_x19- 2774  0.74  0.73  0.64   0.55  4.4 1.4\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x2  0.03 0.09 0.11 0.23 0.35 0.20 0.01\nq1_x8  0.18 0.20 0.12 0.22 0.17 0.10 0.01\nq1_x10 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nq1_x17 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nq1_x19 0.28 0.29 0.17 0.16 0.08 0.02 0.01\n\n$E\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x3        0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nq1_x13       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\nq1_x21-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nq1_x22-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nq1_x24       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x3   2775  0.68  0.70  0.58   0.50  4.0 1.4\nq1_x13  2779  0.64  0.66  0.52   0.45  4.4 1.3\nq1_x21- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nq1_x22- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nq1_x24  2791  0.75  0.75  0.66   0.58  4.4 1.5\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x3  0.05 0.11 0.15 0.30 0.27 0.13 0.01\nq1_x13 0.03 0.08 0.10 0.22 0.34 0.22 0.01\nq1_x21 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nq1_x22 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nq1_x24 0.05 0.09 0.10 0.16 0.34 0.26 0.00\n\n$N\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x9       0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nq1_x14      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nq1_x16      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nq1_x18      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nq1_x20      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x9  2779  0.79  0.79  0.75   0.65  3.5 1.5\nq1_x14 2778  0.80  0.80  0.76   0.67  2.9 1.6\nq1_x16 2764  0.72  0.71  0.60   0.54  3.2 1.6\nq1_x18 2789  0.81  0.81  0.74   0.67  3.2 1.6\nq1_x20 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x9  0.12 0.19 0.15 0.26 0.18 0.10 0.01\nq1_x14 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nq1_x16 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nq1_x18 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nq1_x20 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$O\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x1        0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nq1_x4-       0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\nq1_x11       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nq1_x12-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nq1_x25       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x1   2772  0.67  0.69  0.59   0.45  4.4 1.2\nq1_x4-  2780  0.67  0.66  0.52   0.42  4.5 1.3\nq1_x11  2778  0.62  0.65  0.52   0.39  4.8 1.1\nq1_x12- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nq1_x25  2786  0.50  0.52  0.29   0.22  4.9 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x1  0.03 0.05 0.11 0.28 0.34 0.20 0.01\nq1_x4  0.27 0.32 0.19 0.13 0.07 0.03 0.01\nq1_x11 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nq1_x12 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nq1_x25 0.02 0.04 0.06 0.17 0.32 0.39 0.01\n\n\nこちらのデータでもうまくできました。"
  }
]