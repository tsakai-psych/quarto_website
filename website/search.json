[
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  psych,\n  microbenchmark\n)"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#demo-data",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#demo-data",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "demo data",
    "text": "demo data\npsych::bfiがちょうどいいので、逆転処理などをしておいてデモデータにします。\n\n\nCode\ndf_bfi &lt;- \n  psych::bfi |&gt;\n  relocate(\n    gender, education, age\n  ) |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  mutate(\n    across(\n      .cols = all_of(\n        unlist(bfi.keys) |&gt;\n          str_subset(pattern = \"^-\") |&gt; # extract reverse item\n          str_remove(pattern = \"^-\")\n      ),\n      .fns = \\(x) {7 - x} # six point scale, so subtract from seven.\n    )\n  ) |&gt;\n1  rename(\n    q_education = education,\n    q_age = age,\n    q_gender = gender\n  ) |&gt;\n  as_tibble() # for better printing\n\n\n\n1\n\nわざわざ列名を変えなくてもいいのですが、tidyselect::starts_with()でいちいちignore.case = TRUEとしないとageとA*、educationとE*の区別をつけてもらえなくてめんどくさいので、変えておきます。\n\n\n\n\n\ndf_bfi\n\n# A tibble: 2,800 × 29\n   id    q_gender q_education q_age    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 61617        1          NA    16     5     4     3     4     4     2     3     3     3     3     4     4\n 2 61618        2          NA    18     5     4     5     2     5     5     4     4     4     3     6     6\n 3 61620        2          NA    17     2     4     5     4     4     4     5     4     5     2     5     3\n 4 61621        2          NA    17     3     4     6     5     5     4     4     3     2     2     2     4\n 5 61622        1          NA    17     5     3     3     4     5     4     4     5     4     5     5     5\n 6 61623        2           3    21     1     6     5     6     5     6     6     6     6     4     5     6\n 7 61624        1          NA    18     5     5     5     3     5     5     4     4     5     4     3     4\n 8 61629        1           2    19     3     3     1     5     1     3     2     4     5     3     4     1\n 9 61630        1           1    19     3     3     6     3     3     6     6     3     3     2     2     4\n10 61633        2          NA    17     5     5     6     6     5     6     5     6     5     6     5     5\n# ℹ 2,790 more rows\n# ℹ 13 more variables: E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;,\n#   O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#rowwise",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#rowwise",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "rowwise()",
    "text": "rowwise()\n心理学のデータ処理で因子分析とα係数の確認を終えたら、次に行うのは尺度得点の計算だと思います。参加者ごとに下位尺度の得点を算出していくのですが、その場合は調べてみるとたいていはdplyr::rowise()1とdplyr::c_across()2を使った処理に行き着くと思います。つまり、rowwise()で行ごとにグルーピングして、c_acrossでまとめたい要素をまとめて、それで処理するという方法です。\n\ndf_bfi |&gt;\n  rowwise() |&gt;\n  mutate(\n    score_A = mean(c_across(starts_with(\"A\"))),\n    score_C = mean(c_across(starts_with(\"C\"))),\n    score_E = mean(c_across(starts_with(\"E\"))),\n    score_N = mean(c_across(starts_with(\"N\"))),\n    score_O = mean(c_across(starts_with(\"O\"))),\n    .after = q_age\n  ) |&gt;\n  ungroup()\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれで処理できるのでそれはそれでいいのですが、いくつか気になる点があります。\n\ndplyr::ungroup()が必要\nungroup()を忘れるとrowwiseグループが維持されるので、その後の処理で面倒なことが起こる可能性があります。\n行数によっては遅い\n行数が少なければ気にならないと思いますが、行数が多いデータだとrowwise()処理は体感できるレベルで遅いです。今回の2800行のデータだと4秒程度かかります。\n\n\nmicrobenchmark::microbenchmark(\n  \"rowwise\" = {\n    df_bfi |&gt;\n      rowwise() |&gt;\n      mutate(\n        score_A = mean(c_across(starts_with(\"A\"))),\n        score_C = mean(c_across(starts_with(\"C\"))),\n        score_E = mean(c_across(starts_with(\"E\"))),\n        score_N = mean(c_across(starts_with(\"N\"))),\n        score_O = mean(c_across(starts_with(\"O\"))),\n        .after = q_age\n      ) |&gt;\n      ungroup()\n  },\n1  times = 5L\n)\n\n\n1\n\nレンダリングの際にこの処理だけであまりにも時間がかかるので、ベンチマークの反復はデフォルトの100回から5回に減らしました。\n\n\n\n\nUnit: seconds\n    expr      min       lq     mean   median       uq      max neval\n rowwise 4.379653 4.452662 4.606178 4.665492 4.692358 4.840726     5\n\n\nというわけで、別のやり方がないか模索したわけです。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#by-argument",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#by-argument",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": ".by argument",
    "text": ".by argument\nグルーピングに.by3引数を用いるやり方です。dplyr::group_by()やrowwiseは関数としてパイプフローに組み込んで、処理の後もグルーピングを維持するのに対して.by = .../by = ...は処理の関数の引数で設定し、その処理限りのグルーピングを行います。戻ってくるdataframeはグループ化されていないので、個人的にはその後の処理がやりやすい感じがしてよく使っています。変数選択にはtidy-selectの文法が使えます。\nなお、.by引数に突っ込めるrowwise()ってある？という質問がPosit Communityのforumに投げられているのですが、悲しいことに回答なしでclosedになっています。おそらく現時点ではそのようなものは実装されてないみたいなので、質問者の方が提示しているように.byには一意のID列を設定すればいいのだと思います。 そして、このID列を.byに入れたときは実質rowwise処理になるので、なんとc_across()がちゃんと動きます。\n\ndf_bfi |&gt;\n  mutate(\n      score_A = mean(c_across(starts_with(\"A\"))),\n      score_C = mean(c_across(starts_with(\"C\"))),\n      score_E = mean(c_across(starts_with(\"E\"))),\n      score_N = mean(c_across(starts_with(\"N\"))),\n      score_O = mean(c_across(starts_with(\"O\"))),\n      .after = q_age,\n      .by = id\n    )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\n先ほどのrowwise()の書き方と全く同じ結果が返ってきています。ID列をちゃんと作ってあれば、.by引数にそれを入れることでもできるわけです。 ただし、実はこの方法も時間がかかる処理で、rowwise()のときと同じくらいの処理時間がかかります。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \".by\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = mean(c_across(starts_with(\"A\"))),\n        score_C = mean(c_across(starts_with(\"C\"))),\n        score_E = mean(c_across(starts_with(\"E\"))),\n        score_N = mean(c_across(starts_with(\"N\"))),\n        score_O = mean(c_across(starts_with(\"O\"))),\n        .after = q_age,\n        .by = id\n      )\n  },\n  times = 5L # for saving time\n)\n\n\nUnit: seconds\n expr      min       lq     mean   median       uq     max neval\n  .by 4.452133 4.503107 4.554612 4.581422 4.597017 4.63938     5"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#rowmeanspick...",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#rowmeanspick...",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "rowMeans(pick(...))",
    "text": "rowMeans(pick(...))\n上記の.by引数に突っ込めるrowwise()みたいな関数ってないのかな～と探していたときに、たまたまこんな記事を見つけました。\n\nRow-wise means in dplyr\n\nbase::rowMeans()にdplyr::pick()4で列を選択して入れるという技です。pick()はmutate()やdplyr::summrise()のような関数の中でtidy-selectの文法を使ってdataframe列を選択できる関数です。pick()の戻り値がdataframeであること、rowMeans()は引数にdataframeも入れられること、rowMeans()の戻り値は各行の値の平均値を収めたベクトルであることを利用して、rowwise()を使わずに実質的にrowwise処理をしてしまおうというわけですね。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = rowMeans(pick(starts_with(\"A\"))),\n    score_C = rowMeans(pick(starts_with(\"C\"))),\n    score_E = rowMeans(pick(starts_with(\"E\"))),\n    score_N = rowMeans(pick(starts_with(\"N\"))),\n    score_O = rowMeans(pick(starts_with(\"O\"))),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれもまた今までの書き方と同じ結果が返ってきています。mean()がrowMeansに、c_across()がpick()に変わっただけなので、コードの可読性も悪くない気がします。\nそして処理速度ですが、上記２つに比べてとても速いです。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = rowMeans(pick(starts_with(\"A\"))),\n        score_C = rowMeans(pick(starts_with(\"C\"))),\n        score_E = rowMeans(pick(starts_with(\"E\"))),\n        score_N = rowMeans(pick(starts_with(\"N\"))),\n        score_O = rowMeans(pick(starts_with(\"O\"))),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n           expr    min    lq     mean median    uq    max neval\n base::rowMeans 4.3955 4.775 5.446678 5.0946 5.985 9.3616   100\n\n\n上記2つは単位が秒だったのに、こちらの単位はミリ秒です。つまり、4ミリ秒程度で処理が終わっています。\nちなみに、もし各項目の合計得点が尺度得点である場合は、base::rowSums()を使えばいいです。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#applymargin-1",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#applymargin-1",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "apply(MARGIN = 1)",
    "text": "apply(MARGIN = 1)\nそういえば、行での計算はbase::apply(MARGIN = 1)でもできるのを思い出しました。引数には同じくpick()で選んだ列を入れて、関数にmean()を選択すれば同じ結果が得られるはずです。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = apply(\n      pick(starts_with(\"A\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_C = apply(\n      pick(starts_with(\"C\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_E = apply(\n      pick(starts_with(\"E\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_N = apply(\n      pick(starts_with(\"N\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    score_O = apply(\n      pick(starts_with(\"O\")),\n      MARGIN = 1,\n      FUN = mean\n    ),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 34\n   id    q_gender q_education q_age score_A score_C score_E score_N score_O    A1    A2    A3    A4    A5\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 61617        1          NA    16     4       2.8     3.8     2.8     3       5     4     3     4     4\n 2 61618        2          NA    18     4.2     4       5       3.8     4       5     4     5     2     5\n 3 61620        2          NA    17     3.8     4       4.2     3.6     4.8     2     4     5     4     4\n 4 61621        2          NA    17     4.6     3       3.6     2.8     3.2     3     4     6     5     5\n 5 61622        1          NA    17     4       4.4     4.8     3.2     3.6     5     3     3     4     5\n 6 61623        2           3    21     4.6     5.6     5.6     3       5       1     6     5     6     5\n 7 61624        1          NA    18     4.6     4.4     4.2     1.4     5.4     5     5     5     3     5\n 8 61629        1           2    19     2.6     3.4     2.4     4.2     4.2     3     3     1     5     1\n 9 61630        1           1    19     3.6     4      NA       3.6     5       3     3     6     3     3\n10 61633        2          NA    17     5.4     5.6     4.8     4.2     5.2     5     5     6     6     5\n# ℹ 2,790 more rows\n# ℹ 20 more variables: C1 &lt;int&gt;, C2 &lt;int&gt;, C3 &lt;int&gt;, C4 &lt;dbl&gt;, C5 &lt;dbl&gt;, E1 &lt;dbl&gt;, E2 &lt;dbl&gt;, E3 &lt;int&gt;,\n#   E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nこれまでの方法と同じ結果が返ってきました。apply()は引数MARGINに入れるのって0or1だっけ1or2だっけ？どっちが行でどっちが列だっけ？となるので、あんまり使ってないです。（覚えればいいんですけど指定は1or2で、1が行に対して、2が列に対しての計算です。）\n処理速度に関してはどうでしょうか。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"apply_margin1\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = apply(\n          pick(starts_with(\"A\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_C = apply(\n          pick(starts_with(\"C\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_E = apply(\n          pick(starts_with(\"E\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_N = apply(\n          pick(starts_with(\"N\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        score_O = apply(\n          pick(starts_with(\"O\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n          expr     min       lq    mean median      uq      max neval\n apply_margin1 69.1538 72.07505 75.1989 73.648 76.3195 178.5305   100\n\n\nbfiデータだと70ミリ秒くらいで済むみたいです。rowMeans()よりは遅いですが、rowwise()と.byよりは速いみたいですね。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#comparison",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#comparison",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "Comparison",
    "text": "Comparison\nせっかくなので処理速度を一度に比べてみます。ただし、このままbfiデータでベンチマークするととんでもない時間がかかるので、もっと数が少ないirisデータで比較します。\n\ndf_iris &lt;- iris |&gt;\n  as_tibble() |&gt; # for better printing\n  rowid_to_column() # for equal results\n\nhead(df_iris)\n\n# A tibble: 6 × 6\n  rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n  &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1     1          5.1         3.5          1.4         0.2 setosa \n2     2          4.9         3            1.4         0.2 setosa \n3     3          4.7         3.2          1.3         0.2 setosa \n4     4          4.6         3.1          1.5         0.2 setosa \n5     5          5           3.6          1.4         0.2 setosa \n6     6          5.4         3.9          1.7         0.4 setosa \n\n\n\n\nCode\nres &lt;- microbenchmark::microbenchmark(\n  \"rowwise\" = {\n    df_iris |&gt;\n      rowwise() |&gt;\n      mutate(\n        sepal = mean(c_across(starts_with(\"Sepal\"))),\n        petal = mean(c_across(starts_with(\"Petal\")))\n      ) |&gt;\n      ungroup()\n  },\n  \".by\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = mean(c_across(starts_with(\"Sepal\"))),\n        petal = mean(c_across(starts_with(\"Petal\"))),\n        .by = rowid\n      )\n  },\n  \"rowMeans\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = rowMeans(pick(starts_with(\"Sepal\"))),\n        petal = rowMeans(pick(starts_with(\"Petal\")))\n      )\n  },\n  \"apply_margin1\" = {\n    df_iris |&gt;\n      mutate(\n        sepal = apply(\n          pick(starts_with(\"Sepal\")),\n          MARGIN = 1,\n          FUN = mean\n        ),\n        petal = apply(\n          pick(starts_with(\"Petal\")),\n          MARGIN = 1,\n          FUN = mean\n        )\n      )\n  },\n  check = \"equal\"\n)\n\nres\n\n\nUnit: milliseconds\n          expr     min       lq      mean  median       uq      max neval cld\n       rowwise 83.6199 87.56705 92.048610 90.2374 93.21040 198.1570   100 a  \n           .by 81.9493 85.52320 89.220028 87.6496 90.60810 108.6058   100  b \n      rowMeans  1.4587  1.56730  1.722859  1.6520  1.73630   4.0320   100   c\n apply_margin1  2.8520  2.99130  3.212039  3.0942  3.26395   5.5633   100   c\n\n\n\nautoplot(res)\n\n\n\n\n\n\n\n\n圧倒的にrowMeans()が速いです。桁が違います。元記事の比較データだと、10000行のデータでもrowMeans()を使う方法が圧倒的に速いです。dplyrのarticle（Row-wise operations）にも書いてありましたが、速さを求めるならこっちと言うのもわかります。base関数はやはり侮れません。apply()も行数が少なければ高速っぽいです。\n\n\n\n\n\n\n番外編：Rfast::rowmeans()\n\n\n\n\n\n同じノリでRfast::rowmeans()もイケるか…？と思って試してみました。Rfast5パッケージの関数は基本はmatrixかつNAなしじゃないと使えない（内部で使ってるCppの都合でNAがあると良くないとのこと）のですが、Helpを見る限りrowmeans()はdataframeでも計算してくれそうな感じっぽいので試してみました。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = Rfast::rowmeans(pick(starts_with(\"A\")))\n  )\n\nError in `mutate()`:\nℹ In argument: `score_A = Rfast::rowmeans(pick(starts_with(\"A\")))`.\nCaused by error:\n! Not compatible with requested type: [type=list; target=double].\n\n\nダメでした。ということで、無理やりmatrixにして再挑戦します。\n\ndf_bfi |&gt;\n  mutate(\n    score_A = pick(starts_with(\"A\")) |&gt;\n      as.matrix() |&gt;\n      Rfast::rowmeans(),\n    .after = q_age\n  )\n\n# A tibble: 2,800 × 30\n   id    q_gender q_education q_age score_A    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1\n   &lt;chr&gt;    &lt;int&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 61617        1          NA    16     4       5     4     3     4     4     2     3     3     3     3     4\n 2 61618        2          NA    18     4.2     5     4     5     2     5     5     4     4     4     3     6\n 3 61620        2          NA    17     3.8     2     4     5     4     4     4     5     4     5     2     5\n 4 61621        2          NA    17     4.6     3     4     6     5     5     4     4     3     2     2     2\n 5 61622        1          NA    17     4       5     3     3     4     5     4     4     5     4     5     5\n 6 61623        2           3    21     4.6     1     6     5     6     5     6     6     6     6     4     5\n 7 61624        1          NA    18     4.6     5     5     5     3     5     5     4     4     5     4     3\n 8 61629        1           2    19     2.6     3     3     1     5     1     3     2     4     5     3     4\n 9 61630        1           1    19     3.6     3     3     6     3     3     6     6     3     3     2     2\n10 61633        2          NA    17     5.4     5     5     6     6     5     6     5     6     5     6     5\n# ℹ 2,790 more rows\n# ℹ 14 more variables: E2 &lt;dbl&gt;, E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;,\n#   N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\nできました。でも、わざわざmatrixにしなきゃいけいないのはめんどくさいです。\nちなみに、bfiデータの場合、rowMeans()の方法と処理時間には差がつきません。同じくらい速いです。\n\n\nCode\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = rowMeans(pick(starts_with(\"A\"))),\n        score_C = rowMeans(pick(starts_with(\"C\"))),\n        score_E = rowMeans(pick(starts_with(\"E\"))),\n        score_N = rowMeans(pick(starts_with(\"N\"))),\n        score_O = rowMeans(pick(starts_with(\"O\"))),\n        .after = q_age\n      )\n  },\n  \"Rfast::rowmeans\" = {\n    df_bfi |&gt;\n      mutate(\n        score_A = pick(starts_with(\"A\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_C = pick(starts_with(\"C\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_E = pick(starts_with(\"E\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_N = pick(starts_with(\"N\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        score_O = pick(starts_with(\"O\")) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans(),\n        .after = q_age\n      )\n  }\n)\n\n\nUnit: milliseconds\n            expr    min      lq     mean median     uq    max neval cld\n  base::rowMeans 4.2664 4.34500 4.555280 4.4065 4.5439 8.7891   100   a\n Rfast::rowmeans 4.2167 4.28615 4.673185 4.3685 4.5172 8.2862   100   a\n\n\nggplot2::diamondsは約54000行あるんですが、計算させてみるとrowMeans()よりも微妙に速かったりします。ただし1ミリ秒も差がつかないので体感できないです。\n\nmicrobenchmark::microbenchmark(\n  \"base::rowMeans\" = {\n    diamonds |&gt;\n      mutate(\n        res = pick(depth:z) |&gt;\n          rowMeans()\n      )\n  },\n  \"Rfast::rowmeans\" = {\n    diamonds |&gt;\n      mutate(\n        res = pick(depth:z) |&gt;\n          as.matrix() |&gt;\n          Rfast::rowmeans()\n      )\n  },\n  check = \"equal\"\n)\n\nUnit: milliseconds\n            expr    min      lq     mean  median      uq    max neval cld\n  base::rowMeans 2.2124 2.37665 2.779288 2.51615 2.70125 5.3981   100  a \n Rfast::rowmeans 1.5674 1.70840 2.082682 1.83450 2.08915 6.0799   100   b\n\n\nものすごくデータ数が多いときなら、選択肢に入る方法かもしれません。"
  },
  {
    "objectID": "posts/20251102_rowwise_wo_rowwise/index.html#footnotes",
    "href": "posts/20251102_rowwise_wo_rowwise/index.html#footnotes",
    "title": "dplyr::rowwiseを使わないで個人の尺度得点を算出する",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://dplyr.tidyverse.org/reference/rowwise.html↩︎\nhttps://dplyr.tidyverse.org/reference/c_across.html↩︎\nhttps://dplyr.tidyverse.org/reference/dplyr_by.html↩︎\nhttps://dplyr.tidyverse.org/reference/pick.html↩︎\nhttps://github.com/RfastOfficial/Rfast?tab=readme-ov-file#readme↩︎"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html",
    "href": "posts/20251025_map_alpha/index.html",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  psych\n)"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすいデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすいデータの場合",
    "text": "やりやすいデータの場合\nまず、処理しやすいようにデータをlong型にします。ついでにtibble::rownames_to_column()でrownameをid列に変えておきます。id列がないとtidyr::pivot_wider()出来なくなるので、少なくともその処理の前にはやっておきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n1    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  )\n\n\n1\n\n引数colsはtidy-selectの文法なので-c(id, gender, education, age)でもいいです。longにしたい列名が「文字1文字数字1文字」なのがわかっているので、dplyr::matches()で正規表現を使って絞りました。\n\n\n\n\n# A tibble: 70,000 × 6\n   id    gender education   age items value\n   &lt;chr&gt;  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n 1 61617      1        NA    16 A1        2\n 2 61617      1        NA    16 A2        4\n 3 61617      1        NA    16 A3        3\n 4 61617      1        NA    16 A4        4\n 5 61617      1        NA    16 A5        4\n 6 61617      1        NA    16 C1        2\n 7 61617      1        NA    16 C2        3\n 8 61617      1        NA    16 C3        3\n 9 61617      1        NA    16 C4        4\n10 61617      1        NA    16 C5        4\n# ℹ 69,990 more rows\n\n\n次に、nest用の列を作ってnestします。今回は列名に下位尺度が入っているので、それを抽出すれば簡単にnest用の列が完成です。やりやすいデータとはこのことを言っています。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n1    nest_key = str_extract(items, \"^.\") |&gt;\n2      tolower()\n  ) |&gt;\n3  nest(.by = nest_key)\n\n\n1\n\nitems（列名のベクトル）の各要素から最初の一文字だけ抜ければいいので、stringr::str_extract()でいいです。\n\n2\n\nあとでbfi.keysから抽出しやすくすために、小文字にします。\n\n3\n\nnestに使った列以外の残りは、引数.keysを指定しなければdata列にまとまります。\n\n\n\n\n# A tibble: 5 × 2\n  nest_key data                 \n  &lt;chr&gt;    &lt;list&gt;               \n1 a        &lt;tibble [14,000 × 6]&gt;\n2 c        &lt;tibble [14,000 × 6]&gt;\n3 e        &lt;tibble [14,000 × 6]&gt;\n4 n        &lt;tibble [14,000 × 6]&gt;\n5 o        &lt;tibble [14,000 × 6]&gt;\n\n\nnestされたデータを処理します。data列はlistなので、中の各要素を処理したいときはpurrr::map()が使えます。data列の各要素はデータフレームで、しかもaの行はAで始まる項目だけ、cの行はCで始まる項目だけ、eの行は…のlongデータになっています。そのため、map()の中でwide型に直して必要列以外取り除き、psych::alpha()に入れてあげればいいわけです。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n1    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n2            keys = bfi.keys[[bfi_key_name]] |&gt;\n3              str_subset(pattern = \"^-\") |&gt;\n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n4      set_names(bfi_key_name),\n5    .by = nest_key\n  )\n\n\n1\n\nalpha()の中でbfi.keysの要素名を使えるようにしたいので、ここで抜き出しておきます。\n\n2\n\nまさに上の処理で抜き出した要素名をここで使います。\n\n3\n\n引数keysは、逆転する項目の項目名を文字列ベクトルで入れるか、逆転する項目は-1、そのままの項目は1にした数値ベクトルを入れます。今回は前者で入れるので、bfi.keysの各要素（文字列ベクトル）のうち、stringr::str_subset()を使って-で始まる項目だけを抽出して、str_remove()で-を取り除きます。\n\n4\n\nmap()の戻り値のlistに名前を付けたいのでmap() |&gt; purrr::set_names()とします。nestデータのほかの列の要素を使えるので、dplyr::group_map() |&gt; set_names()よりもいい気がします。\n\n5\n\n引数.byにnest_key列を指定して、実質rowwise処理をします。これを指定することで、(2)の処理で[[bfi_key_name]]のところにbfi_key_name列の要素が1つだけ入ります。これがないと、(2)の処理で要素5の文字列ベクトルc（\"agree\", \"conscientious\", ...）が[[の中に入ってしまうのでエラーになります。\n\n\n\n\n# A tibble: 5 × 4\n  nest_key data                  bfi_key_name  res_alpha   \n  &lt;chr&gt;    &lt;list&gt;                &lt;chr&gt;         &lt;named list&gt;\n1 a        &lt;tibble [14,000 × 6]&gt; agree         &lt;psych&gt;     \n2 c        &lt;tibble [14,000 × 6]&gt; conscientious &lt;psych&gt;     \n3 e        &lt;tibble [14,000 × 6]&gt; extraversion  &lt;psych&gt;     \n4 n        &lt;tibble [14,000 × 6]&gt; neuroticism   &lt;psych&gt;     \n5 o        &lt;tibble [14,000 × 6]&gt; openness      &lt;psych&gt;     \n\n\n最後に結果が詰まったres_alpha列だけ取り出します。データフレームからある1列の要素を取り出すときはdplyr::pull()が使えます。res_alpha列はmap()を使って作ったので、各下位尺度のα係数が入ったlistが返ってきます。\n\nbfi |&gt;\n  rownames_to_column(var = \"id\") |&gt;\n  pivot_longer(\n    cols = matches(\"\\\\w\\\\d\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    nest_key = str_extract(items, \"^.\") |&gt;\n      tolower()\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n  mutate(\n    bfi_key_name = str_subset(\n      names(bfi.keys),\n      pattern = paste0(\"^\", nest_key)\n    ),\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n         x |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(matches(\"\\\\w\\\\d\")) |&gt;\n          psych::alpha(\n            keys = bfi.keys[[bfi_key_name]] |&gt; \n              str_subset(pattern = \"^-\") |&gt; \n              str_remove(pattern = \"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(bfi_key_name),\n    .by = nest_key\n  ) |&gt;\n1  pull(res_alpha)\n\n\n1\n\npullは最後に作られた列を返すので、実はpull()だけでもres_alpha列を引っ張ってこれます。\n\n\n\n\n$agree\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nA1-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nA2       0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\nA3       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nA4       0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nA5       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nA1- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nA2  2773  0.73  0.75  0.67   0.56  4.8 1.2\nA3  2774  0.76  0.77  0.71   0.59  4.6 1.3\nA4  2781  0.65  0.63  0.47   0.39  4.7 1.5\nA5  2784  0.69  0.70  0.59   0.49  4.6 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nA1 0.33 0.29 0.14 0.12 0.08 0.03 0.01\nA2 0.02 0.05 0.05 0.20 0.37 0.31 0.01\nA3 0.03 0.06 0.07 0.20 0.36 0.27 0.01\nA4 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nA5 0.02 0.07 0.09 0.22 0.35 0.25 0.01\n\n$conscientious\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nC1       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nC2       0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nC3       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nC4-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\nC5-      0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nC1  2779  0.65  0.67  0.54   0.45  4.5 1.2\nC2  2776  0.70  0.71  0.60   0.50  4.4 1.3\nC3  2780  0.66  0.67  0.54   0.46  4.3 1.3\nC4- 2774  0.74  0.73  0.64   0.55  4.4 1.4\nC5- 2784  0.72  0.68  0.57   0.48  3.7 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nC1 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nC2 0.03 0.09 0.11 0.23 0.35 0.20 0.01\nC3 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nC4 0.28 0.29 0.17 0.16 0.08 0.02 0.01\nC5 0.18 0.20 0.12 0.22 0.17 0.10 0.01\n\n$extraversion\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nE1-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nE2-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nE3       0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nE4       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\nE5       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nE1- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nE2- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nE3  2775  0.68  0.70  0.58   0.50  4.0 1.4\nE4  2791  0.75  0.75  0.66   0.58  4.4 1.5\nE5  2779  0.64  0.66  0.52   0.45  4.4 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nE1 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nE2 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nE3 0.05 0.11 0.15 0.30 0.27 0.13 0.01\nE4 0.05 0.09 0.10 0.16 0.34 0.26 0.00\nE5 0.03 0.08 0.10 0.22 0.34 0.22 0.01\n\n$neuroticism\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nN1      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nN2      0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nN3      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nN4      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nN5      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n      n raw.r std.r r.cor r.drop mean  sd\nN1 2778  0.80  0.80  0.76   0.67  2.9 1.6\nN2 2779  0.79  0.79  0.75   0.65  3.5 1.5\nN3 2789  0.81  0.81  0.74   0.67  3.2 1.6\nN4 2764  0.72  0.71  0.60   0.54  3.2 1.6\nN5 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nN1 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nN2 0.12 0.19 0.15 0.26 0.18 0.10 0.01\nN3 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nN4 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nN5 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$openness\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(x, names_from = items, values_from = value), \n    matches(\"\\\\w\\\\d\")), keys = str_remove(str_subset(bfi.keys[[bfi_key_name]], \n    pattern = \"^-\"), pattern = \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nO2-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\nO5-      0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean  sd\nO1  2778  0.62  0.65  0.52   0.39  4.8 1.1\nO2- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nO3  2772  0.67  0.69  0.59   0.45  4.4 1.2\nO4  2786  0.50  0.52  0.29   0.22  4.9 1.2\nO5- 2780  0.67  0.66  0.52   0.42  4.5 1.3\n\nNon missing response frequency for each item\n      1    2    3    4    5    6 miss\nO1 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nO2 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nO3 0.03 0.05 0.11 0.28 0.34 0.20 0.01\nO4 0.02 0.04 0.06 0.17 0.32 0.39 0.01\nO5 0.27 0.32 0.19 0.13 0.07 0.03 0.01\n\n\nということで、元のdfからmapを使って一気にα係数を求めることができました。一つのnamed listに下位尺度5つ分の出力が詰まっているので、オブジェクトに入れた際にGrobal Env.がオブジェクトだらけにならないというのはいい点かもしれないです。"
  },
  {
    "objectID": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "href": "posts/20251025_map_alpha/index.html#やりやすくなさそうなデータの場合",
    "title": "purrr::mapを使ってpsych::alphaを一気に処理する",
    "section": "やりやすくなさそうなデータの場合",
    "text": "やりやすくなさそうなデータの場合\nbfiデータは列名に下位尺度の分類が含まれていたのでやりやすかったのですが、実際に扱うデータだとそうはいかない場合もあると思います。というわけで、bfiを少しいじってこんなデータを用意してみました。\n\n\n\n\n\n\n改造の処理はこちら\n\n\n\n\n\nbfiの列をデモグラフィック列（26-28）以外ランダムに並べ替えるために、列番号をシャッフルします。\n\nset.seed(20251025)\n\n(vec_col_order &lt;- c(sample(1:25), 26:28))\n\n [1] 23  7 13 25  1  3  5 10 17  8 21 22 15 16  4 19  6 18  9 20 12 11  2 14 24\n[26] 26 27 28\n\n\n後でkeyを作る用のベクトルを作成。チェック用に元のbfiの列名を名前に付けておきます。\n\nvec_names_df_test &lt;- c(\n  paste0(\"q1_x\", 1:25),\n  paste0(\"q2_x\", 1:3)\n) |&gt;\n  setNames(\n    colnames(bfi[, vec_col_order])\n  )\n\nvec_names_df_test\n\n       O3        C2        E3        O5        A1        A3        A5        C5 \n  \"q1_x1\"   \"q1_x2\"   \"q1_x3\"   \"q1_x4\"   \"q1_x5\"   \"q1_x6\"   \"q1_x7\"   \"q1_x8\" \n       N2        C3        O1        O2        E5        N1        A4        N4 \n  \"q1_x9\"  \"q1_x10\"  \"q1_x11\"  \"q1_x12\"  \"q1_x13\"  \"q1_x14\"  \"q1_x15\"  \"q1_x16\" \n       C1        N3        C4        N5        E2        E1        A2        E4 \n \"q1_x17\"  \"q1_x18\"  \"q1_x19\"  \"q1_x20\"  \"q1_x21\"  \"q1_x22\"  \"q1_x23\"  \"q1_x24\" \n       O4    gender education       age \n \"q1_x25\"   \"q2_x1\"   \"q2_x2\"   \"q2_x3\" \n\n\n列番号をシャッフルしたベクトルを使って、列を並び替えたdfを作ります。ついでにid列もつけておきます。\n\ndf_test &lt;- bfi[, vec_col_order] |&gt;\n  `colnames&lt;-`(vec_names_df_test) |&gt;\n  rownames_to_column(var = \"id\")\n\ncolnames(df_test)\n\n [1] \"id\"     \"q1_x1\"  \"q1_x2\"  \"q1_x3\"  \"q1_x4\"  \"q1_x5\"  \"q1_x6\"  \"q1_x7\" \n [9] \"q1_x8\"  \"q1_x9\"  \"q1_x10\" \"q1_x11\" \"q1_x12\" \"q1_x13\" \"q1_x14\" \"q1_x15\"\n[17] \"q1_x16\" \"q1_x17\" \"q1_x18\" \"q1_x19\" \"q1_x20\" \"q1_x21\" \"q1_x22\" \"q1_x23\"\n[25] \"q1_x24\" \"q1_x25\" \"q2_x1\"  \"q2_x2\"  \"q2_x3\" \n\n\n構成要素の分類を示すkeyを作ります。実際の場合はnamed listを自力で作成すればいいと思います（それか、psych::make.keys()あたりを使うか）。今回は手入力したくないのでゴリ押します。\n\nlist_test_keys &lt;-\n  names(bfi.keys) |&gt;\n  str_extract(pattern = \"^.\") |&gt;\n  toupper() |&gt;\n  set_names() |&gt;\n  map(\n    .f = \\(x){\n      temp_vec &lt;- vec_names_df_test[1:25]\n      temp_vec[str_starts(names(temp_vec), x)]\n    }\n  ) |&gt;\n  map(\n    .f = \\(x) {\n      if_else(\n        names(x) %in% unlist(bfi.keys),\n        x,\n        paste0(\"-\", x) |&gt;\n          set_names(names(x))\n      )\n    }\n  )\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\n手入力の方がむしろ省コードだし楽だろ！！という指摘は今回はなかったことにします。\nちなみに逆転項目の設定はちゃんとできています。\n\ndata.frame(\n  enframe(\n    unlist(list_test_keys)\n  ) |&gt;\n    arrange(name),\n  enframe(\n    unlist(bfi.keys)\n  )\n)\n\n   name   value         name.1 value.1\n1  A.A1  -q1_x5         agree1     -A1\n2  A.A2  q1_x23         agree2      A2\n3  A.A3   q1_x6         agree3      A3\n4  A.A4  q1_x15         agree4      A4\n5  A.A5   q1_x7         agree5      A5\n6  C.C1  q1_x17 conscientious1      C1\n7  C.C2   q1_x2 conscientious2      C2\n8  C.C3  q1_x10 conscientious3      C3\n9  C.C4 -q1_x19 conscientious4     -C4\n10 C.C5  -q1_x8 conscientious5     -C5\n11 E.E1 -q1_x22  extraversion1     -E1\n12 E.E2 -q1_x21  extraversion2     -E2\n13 E.E3   q1_x3  extraversion3      E3\n14 E.E4  q1_x24  extraversion4      E4\n15 E.E5  q1_x13  extraversion5      E5\n16 N.N1  q1_x14   neuroticism1      N1\n17 N.N2   q1_x9   neuroticism2      N2\n18 N.N3  q1_x18   neuroticism3      N3\n19 N.N4  q1_x16   neuroticism4      N4\n20 N.N5  q1_x20   neuroticism5      N5\n21 O.O1  q1_x11      openness1      O1\n22 O.O2 -q1_x12      openness2     -O2\n23 O.O3   q1_x1      openness3      O3\n24 O.O4  q1_x25      openness4      O4\n25 O.O5  -q1_x4      openness5     -O5\n\n\n\n\n\n元のデータと行数列数は一緒ですが、列名と順番が変わりました。コードブックの作成が必須ですね。\n\nstr(df_test)\n\n'data.frame':   2800 obs. of  29 variables:\n $ id    : chr  \"61617\" \"61618\" \"61620\" \"61621\" ...\n $ q1_x1 : int  3 4 5 4 4 5 5 4 6 5 ...\n $ q1_x2 : int  3 4 5 4 4 6 4 2 6 5 ...\n $ q1_x3 : int  3 6 4 4 5 6 4 4 NA 4 ...\n $ q1_x4 : int  3 3 2 5 3 1 1 3 1 2 ...\n $ q1_x5 : int  2 2 5 4 2 6 2 4 4 2 ...\n $ q1_x6 : int  3 5 5 6 3 5 5 1 6 6 ...\n $ q1_x7 : int  4 5 4 5 5 5 5 1 3 5 ...\n $ q1_x8 : int  4 4 5 5 2 3 3 4 5 1 ...\n $ q1_x9 : int  4 3 5 5 3 5 2 3 5 5 ...\n $ q1_x10: int  3 4 4 3 5 6 4 4 3 6 ...\n $ q1_x11: int  3 4 4 3 3 4 5 3 6 5 ...\n $ q1_x12: int  6 2 2 3 3 3 2 2 6 1 ...\n $ q1_x13: int  4 3 5 4 5 6 5 1 3 5 ...\n $ q1_x14: int  3 3 4 2 2 3 1 6 5 5 ...\n $ q1_x15: int  4 2 4 5 4 6 3 5 3 6 ...\n $ q1_x16: int  2 5 2 4 4 2 1 6 3 2 ...\n $ q1_x17: int  2 5 4 4 4 6 5 3 6 6 ...\n $ q1_x18: int  2 3 4 2 4 2 2 2 2 5 ...\n $ q1_x19: int  4 3 2 5 3 1 2 2 4 2 ...\n $ q1_x20: int  3 5 3 1 3 3 1 4 3 4 ...\n $ q1_x21: int  3 1 4 3 2 1 3 6 3 2 ...\n $ q1_x22: int  3 1 2 5 2 2 4 3 5 2 ...\n $ q1_x23: int  4 4 4 4 3 6 5 3 3 5 ...\n $ q1_x24: int  4 4 4 4 4 5 5 2 4 5 ...\n $ q1_x25: int  4 3 5 3 3 6 6 5 6 5 ...\n $ q2_x1 : int  1 2 2 2 1 2 1 1 1 2 ...\n $ q2_x2 : int  NA NA NA NA NA 3 NA 2 1 NA ...\n $ q2_x3 : int  16 18 17 17 17 21 18 19 19 17 ...\n\n\nそして構成要素の分類のkeyも用意しました。要素のベクトルに名前がついているのはチェックのためで、本来はないと思ってください。QualtricsとかMicrosoft Formsとかには項目をランダムに提示する機能があるので、それを使えば列をこんなにシャッフルする必要はないんですが、「よくわからなかったので、尺度の項目の順番を自分で頑張ってシャッフルしちゃいました…」という事案はあると思います。\n\nlist_test_keys\n\n$A\n      A1       A3       A5       A4       A2 \n\"-q1_x5\"  \"q1_x6\"  \"q1_x7\" \"q1_x15\" \"q1_x23\" \n\n$C\n       C2        C5        C3        C1        C4 \n  \"q1_x2\"  \"-q1_x8\"  \"q1_x10\"  \"q1_x17\" \"-q1_x19\" \n\n$E\n       E3        E5        E2        E1        E4 \n  \"q1_x3\"  \"q1_x13\" \"-q1_x21\" \"-q1_x22\"  \"q1_x24\" \n\n$N\n      N2       N1       N4       N3       N5 \n \"q1_x9\" \"q1_x14\" \"q1_x16\" \"q1_x18\" \"q1_x20\" \n\n$O\n       O3        O5        O1        O2        O4 \n  \"q1_x1\"  \"-q1_x4\"  \"q1_x11\" \"-q1_x12\"  \"q1_x25\" \n\n\nというわけで、こちらのデータでもmap()でalpha()を一気に処理してみようと思います。先ほどはデータセットの列名に下位尺度の分類が入っていたので処理が少なく済みましたが、今回はそうではないので処理が少し増えます。\nまずlong型にした後、新たにマッチ用の列を作って逆転項目に-をつけます。（別にわざわざ新しい列を作らなくてもいいのですが、alpha()の出力のときに項目名の前後に-がつくのが気になるので、あえてマッチ用の列を作っています。）\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    )\n  )\n\n# A tibble: 70,000 × 7\n   id    q2_x1 q2_x2 q2_x3 items  value match_items\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;      \n 1 61617     1    NA    16 q1_x1      3 q1_x1      \n 2 61617     1    NA    16 q1_x2      3 q1_x2      \n 3 61617     1    NA    16 q1_x3      3 q1_x3      \n 4 61617     1    NA    16 q1_x4      3 -q1_x4     \n 5 61617     1    NA    16 q1_x5      2 -q1_x5     \n 6 61617     1    NA    16 q1_x6      3 q1_x6      \n 7 61617     1    NA    16 q1_x7      4 q1_x7      \n 8 61617     1    NA    16 q1_x8      4 -q1_x8     \n 9 61617     1    NA    16 q1_x9      4 q1_x9      \n10 61617     1    NA    16 q1_x10     3 q1_x10     \n# ℹ 69,990 more rows\n\n\n次に、マッチ用の列をdplyr::case_match()に入れて、分類用のリストと照合します。case_match()は、第一引数.xにマッチさせたいベクトル、以降はtwo-sided formula形式で処理を書いていきます。左側（LHS）はマッチさせたい要素、右側（RHS）にはマッチしたものに対する出力を入れます。このformulaの部分だけは頑張って書かないといけません。今回は分類リストがnamed listでその要素は文字列ベクトルなので、各要素にマッチしたらその要素のリスト名（＝下位尺度の分類）を返すようにします。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\",\n    )\n  )\n\n# A tibble: 70,000 × 8\n   id    q2_x1 q2_x2 q2_x3 items  value match_items nest_key\n   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;   \n 1 61617     1    NA    16 q1_x1      3 q1_x1       O       \n 2 61617     1    NA    16 q1_x2      3 q1_x2       C       \n 3 61617     1    NA    16 q1_x3      3 q1_x3       E       \n 4 61617     1    NA    16 q1_x4      3 -q1_x4      O       \n 5 61617     1    NA    16 q1_x5      2 -q1_x5      A       \n 6 61617     1    NA    16 q1_x6      3 q1_x6       A       \n 7 61617     1    NA    16 q1_x7      4 q1_x7       A       \n 8 61617     1    NA    16 q1_x8      4 -q1_x8      C       \n 9 61617     1    NA    16 q1_x9      4 q1_x9       N       \n10 61617     1    NA    16 q1_x10     3 q1_x10      C       \n# ℹ 69,990 more rows\n\n\n後の処理は先ほどと変わりません。nestしたうえでmap処理をしていけばいいです。\n\ndf_test |&gt;\n  pivot_longer(\n    cols = starts_with(\"q1\"),\n    names_to = \"items\"\n  ) |&gt;\n  mutate(\n    match_items = if_else(\n      items %in% unlist(list_test_keys),\n      items,\n      str_c(\"-\", items)\n    ),\n    nest_key = case_match(\n      match_items,\n      list_test_keys[[\"A\"]] ~ \"A\",\n      list_test_keys[[\"C\"]] ~ \"C\",\n      list_test_keys[[\"E\"]] ~ \"E\",\n      list_test_keys[[\"N\"]] ~ \"N\",\n      list_test_keys[[\"O\"]] ~ \"O\"\n    )\n  ) |&gt;\n  nest(.by = nest_key) |&gt;\n1  arrange(nest_key) |&gt;\n  mutate(\n    res_alpha = map(\n      .x = data,\n      .f = \\(x) {\n        x |&gt;\n2          select(-match_items) |&gt;\n          pivot_wider(\n            names_from = items,\n            values_from = value\n          ) |&gt;\n          select(starts_with(\"q1\")) |&gt;\n          psych::alpha(\n            keys = list_test_keys[[nest_key]] |&gt;\n              str_subset(\"^-\") |&gt;\n              str_remove(\"^-\")\n          )\n      }\n    ) |&gt;\n      set_names(nest_key),\n    .by = nest_key\n  ) |&gt;\n  pull(res_alpha)\n\n\n1\n\ndplyrとかで実装されている引数.byはデータを出現順に並べるので、データによっては出力がきれいな順番（A, B, C, … ）にならないです。なので見やすくするためにdplyr::arrange()でソートしておきます。\n\n2\n\nマッチ用の列は残しておくとwide型にしたときにデータがズレるので外しておきます。マッチ用の列を作っていなかったら省略してOKです。\n\n\n\n\n$A\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n       0.7      0.71    0.68      0.33 2.5 0.009  4.7 0.9     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.69   0.7  0.72\nDuhachek  0.69   0.7  0.72\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x5-      0.72      0.72    0.67      0.40 2.6   0.0087 0.0065  0.38\nq1_x6       0.60      0.61    0.56      0.28 1.6   0.0124 0.0094  0.32\nq1_x7       0.64      0.66    0.60      0.32 1.9   0.0111 0.0125  0.34\nq1_x15      0.69      0.69    0.65      0.36 2.3   0.0098 0.0157  0.37\nq1_x23      0.62      0.63    0.58      0.29 1.7   0.0119 0.0168  0.29\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x5- 2784  0.58  0.57  0.38   0.31  4.6 1.4\nq1_x6  2774  0.76  0.77  0.71   0.59  4.6 1.3\nq1_x7  2784  0.69  0.70  0.59   0.49  4.6 1.3\nq1_x15 2781  0.65  0.63  0.47   0.39  4.7 1.5\nq1_x23 2773  0.73  0.75  0.67   0.56  4.8 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x5  0.33 0.29 0.14 0.12 0.08 0.03 0.01\nq1_x6  0.03 0.06 0.07 0.20 0.36 0.27 0.01\nq1_x7  0.02 0.07 0.09 0.22 0.35 0.25 0.01\nq1_x15 0.05 0.08 0.07 0.16 0.24 0.41 0.01\nq1_x23 0.02 0.05 0.05 0.20 0.37 0.31 0.01\n\n$C\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.73      0.73    0.69      0.35 2.7 0.0081  4.3 0.95     0.34\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.71  0.73  0.74\nDuhachek  0.71  0.73  0.74\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x2        0.67      0.68    0.62      0.34 2.1   0.0099 0.0056  0.34\nq1_x8-       0.69      0.69    0.63      0.36 2.2   0.0096 0.0017  0.35\nq1_x10       0.69      0.69    0.64      0.36 2.3   0.0096 0.0070  0.36\nq1_x17       0.69      0.70    0.64      0.36 2.3   0.0093 0.0037  0.35\nq1_x19-      0.65      0.66    0.60      0.33 2.0   0.0107 0.0037  0.32\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x2   2776  0.70  0.71  0.60   0.50  4.4 1.3\nq1_x8-  2784  0.72  0.68  0.57   0.48  3.7 1.6\nq1_x10  2780  0.66  0.67  0.54   0.46  4.3 1.3\nq1_x17  2779  0.65  0.67  0.54   0.45  4.5 1.2\nq1_x19- 2774  0.74  0.73  0.64   0.55  4.4 1.4\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x2  0.03 0.09 0.11 0.23 0.35 0.20 0.01\nq1_x8  0.18 0.20 0.12 0.22 0.17 0.10 0.01\nq1_x10 0.03 0.09 0.11 0.27 0.34 0.17 0.01\nq1_x17 0.03 0.06 0.10 0.24 0.37 0.21 0.01\nq1_x19 0.28 0.29 0.17 0.16 0.08 0.02 0.01\n\n$E\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.76      0.76    0.73      0.39 3.2 0.007  4.1 1.1     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.75  0.76  0.78\nDuhachek  0.75  0.76  0.78\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x3        0.73      0.73    0.67      0.40 2.7   0.0082 0.0071  0.40\nq1_x13       0.74      0.74    0.69      0.42 2.9   0.0078 0.0043  0.42\nq1_x21-      0.69      0.69    0.63      0.36 2.3   0.0095 0.0028  0.35\nq1_x22-      0.73      0.73    0.67      0.40 2.6   0.0084 0.0044  0.38\nq1_x24       0.70      0.70    0.65      0.37 2.4   0.0091 0.0033  0.38\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x3   2775  0.68  0.70  0.58   0.50  4.0 1.4\nq1_x13  2779  0.64  0.66  0.52   0.45  4.4 1.3\nq1_x21- 2784  0.78  0.76  0.69   0.61  3.9 1.6\nq1_x22- 2777  0.72  0.70  0.59   0.52  4.0 1.6\nq1_x24  2791  0.75  0.75  0.66   0.58  4.4 1.5\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x3  0.05 0.11 0.15 0.30 0.27 0.13 0.01\nq1_x13 0.03 0.08 0.10 0.22 0.34 0.22 0.01\nq1_x21 0.19 0.24 0.12 0.22 0.14 0.09 0.01\nq1_x22 0.24 0.23 0.15 0.16 0.13 0.09 0.01\nq1_x24 0.05 0.09 0.10 0.16 0.34 0.26 0.00\n\n$N\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r\n      0.81      0.81     0.8      0.47 4.4 0.0056  3.2 1.2     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt      0.8  0.81  0.82\nDuhachek   0.8  0.81  0.82\n\n Reliability if an item is dropped:\n       raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x9       0.76      0.76    0.72      0.45 3.2   0.0073 0.0054  0.41\nq1_x14      0.76      0.76    0.71      0.44 3.1   0.0075 0.0061  0.41\nq1_x16      0.80      0.80    0.77      0.49 3.9   0.0064 0.0181  0.49\nq1_x18      0.76      0.76    0.73      0.44 3.1   0.0077 0.0178  0.39\nq1_x20      0.81      0.81    0.79      0.52 4.3   0.0059 0.0137  0.53\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nq1_x9  2779  0.79  0.79  0.75   0.65  3.5 1.5\nq1_x14 2778  0.80  0.80  0.76   0.67  2.9 1.6\nq1_x16 2764  0.72  0.71  0.60   0.54  3.2 1.6\nq1_x18 2789  0.81  0.81  0.74   0.67  3.2 1.6\nq1_x20 2771  0.68  0.67  0.53   0.49  3.0 1.6\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x9  0.12 0.19 0.15 0.26 0.18 0.10 0.01\nq1_x14 0.24 0.24 0.15 0.19 0.12 0.07 0.01\nq1_x16 0.17 0.24 0.15 0.22 0.14 0.09 0.01\nq1_x18 0.18 0.23 0.13 0.21 0.16 0.09 0.00\nq1_x20 0.24 0.24 0.14 0.18 0.12 0.09 0.01\n\n$O\n\nReliability analysis   \nCall: psych::alpha(x = select(pivot_wider(select(x, -match_items), \n    names_from = items, values_from = value), starts_with(\"q1\")), \n    keys = str_remove(str_subset(list_test_keys[[nest_key]], \n        \"^-\"), \"^-\"))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n       0.6      0.61    0.57      0.24 1.5 0.012  4.6 0.81     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.58   0.6  0.62\nDuhachek  0.58   0.6  0.62\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nq1_x1        0.50      0.50    0.44      0.20 1.0    0.015 0.0071  0.20\nq1_x4-       0.51      0.53    0.47      0.22 1.1    0.015 0.0116  0.20\nq1_x11       0.53      0.53    0.48      0.22 1.1    0.014 0.0092  0.23\nq1_x12-      0.57      0.57    0.51      0.25 1.3    0.013 0.0076  0.22\nq1_x25       0.61      0.62    0.56      0.29 1.6    0.012 0.0044  0.29\n\n Item statistics \n           n raw.r std.r r.cor r.drop mean  sd\nq1_x1   2772  0.67  0.69  0.59   0.45  4.4 1.2\nq1_x4-  2780  0.67  0.66  0.52   0.42  4.5 1.3\nq1_x11  2778  0.62  0.65  0.52   0.39  4.8 1.1\nq1_x12- 2800  0.65  0.60  0.43   0.33  4.3 1.6\nq1_x25  2786  0.50  0.52  0.29   0.22  4.9 1.2\n\nNon missing response frequency for each item\n          1    2    3    4    5    6 miss\nq1_x1  0.03 0.05 0.11 0.28 0.34 0.20 0.01\nq1_x4  0.27 0.32 0.19 0.13 0.07 0.03 0.01\nq1_x11 0.01 0.04 0.08 0.22 0.33 0.33 0.01\nq1_x12 0.29 0.26 0.14 0.16 0.10 0.06 0.00\nq1_x25 0.02 0.04 0.06 0.17 0.32 0.39 0.01\n\n\nこちらのデータでもうまくできました。"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "LICENSE",
    "section": "",
    "text": "MIT License\nCopyright 2025 Takuto SAKAI\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content",
    "section": "",
    "text": "大体の投稿がRについての個人的な備忘録とかメモです。\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nR 確認的因子分析の表を作ってみた、1パイプラインで\n\n\n\nR memo\n\npreprocess\n\nanalysis\n\n\n\n1パイプラインで完結することの実用性？多分ないです。\n\n\n\n\n\n2026-01-18\n\n\n\n\n\n\n\n\n\n\n\n\nR 標準で用意されている色の名前\n\n\n\nR memo\n\nvisualization\n\n\n\ncolors()で出せる、Rが組み込みで用意している色の名前についてのメモ。\n\n\n\n\n\n2026-01-02\n\n\n\n\n\n\n\n\n\n\n\n\nRでデータの国名を日本語名にする\n\n\n\nR memo\n\npreprocess\n\n\n\ncountrycodeパッケージやcountriesパッケージを使って、データに含まれている国名を日本語名にしてみた。\n\n\n\n\n\n2025-12-13\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr::rowwiseを使わないで個人の尺度得点を算出する\n\n\n\nR memo\n\npreprocess\n\n\n\ndplyr::rowwise()を使わずに、個人の尺度得点を算出する方法を試してみた。rowMeans(pick(...))が速くていいかも。\n\n\n\n\n\n2025-11-02\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける\n\n\n\nR memo\n\nvisualization\n\n\n\nggplot2でもExcelみたいに片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつけてみた。\n\n\n\n\n\n2025-10-26\n\n\n\n\n\n\n\n\n\n\n\n\npurrr::mapを使ってpsych::alphaを一気に処理する\n\n\n\nR memo\n\npreprocess\n\nanalysis\n\n\n\npurrr::map()の中でpsych::alpha()を使って、元のdfから一気にα係数の処理をしてみた。\n\n\n\n\n\n2025-10-25\n\n\n\n\n\n\n\n\n\n\n\n\n個人ウェブサイトを立ち上げてみました\n\n\n\nannouncement\n\n\n\nQuartoの機能を使って、個人のウェブサイトを作ってみました。\n\n\n\n\n\n2025-10-19\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "名前\n酒井拓人 (SAKAI, Takuto)\n\n\n現所属\n\n愛知学院大学心理学部 実験助手（教員）\n\n（教員）までが正式な職位です。\n\n\n\n\n研究領域\n\n言い方や言葉遣いが印象にどのような影響を与えるかについて興味があります。\n\n自分は心理学科で育ったので心理学の人だと思っているのですが、やっている内容はなんかそうじゃない気もします。\n\n\n\n\nLinks\n\nreseachmap.jp\n\n業績などはresearchmapにまとめてあります。学会発表のポスター（pdf）もなるべくアップロードするようにしています。\n\n  https://orcid.org/0009-0004-0736-5229 \n\nE-mailアドレスはORCIDに掲載しています。\n\nOSF\n\n一部の発表資料はOSFに置いています。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "さかいの備忘録",
    "section": "",
    "text": "Welcom to my website!\n社会心理学者・社会言語学者（？）の酒井拓人のウェブサイトです。\nX (Twitter)で発信できる量にも限界がある（特にRのコード）ので、ウェブサイトを作ってみました。\n\nAbout：私について\nContent：大体の投稿がRの個人的なメモです。"
  },
  {
    "objectID": "posts/20251019_start_website/index.html",
    "href": "posts/20251019_start_website/index.html",
    "title": "個人ウェブサイトを立ち上げてみました",
    "section": "",
    "text": "Quartoの機能を使って個人のウェブサイトを作ってみました。\nRのメモについては、今までX（Twitter）にコードをスクショしたものを上げていたのですが、それもちょっと不便に感じてきたので、これからはコードについてはこのウェブサイトで公開しようと思います。"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#problem",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Problem",
    "text": "Problem\nまず、データをテキトーに用意します。実験データを読み込んで集計した後だと思ってください。\n\ntest_df &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(3, 3, 4, 5),\n    sd = rep(1.5, 4),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c              3   1.5   1.5   4.5\n2 a          d              3   1.5   1.5   4.5\n3 b          c              4   1.5   2.5   5.5\n4 b          d              5   1.5   3.5   6.5\n\n\nyminは誤差範囲の下側、ymaxは誤差範囲の上側の数値です。実際のデータではdplyr::reframe()やdplyr::summarise()の中で、ggplot2::mean_sdl(..., mult = 1)とすれば、平均値と平均値±1SDの値が一発で得られます。\n\n\n\n\n\n\nggplot2::mean_sdl()の例\n\n\n\n\n\n\niris |&gt;\n  reframe(\n    check_mean = mean(Sepal.Length),\n    check_sd = sd(Sepal.Length),\n    ggplot2::mean_sdl(Sepal.Length, mult = 1),\n    .by = Species\n  )\n\n     Species check_mean  check_sd     y     ymin     ymax\n1     setosa      5.006 0.3524897 5.006 4.653510 5.358490\n2 versicolor      5.936 0.5161711 5.936 5.419829 6.452171\n3  virginica      6.588 0.6358796 6.588 5.952120 7.223880\n\n\nmean_sdl()のHelpを見るとわかりますが、もともとはHmisc::smean.sdl()なのでその引数（mult、na.rm）が使えます。引数multは標準偏差を何倍して平均値に加えるかなので1でいいです。\n\n\n\n普通にggplotで作るならこんな感じでしょうか。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5,\n    position = position_dodge(.2)\n  ) +\n  geom_line(position = position_dodge(.2)) +\n  geom_point(\n    aes(shape = group_line),\n    position = position_dodge(.2),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  )\n\n\n\n\n\n\n\n\n普段の自分で使う用のグラフならこれでいいです。ただ、誤差範囲が両側なのと、Excelでは（簡単に）できない位置ずらしがあるので、答え合わせ用としてはよくないですね。というわけで、位置ずらしせず誤差範囲を片側にしたグラフをggplotでも作りたいわけです。"
  },
  {
    "objectID": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "href": "posts/20251026_ggplot_onesided_errorbar/index.html#solution",
    "title": "ggplot2で片側だけ（上にあるやつは上、下にあるやつは下）にエラーバーをつける",
    "section": "Solution",
    "text": "Solution\nまず、ggplot2::geom_errorbar()は要素としてyminとymaxが必須です。なので、例えば上下のどちらかを消そうとしてaes()の中で省いてしまうとエラーになります。\n\ntest_df |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  geom_errorbar(\n    aes(ymax = ymax)\n  )\n\nError in `geom_errorbar()`:\n! Problem while setting up geom.\nℹ Error occurred in the 1st layer.\nCaused by error in `compute_geom_1()`:\n! `geom_errorbar()` requires the following missing aesthetics: ymin or\n  xmin and xmax.\n\n\nということで、yminとymaxに適切な値を設定する必要があります。\nまず、横軸の水準ごとに値が小さい方がTRUEを返す列を作り、値が小さい方はymaxがNAに、値が大きい方はyminがNAになるようにします。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n1    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  )\n\n\n1\n\n横軸の水準ごとになので、ここの.byは必須。それかgroup_by() |&gt; mutate() |&gt; ungroup()。\n\n\n\n\n# A tibble: 4 × 7\n  group_line group_axis     y    sd  ymin  ymax is_smaller\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;     \n1 a          c              3   1.5   1.5  NA   TRUE      \n2 a          d              3   1.5   1.5  NA   TRUE      \n3 b          c              4   1.5  NA     5.5 FALSE     \n4 b          d              5   1.5  NA     6.5 FALSE     \n\n\nこの状態でgeom_errorbar()を使うとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, NA),\n    ymax = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n片側だけにひげができていますが、線分が消えてしまっています。ではNAの代わりにyの値を入れるとどうなるでしょうか。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  )\n\n\n\n\n\n\n\n\n線分の表示が戻りました。一見よさそうに見えるのですが、これをマーカーと合わせるとこうなります。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_errorbar(\n    aes(ymin = ymin, ymax = ymax),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nマーカーにひげが重なってしまい見栄えが悪いです。ということで、geom::errorbar()にはyを渡すよりはNAを渡す方が作りたいグラフに近いことになります。では、同じことをgeom_linerange()でやるとどうなるでしょう。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin = if_else(is_smaller, ymin, y),\n    ymax = if_else(!is_smaller, ymax, y)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin, ymax = ymax),\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    size = 3,\n    fill = \"white\"\n  )\n\n\n\n\n\n\n\n\nこちらは線分が表示されていますね。\nということは、geom_errorbar()にNAを入れて片方のひげだけ作り、geom_linerange()にyを入れて線分を描いてもらえば解決しそうな気がします。というわけで、geom_errorbar()用の参照列とgeom_linerange()用の参照列を作ってグラフを描いてもらいます。\n\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n求めていたものができました。後はいろいろ調整すれば完成です。\n\n\nCode\ntest_df |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3,\n    fill = \"white\"\n  ) +\n  scale_shape_manual(\n    values = c(19, 21)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 8),\n    expand = expansion()\n  )\n\n\n\n\n\n\n\n\n\nこんな風に微妙にクロスしているデータでもうまくいきます。\n\ntest_df_cross &lt;-\n  tibble(\n    group_line = rep(letters[1:2], each = 2),\n    group_axis = rep(letters[3:4], times = 2),\n    y = c(2.1, 2, 2, 4),\n    sd = c(1.7, 1, .5, .2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_cross\n\n# A tibble: 4 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          c            2.1   1.7   0.4   3.8\n2 a          d            2     1     1     3  \n3 b          c            2     0.5   1.5   2.5\n4 b          d            4     0.2   3.8   4.2\n\n\n\n\nCode\ntest_df_cross |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcelで同じことをする場合\n\n\n\n\n\nちなみに、Excelで誤差範囲を片側にしているときにデータがクロスしている場合、誤差範囲の値の片方を負の値にする必要があります。\n\n\n\n平均値\n水準c\n水準d\n\n\n\n\n条件A\n2.1\n2\n\n\n条件B\n2\n4\n\n\n\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n1.7\n1\n\n\n条件B\n0.5\n0.2\n\n\n\nこんな感じでグラフ作成用にデータを配置していた場合、以下のように標準偏差の方のどちらかの水準の列を負の値にすると、誤差範囲の向きが反転します。\n\n\n\n標準偏差\n水準c\n水準d\n\n\n\n\n条件A\n-1.7\n1\n\n\n条件B\n-0.5\n0.2\n\n\n\n\n\n\n横軸の要因が2要因以上あっても大丈夫です。\n\nset.seed(20251026)\n\ntest_df_2x4 &lt;- \n  tibble(\n    group_line = rep(letters[1:2], each = 4),\n    group_axis = rep(LETTERS[1:4], times = 2),\n    y = runif(8, min = 3, max = 5),\n    sd = runif(8, min = .5, max = 2),\n    ymin = y - sd,\n    ymax = y + sd\n  )\n\ntest_df_2x4\n\n# A tibble: 8 × 6\n  group_line group_axis     y    sd  ymin  ymax\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a          A           3.65 1.89   1.76  5.54\n2 a          B           3.88 1.77   2.11  5.65\n3 a          C           4.90 0.816  4.08  5.71\n4 a          D           4.51 1.38   3.13  5.88\n5 b          A           3.98 1.25   2.73  5.22\n6 b          B           4.79 1.03   3.76  5.82\n7 b          C           3.30 1.39   1.91  4.70\n8 b          D           4.50 0.681  3.82  5.18\n\n\n\n\nCode\ntest_df_2x4 |&gt;\n  mutate(\n    is_smaller = rank(y, ties.method = \"first\") == 1,\n    .by = group_axis\n  ) |&gt;\n  mutate(\n    ymin_lr = if_else(is_smaller, ymin, y),\n    ymin_eb = if_else(is_smaller, ymin, NA),\n    ymax_lr = if_else(!is_smaller, ymax, y),\n    ymax_eb = if_else(!is_smaller, ymax, NA)\n  ) |&gt;\n  ggplot(aes(x = group_axis, y = y, group = group_line)) +\n  theme_classic() +\n  geom_linerange(\n    aes(ymin = ymin_lr, ymax = ymax_lr),\n    alpha = .5\n  ) +\n  geom_errorbar(\n    aes(ymin = ymin_eb, ymax = ymax_eb),\n    width = .2,\n    alpha = .5\n  ) +\n  geom_line() +\n  geom_point(\n    aes(shape = group_line),\n    size = 3\n  )"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html",
    "href": "posts/20251213_localize_country_name/index.html",
    "title": "Rでデータの国名を日本語名にする",
    "section": "",
    "text": "サムネイルはcopilotに作ってもらいました。"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#packages",
    "href": "posts/20251213_localize_country_name/index.html#packages",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Packages",
    "text": "Packages\n\npacman::p_load(\n  tidyverse,\n  countrycode,\n  countries\n)"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#contents",
    "href": "posts/20251213_localize_country_name/index.html#contents",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Contents",
    "text": "Contents\n\nBackground\n補助に入っている授業で、学生たちはオープンデータを使ってRの練習をしています。使用しているデータはYamada et al. (2021)1によるCOVIDiSTRESS Global Surveyデータセットです。データセットはOSFで公開されています。そのうち最終データ（COVIDiSTRESS_May_30_cleaned.csv）を利用しています。\n\ndf_covid &lt;- read_csv(\n  list.files(\"~/datasets/Yamada_etal_2020\", full.names = TRUE)\n)\n\ndf_covid\n\n# A tibble: 125,306 × 154\n      ID answered_all Duration..in.seconds. RecordedDate        UserLanguage Dem_age Dem_gender     \n   &lt;dbl&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n 1     1 No                             180 2020-05-30 23:47:17 SAR               29 Female         \n 2     2 No                            3100 2020-05-29 23:30:15 UR                20 Male           \n 3     3 No                             127 2020-05-30 22:40:15 SAR               47 Female         \n 4     4 No                            1710 2020-05-29 22:47:17 BG                79 Male           \n 5     5 No                            2239 2020-05-29 22:42:30 SAR               61 Female         \n 6     6 Yes                           1221 2020-05-29 21:25:09 IT                68 Male           \n 7     7 Yes                           1283 2020-05-29 21:25:37 SAR               29 Other/would ra…\n 8     8 Yes                           1442 2020-05-29 21:25:17 SAR               38 Female         \n 9     9 No                            1952 2020-05-29 21:18:48 SME               35 Female         \n10    10 No                             144 2020-05-30 20:24:41 SAR               23 Female         \n# ℹ 125,296 more rows\n# ℹ 147 more variables: Dem_edu &lt;chr&gt;, Dem_edu_mom &lt;chr&gt;, Dem_employment &lt;chr&gt;, Country &lt;chr&gt;,\n#   Dem_Expat &lt;chr&gt;, Dem_state &lt;chr&gt;, Dem_maritalstatus &lt;chr&gt;, Dem_dependents &lt;dbl&gt;,\n#   Dem_riskgroup &lt;chr&gt;, Dem_isolation &lt;chr&gt;, Dem_isolation_adults &lt;dbl&gt;, Dem_isolation_kids &lt;dbl&gt;,\n#   AD_gain &lt;chr&gt;, AD_loss &lt;chr&gt;, AD_check &lt;chr&gt;, Scale_PSS10_UCLA_1 &lt;dbl&gt;,\n#   Scale_PSS10_UCLA_2 &lt;dbl&gt;, Scale_PSS10_UCLA_3 &lt;dbl&gt;, Scale_PSS10_UCLA_4 &lt;dbl&gt;,\n#   Scale_PSS10_UCLA_5 &lt;dbl&gt;, Scale_PSS10_UCLA_6 &lt;dbl&gt;, Scale_PSS10_UCLA_7 &lt;dbl&gt;, …\n\n\n学生たちは課題として推しグラフを1枚作るのですが、そのときにどこの国とどこの国を比較すると面白いかということで、国を選ぶ作業があります。データには参加者の居住国（Country）が英語で収められているのですが、学生たちが国を選ぶときに日本語化されていると見やすいかなと思ったのが今回の背景です。\n実は、この事象は去年の段階で遭遇していて、テキトーにメモを取っておいてそのままにしておいたのですが、今年もまた需要が出てきたので、改めて備忘録として残しておこうということになりました。\n\n\nRの備忘録授業のお手伝いしてるときに、国名一覧をオブジェクトに入れてくれてるパッケージないかな～とか思ってたんですが、やっぱりありました ⇒ {countrycode}日本語含むいろんな言語が収録されてて便利https://t.co/CQC7C0MqPphttps://t.co/kDYHaZsOL5 pic.twitter.com/rpcXU8ru51\n\n— Takuto SAKAI (@tsakai_psych) June 16, 2024\n\n\nまずdplyr::count()でCountry列を集計してみます。\n\ndf_covid |&gt;\n  count(Country, sort = TRUE) |&gt;\n  filter(!Country %in% c(\"other\", NA)) |&gt;\n  rmarkdown::paged_table() # for better view on website\n\n\n  \n\n\n\nどうやらフィンランドからの回答が一番多いようです。ちなみに、私の環境ではreadr::read_csv()でlocaleを特に設定せずデータを読み込むと、コートジボワール（n = 6）だけ文字化けします。\n\nunique(df_covid$Country) |&gt;\n  sort() |&gt;\n  _[1] |&gt;\n  assign(\"cotedivoire\", value = _) |&gt; \n  print()\n\n[1] \"C\\xf4te d\\x92Ivoire\"\n\n\nstringi::stri_enc_detect()を使って文字コードを推測します。\n\n1stringi::stri_enc_detect(cotedivoire) |&gt;\n  _[[1]] |&gt;\n  mutate(\n    result = str_conv(cotedivoire, encoding = Encoding),\n    .by = Encoding\n  )\n\n\n1\n\nstri_enc_detect()は引数strに入れた要素の数に合わせたdf入りlistを返すみたいです。今回は1個だけなので[[で抽出してdfにしておきます。\n\n\n\n\n      Encoding Language Confidence          result\n1 windows-1252       fr       0.64 Côte d’Ivoire\n2 windows-1250       ro       0.64 Côte d’Ivoire\n3     UTF-16BE                0.10     䏴瑥⁤鉉癯楲�\n4     UTF-16LE                0.10    整搠䦒潶物�\n5    Shift_JIS       ja       0.10    Ce d棚voire\n6      GB18030       zh       0.10   C魌e d扞voire\n7         Big5       zh       0.10    C矌e dvoire\n\n\nこの結果を見る限り、windows-1252でエンコードすればよさそうですね。というわけで、国名と人数だけのdfを作ります。\n\ndf_country_name &lt;-\n  df_covid |&gt;\n  count(Country, sort = TRUE) |&gt;\n  filter(!Country %in% c(\"other\", NA)) |&gt;\n  mutate(\n    Country = if_else(\n      str_detect(Country, \"voire$\"),\n      str_conv(Country, encoding = \"windows-1252\"),\n      Country\n    )\n  )\n\ndf_country_name\n\n# A tibble: 176 × 2\n   Country       n\n   &lt;chr&gt;     &lt;int&gt;\n 1 Finland   22933\n 2 France    13475\n 3 Denmark   10891\n 4 Mexico     9169\n 5 Lithuania  8255\n 6 Argentina  5923\n 7 Japan      5072\n 8 Bulgaria   4785\n 9 Poland     3088\n10 Sweden     3055\n# ℹ 166 more rows\n\n\n\n\ncountrycode::countrycode\ncountrycode::countrycode()2を使って国名を日本語にします。countrycode::countryname()という関数もあるんですが、こちらの方がsaferみたいなことがHelpに記載されていたのでこちらを使うことにします。\n\nsourcevar：国のコードもしくは国名が含まれているベクトル。\norigin：sourcevarのコーディングスキーム。\ndestination: 変換したいコーディングスキーム。\n\nベクトルで入れた場合は第1要素から順にマッチングしていく。\n\nwarn: デフォルトでTRUE。マッチしなかったものについて教えてくれる。\n\n日本語の国名は、cldr.name.ja、cldr.short.ja、cldr.variant.jaの三種類あります。とりあえずcldr.name.jaにしておきます。\n\ndf_country_name |&gt;\n  mutate(\n    res = countrycode::countrycode(\n      Country,\n      origin = \"country.name\",\n      destination = \"cldr.name.ja\"\n    )\n  ) |&gt;\n  rmarkdown::paged_table() # for better view on website\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `res = countrycode::countrycode(Country, origin = \"country.name\", destination =\n  \"cldr.name.ja\")`.\nCaused by warning:\n! Some values were not matched unambiguously: Micronesia, Federated States of, Sudan, South\n\n\n\n  \n\n\n\nミクロネシア連邦と南スーダン以外は日本語化できました。なお、南スーダンは元のデータでは\"Sudan, South\"で入っていますが、\"South Sudan\"でマッチさせると日本語化できます。\n\ncountrycode::countrycode(\n  \"South Sudan\",\n  origin = \"country.name\",\n  destination = \"cldr.name.ja\"\n)\n\n[1] \"南スーダン\"\n\n\ncoutrycode::codelistで変換テーブルを見てみると、ミクロネシア連邦については日本語名が用意されていませんでした。\n\ncountrycode::codelist |&gt;\n  filter(str_detect(country.name.en, \"Micronesia\")) |&gt;\n  select(country.name.en, ends_with(\"ja\"))\n\n# A tibble: 1 × 4\n  country.name.en                  cldr.name.ja cldr.short.ja cldr.variant.ja\n  &lt;chr&gt;                            &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;          \n1 Micronesia (Federated States of) &lt;NA&gt;         &lt;NA&gt;          &lt;NA&gt;           \n\n\ncoutrycode::countrycode()には引数custom_matchが用意されていて、ここに名前付きベクトルでoriginとdestinationのマッチング指定すれば、元の結果を上書きできます。\n\ndf_country_name |&gt;\n  filter(Country %in% c(\"Sudan, South\", \"Micronesia, Federated States of\")) |&gt;\n  mutate(\n    res = countrycode::countrycode(\n      Country,\n      origin = \"country.name\",\n      destination = \"cldr.name.ja\",\n      custom_match = c(\n        \"Sudan, South\" = \"南スーダン\",\n        \"Micronesia, Federated States of\" = \"ミクロネシア連邦\"\n      )\n    )\n  )\n\n# A tibble: 2 × 3\n  Country                             n res             \n  &lt;chr&gt;                           &lt;int&gt; &lt;chr&gt;           \n1 Micronesia, Federated States of     3 ミクロネシア連邦\n2 Sudan, South                        1 南スーダン      \n\n\n一応この関数だけで事足りるのですが、もう一つパッケージを見つけたのでそちらも使ってみます。\n\n\ncountries::country_name\ncountries::country_name()3も似たような感じで使えます。こちらは変換先だけ指定すればいいです。\n\nx: 国名のベクトル。\nto: 変換先。\n\n日本語の場合は\"name_ja\"を指定します。\n\nverbose: TRUEにしておくと、正確・ファジーにマッチした要素の数やマッチしなかったものについて教えてくれます。\n\n\ndf_country_name |&gt;\n  mutate(\n    res = countries::country_name(\n      Country,\n      to = \"name_ja\",\n      verbose = TRUE\n    )\n  ) |&gt;\n  rmarkdown::paged_table()\n\n\nIn total 176 unique country names were provided\n173/176 have been matched with EXACT matching\n3/176 have been matched with FUZZY matching\n\n\nThe following country IDs do not have a match in one or more of the requested naming conventions,  NA returned:\n(To avoid NAs, use - to = 'simple'- or set - na_fill = TRUE)\n  - Kosovo\n\n\n\n  \n\n\n\nこちらはコソボがヒットしませんでした。countries::country_reference_list(_long)で変換テーブルを見てみると、他のほとんどの言語でもコソボは用意されていないようです。\n\ncountries::country_reference_list |&gt;\n1  rowid_to_column() |&gt;\n  filter(if_any(.cols = everything(), .fns = ~str_detect(.x, \"Kosovo\"))) |&gt;\n  pull(rowid) |&gt;\n  (\\(x) {\n    countries::country_reference_list_long |&gt;\n      filter(ID == x)\n  }) ()\n\n\n1\n\ncountry_reference_listの行番号がcountry_reference_list_longのIDに対応しているので、行番号をつけておきます。\n\n\n\n\n# A tibble: 10 × 3\n   ID    nomenclature name            \n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;           \n 1 349   simple       Kosovo          \n 2 349   Name0        Kosovo          \n 3 349   Name1        kosovo          \n 4 349   Name2        XKO             \n 5 349   Name3        Kosovo2         \n 6 349   Name4        UNMIK/Kosovo    \n 7 349   Name5        UVK             \n 8 349   Name6        XK              \n 9 349   Name7        Република Косово\n10 349   Name8        UNK             \n\n\nこちらの関数もマッチングテーブルを修正して、マッチしない国名を修正することができます4。ただ、ちょっとめんどくさいかも。\n\n# create matching table\ntemp_df_match &lt;- countries::match_table(\n  df_country_name$Country,\n  to = \"name_ja\"\n)\n\nSome country IDs have no match in one or more of the requested country naming conventions, NA returned.\n\n# search Kosovo\ntemp_df_match |&gt;\n  rowid_to_column() |&gt;\n  filter(list_countries == \"Kosovo\")\n\n  rowid list_countries name_ja\n1    86         Kosovo    &lt;NA&gt;\n\n# correct\ntemp_df_match$name_ja[86] &lt;- \"コソボ\"\n\ndf_country_name |&gt;\n  filter(Country == \"Kosovo\") |&gt;\n  mutate(\n    res = countries::country_name(\n      Country,\n      to = \"name_ja\",\n      custom_table = temp_df_match\n    )\n  )\n\n# A tibble: 1 × 3\n  Country     n res   \n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt; \n1 Kosovo   2707 コソボ"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#solution",
    "href": "posts/20251213_localize_country_name/index.html#solution",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Solution",
    "text": "Solution\n2つの関数でNAになった国が被っていないので、今回はこの2つを使えばやりたいことが達成できます。\n\ndf_country_name |&gt;\n  mutate(\n    temp_name_ja_1 = countrycode::countrycode(\n      Country,\n      origin = \"country.name\",\n      destination = \"cldr.name.ja\"\n    ),\n    temp_name_ja_2 = countries::country_name(\n      Country,\n      to = \"name_ja\"\n    ),\n    res_name_ja = if_else(\n      is.na(temp_name_ja_1),\n      temp_name_ja_2,\n      temp_name_ja_1\n    ),\n    .after = Country\n  ) |&gt;\n  select(-starts_with(\"temp\")) |&gt;\n  rmarkdown::paged_table() # for better view on website\n\n\n  \n\n\n\nというわけで国名を日本語化することができました。\n\nSupplement\ncountrycode::codelistには国名以外にも地域区分などが収められているので、国名以外の情報も表示できます。例として、国連による地域区分も表示してみます。なお、地域区分は英語なので地域の日本語化は人力です。\n\n\nCode\ndf_country_name |&gt;\n  mutate(\n    # Japanese localize \n    temp_name_ja_1 = countrycode::countrycode(\n      Country,\n      origin = \"country.name\",\n      destination = \"cldr.name.ja\"\n    ),\n    temp_name_ja_2 = countries::country_name(\n      Country,\n      to = \"name_ja\"\n    ),\n    res_name_ja = if_else(\n      is.na(temp_name_ja_1),\n      temp_name_ja_2,\n      temp_name_ja_1\n    ),\n    # for region matching\n    temp_name_en_pre = countrycode::countrycode(\n      Country,\n      origin = \"country.name\",\n      destination = \"country.name.en\"\n    ),\n    temp_name_en = if_else(\n      is.na(temp_name_en_pre),\n      countries::country_name(Country, to = \"name_en\"),\n      temp_name_en_pre\n    ),\n    # matching UN region\n    region_un = countrycode::countrycode(\n      temp_name_en,\n      origin = \"country.name\",\n      destination = \"un.region.name\"\n    ),\n    region_un_sub = countrycode::countrycode(\n      temp_name_en,\n      origin = \"country.name\",\n      destination = \"un.regionsub.name\"\n    ),\n    region_un_intrmd = countrycode::countrycode(\n      temp_name_en,\n      origin = \"country.name\",\n      destination = \"un.regionintermediate.name\"\n    )\n  ) |&gt;\n  # JP localize region. Not necessary.\n  mutate(\n    region_un = str_replace_all(\n      region_un,\n      pattern = c(\n        \"Africa\" = \"アフリカ州\",\n        \"Americas\" = \"アメリカ州\",\n        \"Asia\" = \"アジア州\",\n        \"Europe\" = \"ヨーロッパ州\",\n        \"Oceania\" = \"オセアニア州\"\n      )\n    ) |&gt;\n      replace_na(\"国連未加盟\"),\n    across(\n      .cols = starts_with(\"region_un_\"),\n      .fns = \\(x) {\n        str_replace_all(\n          x,\n          pattern = c(\n            \"Africa\" = \"アフリカ\",\n            \"America\\\\s*\" = \"アメリカ\",\n            \"Asia\" = \"アジア\",\n            \"Europe\" = \"ヨーロッパ\",\n            \"Northern\\\\s\" = \"北\",\n            \"Eastern\\\\s\" = \"東\",\n            \"(Southern|South)\\\\s\" = \"南\",\n            \"South-eastern\\\\s\" = \"東南\",\n            \"Western\\\\s\" = \"西\",\n            \"Central\\\\s\" = \"中央\",\n            \"Middle\\\\s\" = \"中部\",\n            \"Latin\\\\s\" = \"ラテン\",\n            \"Sub-Saharan\\\\s\" = \"サハラ以南\",\n            \"and\\\\sthe\\\\s\" = \"・\",\n            \"Caribbean\" = \"カリブ海地域\",\n            \"Australia\\\\sand\\\\sNew\\\\sZealand\" = \"オーストラリア・ニュージーランド\",\n            \"Melanesia\" = \"メラネシア\",\n            \"Micronesia\" = \"ミクロネシア\",\n            \"Polinesia\" = \"ポリネシア\"\n          )\n        )\n      }\n    )\n  ) |&gt;\n  select(-starts_with(\"temp\")) |&gt;\n  relocate(\n    n,\n    .after = last_col()\n  ) |&gt;\n  rmarkdown::paged_table(options = list(rows.print = 20))\n\n\n\n  \n\n\n\n実際にはcount()のところで性別の回答も入れたものを補助資料として作成しました。"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#conclusion",
    "href": "posts/20251213_localize_country_name/index.html#conclusion",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Conclusion",
    "text": "Conclusion\ncountrycodeパッケージやcountriesパッケージを使って、データに含まれている国名を日本語名にしました。翻訳データがないワールドワイドなオープンデータで、国名を一括で変換したいときには便利だと思います。少なくとも人力で全部頑張るよりかはましです。"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#sessioninfo",
    "href": "posts/20251213_localize_country_name/index.html#sessioninfo",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Sessioninfo",
    "text": "Sessioninfo\n\n\n\n\n\n\nsessioninfo\n\n\n\n\n\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Japanese_Japan.utf8  LC_CTYPE=Japanese_Japan.utf8    LC_MONETARY=Japanese_Japan.utf8\n[4] LC_NUMERIC=C                    LC_TIME=Japanese_Japan.utf8    \n\ntime zone: Asia/Tokyo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] countries_1.2.2   countrycode_1.6.1 lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n [6] dplyr_1.1.4       purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n[11] ggplot2_3.5.2     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] bit_4.6.0          gtable_0.3.6       jsonlite_2.0.0     crayon_1.5.3       compiler_4.4.2    \n [6] tidyselect_1.2.1   parallel_4.4.2     scales_1.4.0       yaml_2.3.10        fastmap_1.2.0     \n[11] R6_2.6.1           generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4  pillar_1.11.0     \n[16] RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6        utf8_1.2.6         fastmatch_1.1-6   \n[21] stringi_1.8.7      xfun_0.52          bit64_4.6.0-1      timechange_0.3.0   cli_3.6.5         \n[26] withr_3.0.2        magrittr_2.0.3     stringdist_0.9.15  digest_0.6.37      grid_4.4.2        \n[31] vroom_1.6.5        rstudioapi_0.17.1  hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[36] evaluate_1.0.5     glue_1.8.0         farver_2.1.2       pacman_0.5.1       rmarkdown_2.29    \n[41] tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#footnotes",
    "href": "posts/20251213_localize_country_name/index.html#footnotes",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYamada, Y., et al. (2021). COVIDiSTRESS Global Survey dataset on psychological and behavioural consequences of the COVID-19 outbreak. Scientific Data, 8, 3. https://doi.org/10.1038/s41597-020-00784-9↩︎\nhttps://vincentarelbundock.github.io/countrycode/#/↩︎\nhttps://fbellelli.github.io/countries/↩︎\nhttps://fbellelli.github.io/countries/articles/dealing_with_names.html↩︎"
  },
  {
    "objectID": "posts/20251213_localize_country_name/index.html#session-infomation",
    "href": "posts/20251213_localize_country_name/index.html#session-infomation",
    "title": "Rでデータの国名を日本語名にする",
    "section": "Session Infomation",
    "text": "Session Infomation\n\n\n\n\n\n\nNotesessioninfo\n\n\n\n\n\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Japanese_Japan.utf8  LC_CTYPE=Japanese_Japan.utf8    LC_MONETARY=Japanese_Japan.utf8\n[4] LC_NUMERIC=C                    LC_TIME=Japanese_Japan.utf8    \n\ntime zone: Asia/Tokyo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] countries_1.2.2   countrycode_1.6.1 lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n [6] dplyr_1.1.4       purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n[11] ggplot2_3.5.2     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] bit_4.6.0          gtable_0.3.6       jsonlite_2.0.0     crayon_1.5.3       compiler_4.4.2    \n [6] tidyselect_1.2.1   parallel_4.4.2     scales_1.4.0       yaml_2.3.10        fastmap_1.2.0     \n[11] R6_2.6.1           generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4  pillar_1.11.0     \n[16] RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6        utf8_1.2.6         fastmatch_1.1-6   \n[21] stringi_1.8.7      xfun_0.52          bit64_4.6.0-1      timechange_0.3.0   cli_3.6.5         \n[26] withr_3.0.2        magrittr_2.0.3     stringdist_0.9.15  digest_0.6.37      grid_4.4.2        \n[31] vroom_1.6.5        rstudioapi_0.17.1  hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[36] evaluate_1.0.5     glue_1.8.0         farver_2.1.2       pacman_0.5.1       rmarkdown_2.29    \n[41] tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1"
  },
  {
    "objectID": "posts/20260102_built-in_color_name/index.html",
    "href": "posts/20260102_built-in_color_name/index.html",
    "title": "R 標準で用意されている色の名前",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  gt,\n  gdata\n)"
  },
  {
    "objectID": "posts/20260102_built-in_color_name/index.html#packages",
    "href": "posts/20260102_built-in_color_name/index.html#packages",
    "title": "R 標準で用意されている色の名前",
    "section": "",
    "text": "pacman::p_load(\n  tidyverse,\n  gt,\n  gdata\n)"
  },
  {
    "objectID": "posts/20260102_built-in_color_name/index.html#contents",
    "href": "posts/20260102_built-in_color_name/index.html#contents",
    "title": "R 標準で用意されている色の名前",
    "section": "Contents",
    "text": "Contents\nRが組み込みで用意している色の名前についてのメモです。\n\ncolors()\n組み込みパッケージであるgrDevicesにあるcolors()は、Rで用意されている色の名前を出してくれる関数です。実行すると657色の色の名前を出してくれます。ここで出てくる色の名前をグラフ作成のときに指定すると、その色が使えます。\n\ncolors()\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"         \"antiquewhite1\"       \n  [5] \"antiquewhite2\"        \"antiquewhite3\"        \"antiquewhite4\"        \"aquamarine\"          \n  [9] \"aquamarine1\"          \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"               \"azure3\"              \n [17] \"azure4\"               \"beige\"                \"bisque\"               \"bisque1\"             \n [21] \"bisque2\"              \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"                \"blue2\"               \n [29] \"blue3\"                \"blue4\"                \"blueviolet\"           \"brown\"               \n [33] \"brown1\"               \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"           \"burlywood3\"          \n [41] \"burlywood4\"           \"cadetblue\"            \"cadetblue1\"           \"cadetblue2\"          \n [45] \"cadetblue3\"           \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"          \"chocolate\"           \n [53] \"chocolate1\"           \"chocolate2\"           \"chocolate3\"           \"chocolate4\"          \n [57] \"coral\"                \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"             \"cornsilk1\"           \n [65] \"cornsilk2\"            \"cornsilk3\"            \"cornsilk4\"            \"cyan\"                \n [69] \"cyan1\"                \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"        \"darkgoldenrod1\"      \n [77] \"darkgoldenrod2\"       \"darkgoldenrod3\"       \"darkgoldenrod4\"       \"darkgray\"            \n [81] \"darkgreen\"            \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"      \"darkolivegreen3\"     \n [89] \"darkolivegreen4\"      \"darkorange\"           \"darkorange1\"          \"darkorange2\"         \n [93] \"darkorange3\"          \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"          \"darkred\"             \n[101] \"darksalmon\"           \"darkseagreen\"         \"darkseagreen1\"        \"darkseagreen2\"       \n[105] \"darkseagreen3\"        \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"       \"darkslategray4\"      \n[113] \"darkslategrey\"        \"darkturquoise\"        \"darkviolet\"           \"deeppink\"            \n[117] \"deeppink1\"            \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"         \"deepskyblue3\"        \n[125] \"deepskyblue4\"         \"dimgray\"              \"dimgrey\"              \"dodgerblue\"          \n[129] \"dodgerblue1\"          \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"           \"firebrick3\"          \n[137] \"firebrick4\"           \"floralwhite\"          \"forestgreen\"          \"gainsboro\"           \n[141] \"ghostwhite\"           \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"            \"goldenrod1\"          \n[149] \"goldenrod2\"           \"goldenrod3\"           \"goldenrod4\"           \"gray\"                \n[153] \"gray0\"                \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"                \"gray7\"               \n[161] \"gray8\"                \"gray9\"                \"gray10\"               \"gray11\"              \n[165] \"gray12\"               \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"               \"gray19\"              \n[173] \"gray20\"               \"gray21\"               \"gray22\"               \"gray23\"              \n[177] \"gray24\"               \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"               \"gray31\"              \n[185] \"gray32\"               \"gray33\"               \"gray34\"               \"gray35\"              \n[189] \"gray36\"               \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"               \"gray43\"              \n[197] \"gray44\"               \"gray45\"               \"gray46\"               \"gray47\"              \n[201] \"gray48\"               \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"               \"gray55\"              \n[209] \"gray56\"               \"gray57\"               \"gray58\"               \"gray59\"              \n[213] \"gray60\"               \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"               \"gray67\"              \n[221] \"gray68\"               \"gray69\"               \"gray70\"               \"gray71\"              \n[225] \"gray72\"               \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"               \"gray79\"              \n[233] \"gray80\"               \"gray81\"               \"gray82\"               \"gray83\"              \n[237] \"gray84\"               \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"               \"gray91\"              \n[245] \"gray92\"               \"gray93\"               \"gray94\"               \"gray95\"              \n[249] \"gray96\"               \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"               \"green2\"              \n[257] \"green3\"               \"green4\"               \"greenyellow\"          \"grey\"                \n[261] \"grey0\"                \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"                \"grey7\"               \n[269] \"grey8\"                \"grey9\"                \"grey10\"               \"grey11\"              \n[273] \"grey12\"               \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"               \"grey19\"              \n[281] \"grey20\"               \"grey21\"               \"grey22\"               \"grey23\"              \n[285] \"grey24\"               \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"               \"grey31\"              \n[293] \"grey32\"               \"grey33\"               \"grey34\"               \"grey35\"              \n[297] \"grey36\"               \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"               \"grey43\"              \n[305] \"grey44\"               \"grey45\"               \"grey46\"               \"grey47\"              \n[309] \"grey48\"               \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"               \"grey55\"              \n[317] \"grey56\"               \"grey57\"               \"grey58\"               \"grey59\"              \n[321] \"grey60\"               \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"               \"grey67\"              \n[329] \"grey68\"               \"grey69\"               \"grey70\"               \"grey71\"              \n[333] \"grey72\"               \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"               \"grey79\"              \n[341] \"grey80\"               \"grey81\"               \"grey82\"               \"grey83\"              \n[345] \"grey84\"               \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"               \"grey91\"              \n[353] \"grey92\"               \"grey93\"               \"grey94\"               \"grey95\"              \n[357] \"grey96\"               \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"            \"honeydew2\"           \n[365] \"honeydew3\"            \"honeydew4\"            \"hotpink\"              \"hotpink1\"            \n[369] \"hotpink2\"             \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"           \"indianred4\"          \n[377] \"ivory\"                \"ivory1\"               \"ivory2\"               \"ivory3\"              \n[381] \"ivory4\"               \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"             \"lavenderblush\"       \n[389] \"lavenderblush1\"       \"lavenderblush2\"       \"lavenderblush3\"       \"lavenderblush4\"      \n[393] \"lawngreen\"            \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"            \"lightblue1\"          \n[401] \"lightblue2\"           \"lightblue3\"           \"lightblue4\"           \"lightcoral\"          \n[405] \"lightcyan\"            \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"      \"lightgoldenrod2\"     \n[413] \"lightgoldenrod3\"      \"lightgoldenrod4\"      \"lightgoldenrodyellow\" \"lightgray\"           \n[417] \"lightgreen\"           \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"           \"lightsalmon\"         \n[425] \"lightsalmon1\"         \"lightsalmon2\"         \"lightsalmon3\"         \"lightsalmon4\"        \n[429] \"lightseagreen\"        \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"       \"lightslategray\"      \n[437] \"lightslategrey\"       \"lightsteelblue\"       \"lightsteelblue1\"      \"lightsteelblue2\"     \n[441] \"lightsteelblue3\"      \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"         \"limegreen\"           \n[449] \"linen\"                \"magenta\"              \"magenta1\"             \"magenta2\"            \n[453] \"magenta3\"             \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"              \"mediumaquamarine\"    \n[461] \"mediumblue\"           \"mediumorchid\"         \"mediumorchid1\"        \"mediumorchid2\"       \n[465] \"mediumorchid3\"        \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"        \"mediumseagreen\"      \n[473] \"mediumslateblue\"      \"mediumspringgreen\"    \"mediumturquoise\"      \"mediumvioletred\"     \n[477] \"midnightblue\"         \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"           \"moccasin\"            \n[485] \"navajowhite\"          \"navajowhite1\"         \"navajowhite2\"         \"navajowhite3\"        \n[489] \"navajowhite4\"         \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"           \"olivedrab3\"          \n[497] \"olivedrab4\"           \"orange\"               \"orange1\"              \"orange2\"             \n[501] \"orange3\"              \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"           \"orchid\"              \n[509] \"orchid1\"              \"orchid2\"              \"orchid3\"              \"orchid4\"             \n[513] \"palegoldenrod\"        \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"        \"paleturquoise1\"      \n[521] \"paleturquoise2\"       \"paleturquoise3\"       \"paleturquoise4\"       \"palevioletred\"       \n[525] \"palevioletred1\"       \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"           \"peachpuff2\"          \n[533] \"peachpuff3\"           \"peachpuff4\"           \"peru\"                 \"pink\"                \n[537] \"pink1\"                \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"                \"plum3\"               \n[545] \"plum4\"                \"powderblue\"           \"purple\"               \"purple1\"             \n[549] \"purple2\"              \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                 \"red4\"                \n[557] \"rosybrown\"            \"rosybrown1\"           \"rosybrown2\"           \"rosybrown3\"          \n[561] \"rosybrown4\"           \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"          \"salmon\"              \n[569] \"salmon1\"              \"salmon2\"              \"salmon3\"              \"salmon4\"             \n[573] \"sandybrown\"           \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"             \"seashell1\"           \n[581] \"seashell2\"            \"seashell3\"            \"seashell4\"            \"sienna\"              \n[585] \"sienna1\"              \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"             \"skyblue3\"            \n[593] \"skyblue4\"             \"slateblue\"            \"slateblue1\"           \"slateblue2\"          \n[597] \"slateblue3\"           \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"           \"slategrey\"           \n[605] \"snow\"                 \"snow1\"                \"snow2\"                \"snow3\"               \n[609] \"snow4\"                \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"            \"steelblue1\"          \n[617] \"steelblue2\"           \"steelblue3\"           \"steelblue4\"           \"tan\"                 \n[621] \"tan1\"                 \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"             \"thistle3\"            \n[629] \"thistle4\"             \"tomato\"               \"tomato1\"              \"tomato2\"             \n[633] \"tomato3\"              \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"           \"violet\"              \n[641] \"violetred\"            \"violetred1\"           \"violetred2\"           \"violetred3\"          \n[645] \"violetred4\"           \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"           \"yellow\"              \n[653] \"yellow1\"              \"yellow2\"              \"yellow3\"              \"yellow4\"             \n[657] \"yellowgreen\"         \n\n\n引数distinctをTRUEにすると、RGB値で同じ色を1つにまとめて返してきます。この場合は502色の色名が返ってきます。\n\ncolors(distinct = TRUE)\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"         \"antiquewhite1\"       \n  [5] \"antiquewhite2\"        \"antiquewhite3\"        \"antiquewhite4\"        \"aquamarine\"          \n  [9] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"          \"azure\"               \n [13] \"azure2\"               \"azure3\"               \"azure4\"               \"beige\"               \n [17] \"bisque\"               \"bisque2\"              \"bisque3\"              \"bisque4\"             \n [21] \"black\"                \"blanchedalmond\"       \"blue\"                 \"blue2\"               \n [25] \"blue3\"                \"blue4\"                \"blueviolet\"           \"brown\"               \n [29] \"brown1\"               \"brown2\"               \"brown3\"               \"brown4\"              \n [33] \"burlywood\"            \"burlywood1\"           \"burlywood2\"           \"burlywood3\"          \n [37] \"burlywood4\"           \"cadetblue\"            \"cadetblue1\"           \"cadetblue2\"          \n [41] \"cadetblue3\"           \"cadetblue4\"           \"chartreuse\"           \"chartreuse2\"         \n [45] \"chartreuse3\"          \"chartreuse4\"          \"chocolate\"            \"chocolate1\"          \n [49] \"chocolate2\"           \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [53] \"coral1\"               \"coral2\"               \"coral3\"               \"coral4\"              \n [57] \"cornflowerblue\"       \"cornsilk\"             \"cornsilk2\"            \"cornsilk3\"           \n [61] \"cornsilk4\"            \"cyan\"                 \"cyan2\"                \"cyan3\"               \n [65] \"cyan4\"                \"darkgoldenrod\"        \"darkgoldenrod1\"       \"darkgoldenrod2\"      \n [69] \"darkgoldenrod3\"       \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [73] \"darkkhaki\"            \"darkmagenta\"          \"darkolivegreen\"       \"darkolivegreen1\"     \n [77] \"darkolivegreen2\"      \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [81] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"          \"darkorange4\"         \n [85] \"darkorchid\"           \"darkorchid1\"          \"darkorchid2\"          \"darkorchid3\"         \n [89] \"darkorchid4\"          \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n [93] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"        \"darkseagreen4\"       \n [97] \"darkslateblue\"        \"darkslategray\"        \"darkslategray1\"       \"darkslategray2\"      \n[101] \"darkslategray3\"       \"darkslategray4\"       \"darkturquoise\"        \"darkviolet\"          \n[105] \"deeppink\"             \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[109] \"deepskyblue\"          \"deepskyblue2\"         \"deepskyblue3\"         \"deepskyblue4\"        \n[113] \"dimgray\"              \"dodgerblue\"           \"dodgerblue2\"          \"dodgerblue3\"         \n[117] \"dodgerblue4\"          \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[121] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"          \"forestgreen\"         \n[125] \"gainsboro\"            \"ghostwhite\"           \"gold\"                 \"gold2\"               \n[129] \"gold3\"                \"gold4\"                \"goldenrod\"            \"goldenrod1\"          \n[133] \"goldenrod2\"           \"goldenrod3\"           \"goldenrod4\"           \"gray\"                \n[137] \"gray1\"                \"gray2\"                \"gray3\"                \"gray4\"               \n[141] \"gray5\"                \"gray6\"                \"gray7\"                \"gray8\"               \n[145] \"gray9\"                \"gray10\"               \"gray11\"               \"gray12\"              \n[149] \"gray13\"               \"gray14\"               \"gray15\"               \"gray16\"              \n[153] \"gray17\"               \"gray18\"               \"gray19\"               \"gray20\"              \n[157] \"gray21\"               \"gray22\"               \"gray23\"               \"gray24\"              \n[161] \"gray25\"               \"gray26\"               \"gray27\"               \"gray28\"              \n[165] \"gray29\"               \"gray30\"               \"gray31\"               \"gray32\"              \n[169] \"gray33\"               \"gray34\"               \"gray35\"               \"gray36\"              \n[173] \"gray37\"               \"gray38\"               \"gray39\"               \"gray40\"              \n[177] \"gray42\"               \"gray43\"               \"gray44\"               \"gray45\"              \n[181] \"gray46\"               \"gray47\"               \"gray48\"               \"gray49\"              \n[185] \"gray50\"               \"gray51\"               \"gray52\"               \"gray53\"              \n[189] \"gray54\"               \"gray55\"               \"gray56\"               \"gray57\"              \n[193] \"gray58\"               \"gray59\"               \"gray60\"               \"gray61\"              \n[197] \"gray62\"               \"gray63\"               \"gray64\"               \"gray65\"              \n[201] \"gray66\"               \"gray67\"               \"gray68\"               \"gray69\"              \n[205] \"gray70\"               \"gray71\"               \"gray72\"               \"gray73\"              \n[209] \"gray74\"               \"gray75\"               \"gray76\"               \"gray77\"              \n[213] \"gray78\"               \"gray79\"               \"gray80\"               \"gray81\"              \n[217] \"gray82\"               \"gray83\"               \"gray84\"               \"gray85\"              \n[221] \"gray86\"               \"gray87\"               \"gray88\"               \"gray89\"              \n[225] \"gray90\"               \"gray91\"               \"gray92\"               \"gray93\"              \n[229] \"gray94\"               \"gray95\"               \"gray96\"               \"gray97\"              \n[233] \"gray98\"               \"gray99\"               \"green\"                \"green2\"              \n[237] \"green3\"               \"green4\"               \"greenyellow\"          \"honeydew\"            \n[241] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"            \"hotpink\"             \n[245] \"hotpink1\"             \"hotpink2\"             \"hotpink3\"             \"hotpink4\"            \n[249] \"indianred\"            \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[253] \"indianred4\"           \"ivory\"                \"ivory2\"               \"ivory3\"              \n[257] \"ivory4\"               \"khaki\"                \"khaki1\"               \"khaki2\"              \n[261] \"khaki3\"               \"khaki4\"               \"lavender\"             \"lavenderblush\"       \n[265] \"lavenderblush2\"       \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[269] \"lemonchiffon\"         \"lemonchiffon2\"        \"lemonchiffon3\"        \"lemonchiffon4\"       \n[273] \"lightblue\"            \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[277] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"            \"lightcyan2\"          \n[281] \"lightcyan3\"           \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[285] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"      \"lightgoldenrodyellow\"\n[289] \"lightgray\"            \"lightgreen\"           \"lightpink\"            \"lightpink1\"          \n[293] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"           \"lightsalmon\"         \n[297] \"lightsalmon2\"         \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[301] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"        \"lightskyblue3\"       \n[305] \"lightskyblue4\"        \"lightslateblue\"       \"lightslategray\"       \"lightsteelblue\"      \n[309] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"      \"lightsteelblue4\"     \n[313] \"lightyellow\"          \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[317] \"limegreen\"            \"linen\"                \"magenta\"              \"magenta2\"            \n[321] \"magenta3\"             \"maroon\"               \"maroon1\"              \"maroon2\"             \n[325] \"maroon3\"              \"maroon4\"              \"mediumorchid\"         \"mediumorchid1\"       \n[329] \"mediumorchid2\"        \"mediumorchid3\"        \"mediumorchid4\"        \"mediumpurple\"        \n[333] \"mediumpurple1\"        \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[337] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"    \"mediumturquoise\"     \n[341] \"mediumvioletred\"      \"midnightblue\"         \"mintcream\"            \"mistyrose\"           \n[345] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"           \"moccasin\"            \n[349] \"navajowhite\"          \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[353] \"navy\"                 \"oldlace\"              \"olivedrab\"            \"olivedrab1\"          \n[357] \"olivedrab2\"           \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[361] \"orange2\"              \"orange3\"              \"orange4\"              \"orangered\"           \n[365] \"orangered2\"           \"orangered3\"           \"orangered4\"           \"orchid\"              \n[369] \"orchid1\"              \"orchid2\"              \"orchid3\"              \"orchid4\"             \n[373] \"palegoldenrod\"        \"palegreen\"            \"palegreen1\"           \"palegreen3\"          \n[377] \"palegreen4\"           \"paleturquoise\"        \"paleturquoise1\"       \"paleturquoise2\"      \n[381] \"paleturquoise3\"       \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[385] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"       \"papayawhip\"          \n[389] \"peachpuff\"            \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[393] \"peru\"                 \"pink\"                 \"pink1\"                \"pink2\"               \n[397] \"pink3\"                \"pink4\"                \"plum\"                 \"plum1\"               \n[401] \"plum2\"                \"plum3\"                \"plum4\"                \"powderblue\"          \n[405] \"purple\"               \"purple1\"              \"purple2\"              \"purple3\"             \n[409] \"purple4\"              \"red\"                  \"red2\"                 \"red3\"                \n[413] \"rosybrown\"            \"rosybrown1\"           \"rosybrown2\"           \"rosybrown3\"          \n[417] \"rosybrown4\"           \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[421] \"royalblue3\"           \"royalblue4\"           \"salmon\"               \"salmon1\"             \n[425] \"salmon2\"              \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[429] \"seagreen\"             \"seagreen1\"            \"seagreen2\"            \"seagreen3\"           \n[433] \"seashell\"             \"seashell2\"            \"seashell3\"            \"seashell4\"           \n[437] \"sienna\"               \"sienna1\"              \"sienna2\"              \"sienna3\"             \n[441] \"sienna4\"              \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[445] \"skyblue3\"             \"skyblue4\"             \"slateblue\"            \"slateblue1\"          \n[449] \"slateblue2\"           \"slateblue3\"           \"slateblue4\"           \"slategray\"           \n[453] \"slategray1\"           \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[457] \"snow\"                 \"snow2\"                \"snow3\"                \"snow4\"               \n[461] \"springgreen\"          \"springgreen2\"         \"springgreen3\"         \"springgreen4\"        \n[465] \"steelblue\"            \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[469] \"steelblue4\"           \"tan\"                  \"tan1\"                 \"tan2\"                \n[473] \"tan4\"                 \"thistle\"              \"thistle1\"             \"thistle2\"            \n[477] \"thistle3\"             \"thistle4\"             \"tomato\"               \"tomato2\"             \n[481] \"tomato3\"              \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[485] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"           \"violet\"              \n[489] \"violetred\"            \"violetred1\"           \"violetred2\"           \"violetred3\"          \n[493] \"violetred4\"           \"wheat\"                \"wheat1\"               \"wheat2\"              \n[497] \"wheat3\"               \"wheat4\"               \"yellow\"               \"yellow2\"             \n[501] \"yellow3\"              \"yellow4\"             \n\n\n重複している色は以下の通りです。\n\ntibble(\n  color_name = colors()\n) |&gt;\n  mutate(\n1    col2rgb(col = color_name) |&gt;\n      t() |&gt;\n      data.frame(),\n    hex = rgb(\n      red = red,\n      green = green,\n      blue = blue,\n2      maxColorValue = 255\n    )\n  ) |&gt;\n3  filter(n() &gt; 1, .by = hex) |&gt;\n  arrange(hex) |&gt;\n  # for better view on website\n  rmarkdown::paged_table(options = list(rows.print = 20))\n\n\n1\n\ncol2rgb()は引数colに色の名前を入れるとrgb値を3*1のmatrixで返してきます。なので、t()で転置してwideにしてdata.frameとして返します。列名をつけないことでpackされずにそのまま列として返ってきます。\n\n2\n\nデフォルトでは1なので255にします。\n\n3\n\n「同一要素が存在する要素のみ抽出」をtidyverse関数群だけで多分一番短く書ける式だけど、なんでちゃんと動くかあんまりわかってない。hex列の要素でグルーピングしてn()でサイズを数えてそれが１より大きいかどうかというpredicateな式になっているからだとは思います。組み込み関数でやるならfilter(duplicated(hex) | duplicated(hex, fromLast = TRUE))。\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNotebuilt-in functions\n\n\n\n\n\n\ndata.frame(\n  color_name = colors()\n) |&gt;\n  transform(\n1    hex = col2rgb(color_name) |&gt;\n      apply(\n        MARGIN = 2,\n        FUN = \\(x) {\n          rgb(\n            red = x[1],\n            green = x[2],\n            blue = x[3],\n            maxColorValue = 255\n          )\n        }\n      )\n  ) |&gt;\n  subset(duplicated(hex) | duplicated(hex, fromLast = TRUE)) |&gt;\n  sort_by(~hex) |&gt;\n  head(10)\n\n\n1\n\ntransform()はtag = valueの形式が必須らしいので、apply(MARGIN = 2, ...)を使っていきなりhex列を作りました。別にtemp = col2rgb() |&gt; ...にしてもいいんですが、dplyr::mutate()と違って、作られる列にtemp.のsuffixがつくのと、transform()内で作成した新規の列は同じtransform()内で新たに使えないっぽいのでもう一度パイプを挟んでtransform()を使う必要があるみたいです。\n\n\n\n\n    color_name     hex\n24       black #000000\n153      gray0 #000000\n261      grey0 #000000\n490       navy #000080\n491   navyblue #000080\n30       blue4 #00008B\n73    darkblue #00008B\n29       blue3 #0000CD\n461 mediumblue #0000CD\n26        blue #0000FF\n\n\n\n\n\n数が多いですが、半分くらいがgray（アメリカ英語）とgrey（イギリス英語）の被りです。gray/greyはほとんど置換可能なんですが、(dark)slategray1-4はgrayしかありません。\n\nlist(\n  gray_only = c(\"gray\", \"grey\"),\n  grey_only = c(\"grey\", \"gray\")\n) |&gt;\n  map(\n    .f = \\(color_name) {\n      str_subset(\n        colors(),\n        pattern = color_name[1]\n      ) |&gt;\n1        discard(\n          .p = \\(x) {\n            str_replace(\n              x,\n              pattern = color_name[1],\n              replacement = color_name[2]\n            ) |&gt;\n              `%in%`(colors())\n          }\n        )\n    }\n  )\n\n\n1\n\npurrr::discard()は第一引数.xの要素を引数.pで書かれたpredicate関数で評価させて、FALSEになる要素だけ返してきます。TRUEを返すものだけ残したい場合はpurrr::keep()を使います。やってることは、[str_replace()]grayとgreyを入れ替え→[%in%()]入れ替えたものがcolors()に存在するか評価→[.p = \\(x) {}]TRUE/FALSEが返ってくるのでそれが.xのインデックスになる→[discard()]インデックスがFALSEである.xの要素のみ返ってくる。\n\n\n\n\n$gray_only\n[1] \"darkslategray1\" \"darkslategray2\" \"darkslategray3\" \"darkslategray4\" \"slategray1\"    \n[6] \"slategray2\"     \"slategray3\"     \"slategray4\"    \n\n$grey_only\ncharacter(0)\n\n\n\n\n\n\n\n\nNotebuilt-in functions\n\n\n\n\n\n\nlist(\n  gray_only = c(\"gray\", \"grey\"),\n  grey_only = c(\"grey\", \"gray\")\n) |&gt;\n  lapply(\n    FUN = \\(color_name) {\n      grep(\n        pattern = color_name[1],\n        x = colors(),\n        value = TRUE\n      ) |&gt;\n        (\\(x) {\n1          gsub(\n            pattern = color_name[1],\n            replacement = color_name[2],\n            x = x\n          ) |&gt;\n            `%in%`(colors()) |&gt;\n            xor(y = TRUE) |&gt;\n            `[`(x, i = _)\n        }) ()\n    }\n  )\n\n\n1\n\nx[!gsub(pattern = color_name[1], replacement = color_name[2], x = x) %in% colors()]をlintrに怒られない文字数で書くためにパイプラインで書くとこうなります。可読性が低すぎてヤバい。やってることは上と同じです。TRUE/FALSEの逆転は、xor(x = ..., y = TRUE)を使って(x = TRUE, y = TRUE)をFALSEに、(x = FALSE, y = TRUE) をTRUEにすることで処理。(\\(x) {!x}) ()でも可。\n\n\n\n\n$gray_only\n[1] \"darkslategray1\" \"darkslategray2\" \"darkslategray3\" \"darkslategray4\" \"slategray1\"    \n[6] \"slategray2\"     \"slategray3\"     \"slategray4\"    \n\n$grey_only\ncharacter(0)\n\n\n\n\n\n\n\nappearance\n実際の色はこちら。greyだけ省いています。gt::opt_interactive()をいじってインタラクティブな表にしてみました。\n\n\nCode\ncolors() |&gt;\n  str_subset(\n    pattern = \"grey\",\n    negate = TRUE\n  ) |&gt;\n  tibble(\n    col_name = _,\n    col_fill = \"\",\n    col_text_w = \"The quick brown fox jumps over the lazy dog.\",\n    col_text_b = col_text_w\n  ) |&gt;\n  mutate(\n    col2rgb(col = col_name) |&gt;\n      t() |&gt;\n      data.frame(),\n    col_hex = rgb(\n      red = red,\n      green = green,\n      blue = blue,\n      maxColorValue = 255\n    )\n  ) |&gt;\n  relocate(col_hex, .after = 1) |&gt;\n  select(starts_with(\"col_\")) |&gt;\n  gt() |&gt;\n  tab_style(\n    style = cell_fill(color = from_column(column = \"col_name\")),\n    locations = cells_body(columns = col_fill)\n  ) |&gt;\n  tab_style(\n    style = cell_borders(color = \"black\"),\n    locations = cells_body(columns = col_fill)\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = from_column(column = \"col_name\")),\n    locations = cells_body(columns = starts_with(\"col_text\"))\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"black\"),\n    locations = cells_body(columns = col_text_b)\n  ) |&gt;\n  tab_header(title = \"Built-in Color Names in R.\") |&gt;\n  tab_footnote(\n    footnote = '\"gray\" can be replaced with \"grey\" except for (dark)slategray1-4.'\n  ) |&gt;\n  opt_footnote_marks(marks = \"standard\") |&gt;\n  tab_source_note(\n    source_note = md(\"From `grDevices::colors()`.\")\n  ) |&gt;\n  cols_width(\n    col_name ~ px(180),\n    col_hex ~ px(100),\n    col_fill ~ px(100)\n  ) |&gt;\n  cols_label(\n    col_name = \"Color name\",\n    col_hex = \"Hex\",\n    col_fill = \"\",\n    col_text_w = \"BG: white\",\n    col_text_b = \"BG: black\"\n  ) |&gt;\n  # for better view on website\n  opt_interactive(\n1    use_filters = TRUE,\n2    use_page_size_select = TRUE,\n3    page_size_values = c(seq(10, 100, 10), seq(200, 600, 100))\n  )\n\n\n\n1\n\nフィルタリング機能が使えます。列名の下のテキストボックスに文字を入れるとフィルターされます。\n\n2\n\n1ページ当たりの表示行数を変更可能にできます。\n\n3\n\n1ページ当たりの表示行数の選択肢を設定できます。\n\n\n\n\n\n\n\nBuilt-in Color Names in R.\n\n\n\n\n\n\nFrom grDevices::colors().\n\n\n\n\n \"gray\" can be replaced with \"grey\" except for (dark)slategray1-4.\n\n\n\n\n\n\n\n画像の一覧表も作ってみました。こちらもgreyは省いています。重いです（.pngで430KB程度）。QuartoのLightbox機能をonにしたので、クリックすると大きくなります。\n\n\nCode\ncolors() |&gt;\n  str_subset(\n    pattern = \"grey\",\n    negate = TRUE\n  ) |&gt;\n  str_replace(\n    pattern = \"(\\\\d+)\",\n    replacement = \\(x) {\n      str_pad(\n        x,\n        width = 3,\n        side = \"left\",\n        pad = 0\n      )\n    }\n  ) |&gt;\n  sort() |&gt;\n  str_remove(\n    pattern = \"(?&lt;=\\\\D)0{1,2}\"\n  ) |&gt;\n  tibble(\n    name = _,\n    fill = \"\",\n    col_num = rep(1:12, times = c(rep(46, 9), rep(45, 3)))\n  ) |&gt;\n  group_by(col_num) |&gt;\n  group_map(\n    .f = \\(x, idx) {\n      rename_with(\n        x,\n        .fn = ~str_c(.x, idx, sep = \"_\")\n      )\n    }\n  ) |&gt;\n  do.call(\n    what = gdata::cbindX,\n    args = _\n  ) |&gt;\n  gt() |&gt;\n  (\\(x) {\n    paste0(\n      ' |&gt; tab_style(style = cell_fill(color = from_column(column = \"name_',\n      1:12,\n      '\", na_value = \"white\")), locations = cells_body(column = fill_',\n      1:12,\n      \"))\"\n    ) |&gt;\n      paste(collapse = \"\") |&gt;\n      paste(\"x\", y = _, sep = \"\") |&gt;\n      parse(text = _) |&gt;\n      eval()\n  }) () |&gt;\n  tab_style(\n    style = cell_borders(color = \"black\"),\n    locations = cells_body(columns = starts_with(\"fill\"))\n  ) |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"right\",\n      color = \"black\"\n    ),\n    locations = cells_column_labels(columns = starts_with(\"fill\"))\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      font = \"HackGen\",\n      indent = px(10)\n    ),\n    locations = cells_body(columns = starts_with(\"name\"))\n  ) |&gt;\n  sub_missing(\n    missing_text = \"\"\n  ) |&gt;\n  cols_width(\n    starts_with(\"name\") ~ px(190),\n    starts_with(\"fill\") ~ px(50)\n  ) |&gt;\n  tab_header(title = md(\"Built-in Color Names in R. (From `grDevices::colors()`)\")) |&gt;\n  tab_footnote(\n    footnote = '\"gray\" can be replaced with \"grey\".',\n    locations = paste0(\n      \"cells_body(columns = name_\",\n      1:12,\n      \", rows = str_detect(name_\",\n      1:12,\n      ', pattern = \"gray\") & !str_detect(name_',\n      1:12,\n      ', pattern = \"slate.*[1-4]\"))'\n    ) |&gt;\n      paste(collapse = \", \") |&gt;\n      paste0(\n        \"list(\",\n        x = _,\n        \")\"\n      ) |&gt;\n      parse(text = _) |&gt;\n      eval()\n  ) |&gt;\n  opt_footnote_marks(marks = \"standard\") |&gt;\n  cols_label(\n    starts_with(\"name\") ~ \"Name\",\n    starts_with(\"fill\") ~ \"\"\n  ) |&gt;\n  tab_options(\n    table.margin.right = px(10),\n    table.margin.left = px(10),\n    data_row.padding = px(7)\n  ) |&gt;\n  gtsave(\n    \"20260102_builtin_colorname_gt.png\",\n    vwidth = 3500,\n    zoom = 1.2\n  )\n\n\n\n\n\nBuilt-in Color Name in R. (From grDevecies::colors().)"
  },
  {
    "objectID": "posts/20260102_built-in_color_name/index.html#conclusion",
    "href": "posts/20260102_built-in_color_name/index.html#conclusion",
    "title": "R 標準で用意されている色の名前",
    "section": "Conclusion",
    "text": "Conclusion\ncolors()で出せる、Rに標準で用意されている色の名前を見ていきました。grayとgreyが完全置換可能じゃないのは驚きました。"
  },
  {
    "objectID": "posts/20260102_built-in_color_name/index.html#session-infomation",
    "href": "posts/20260102_built-in_color_name/index.html#session-infomation",
    "title": "R 標準で用意されている色の名前",
    "section": "Session Infomation",
    "text": "Session Infomation\n\n\n\n\n\n\nNotesessioninfo\n\n\n\n\n\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Japanese_Japan.utf8  LC_CTYPE=Japanese_Japan.utf8    LC_MONETARY=Japanese_Japan.utf8\n[4] LC_NUMERIC=C                    LC_TIME=Japanese_Japan.utf8    \n\ntime zone: Asia/Tokyo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gdata_3.0.1     gt_1.2.0        lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [7] purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0    ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] sass_0.4.10        generics_0.1.4     xml2_1.4.0         gtools_3.9.5       stringi_1.8.7     \n [6] hms_1.1.3          digest_0.6.37      magrittr_2.0.3     evaluate_1.0.5     grid_4.4.2        \n[11] timechange_0.3.0   RColorBrewer_1.1-3 fastmap_1.2.0      jsonlite_2.0.0     processx_3.8.6    \n[16] chromote_0.5.1     ps_1.9.1           promises_1.3.3     crosstalk_1.2.1    scales_1.4.0      \n[21] cli_3.6.5          rlang_1.1.6        litedown_0.7       commonmark_2.0.0   reactR_0.6.1      \n[26] base64enc_0.1-3    withr_3.0.2        yaml_2.3.10        tools_4.4.2        tzdb_0.5.0        \n[31] pacman_0.5.1       vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4    reactable_0.4.4   \n[36] fs_1.6.6           htmlwidgets_1.6.4  pkgconfig_2.0.3    later_1.4.2        pillar_1.11.0     \n[41] gtable_0.3.6       Rcpp_1.1.0         glue_1.8.0         xfun_0.52          tidyselect_1.2.1  \n[46] rstudioapi_0.17.1  knitr_1.50         farver_2.1.2       websocket_1.4.4    htmltools_0.5.8.1 \n[51] webshot2_0.1.2     rmarkdown_2.29     compiler_4.4.2     markdown_2.0"
  },
  {
    "objectID": "posts/20260118_CFA_table_in_one_pipeline/index.html#packages",
    "href": "posts/20260118_CFA_table_in_one_pipeline/index.html#packages",
    "title": "R 確認的因子分析の表を作ってみた、1パイプラインで",
    "section": "packages",
    "text": "packages\n\npacman::p_load(\n  tidyverse,\n  lavaan,\n  psych,\n  gt\n)"
  },
  {
    "objectID": "posts/20260118_CFA_table_in_one_pipeline/index.html#contents",
    "href": "posts/20260118_CFA_table_in_one_pipeline/index.html#contents",
    "title": "R 確認的因子分析の表を作ってみた、1パイプラインで",
    "section": "Contents",
    "text": "Contents\nこれのことです。\n\n\nRの備忘録Rだけで確認的因子分析の表をつくってみた、1パイプラインでなお、1パイプラインで完結させること実用性はほとんどない模様技術的には可能ってやつ？ pic.twitter.com/pg5Flkj25t\n\n— Takuto SAKAI (@tsakai_psych) January 17, 2026\n\n\nタイトルの通りです。1パイプラインで完結するために、かなり無理した書き方をしているところがあります。わざわざ1パイプラインで完結する必要はありません。\n非常に長いので折りたたみました。見たい人は開いてみてください。\n\n\nCode\nbfi |&gt;\n  relocate(\n    age, gender, education\n  ) |&gt;\n  mutate(\n    across(\n      .cols = bfi.dictionary |&gt;\n        subset(\n          Keying == -1\n        ) |&gt;\n        rownames(),\n      .fns = \\(x) {\n        7 - x\n      }\n    )\n  ) |&gt; \n  rownames_to_column(var = \"id\") |&gt;\n  group_walk(\n    .f = \\(x, idx) {\n      x |&gt;\n        pivot_longer(\n          cols = matches(\"\\\\w\\\\d\"),\n          names_to = \"items\"\n        ) |&gt;\n        mutate(\n          nest_item = str_extract(\n            items,\n            pattern = \"^\\\\w\"\n          )\n        ) |&gt;\n        nest(.by = nest_item) |&gt;\n        mutate(\n          res = map(\n            .x = data,\n            .f = \\(x) {\n              x |&gt;\n                pivot_wider(\n                  names_from = items,\n                  values_from = value\n                ) |&gt;\n                select(matches(\"\\\\w\\\\d\")) |&gt;\n                psych::alpha() |&gt;\n                _$total |&gt;\n                _$raw_alpha |&gt;\n                round(digits = 3) |&gt;\n                sprintf(\"%.3f\", x = _) |&gt;\n                str_remove(pattern = \"^0\")\n            }\n          ) |&gt;\n            set_names(nest_item),\n          .by = nest_item\n        ) |&gt;\n        pull(res) |&gt;\n        assign(\n          \"res_alpha\",\n          value = _,\n          envir = .GlobalEnv\n        )\n    }\n  ) |&gt;\n  cfa(\n    model = '\n      Agreeableness =~ A1 + A2 + A3 + A4 + A5\n      Conscientiousness =~ C1 + C2 + C3 + C4 + C5\n      Extraversion =~ E1 + E2 + E3 + E4 + E5\n      Neuroticism =~ N1 + N2 + N3 + N4 + N5\n      Opennness =~ O1 + O2 + O3 + O4 + O5\n    ',\n    data = _,\n    std.lv = TRUE\n  ) |&gt;\n  assign(\n    \"fit_bfi\",\n    value = _\n  ) |&gt;\n  inspect(\n    what = \"std.all\"\n  ) |&gt;\n  _$lambda |&gt;\n  unclass() |&gt;\n  as_tibble(\n    rownames = \"items\"\n  ) |&gt;\n  arrange(\n    across(\n      .cols = c(Agreeableness, Conscientiousness, Extraversion, Neuroticism, Opennness),\n      .fns = ~desc(.x)\n    )\n  ) |&gt;\n  left_join(\n    y = bfi.dictionary |&gt;\n      as_tibble(rownames = \"items\") |&gt;\n      select(items, Item, Keying),\n    by = join_by(\"items\")\n  ) |&gt;\n  relocate(\n    item_label = Item,\n    is_rev = Keying,\n    .after = items\n  ) |&gt;\n  mutate(\n    is_rev = if_else(\n      is_rev &gt; 0,\n      \"\",\n      \"rev\"\n    )\n  ) |&gt;\n  bind_rows(\n    y = fit_bfi |&gt;\n      inspect(what = \"std.all\") |&gt;\n      _$psi |&gt;\n      unclass() |&gt;\n      (\\(x) {\n        `[&lt;-`(x, lower.tri(x, diag = TRUE), NA)\n      }) () |&gt;\n      as_tibble(\n        rownames = \"item_label\"\n      ) |&gt;\n      mutate(\n        items = c(\"I\", \"II\", \"III\", \"IV\", \"V\")\n      ),\n    .id = \"data_from\"\n  ) |&gt;\n  group_by(data_from) |&gt;\n  gt() |&gt;\n  fmt_number(\n    decimals = 3\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      weight = \"bold\",\n      font = \"Times New Roman\"\n    ),\n    locations = list(\n      cells_body(\n        columns = Agreeableness,\n        rows = 1:5\n      ),\n      cells_body(\n        columns = Conscientiousness,\n        rows = 6:10\n      ),\n      cells_body(\n        columns = Extraversion,\n        rows = 11:15\n      ),\n      cells_body(\n        columns = Neuroticism,\n        rows = 16:20\n      ),\n      cells_body(\n        columns = Opennness,\n        rows = 21:25\n      )\n    )\n  ) |&gt;\n  tab_stubhead(\n    label = \"番号\"\n  ) |&gt;\n  tab_header(\n    title = \"表X. 確認的因子分析の表をRだけで作りたい\",\n    subtitle = md(\"なんか前処理から表まで１パイプラインで完結できたかもしれない……（これ実用性あるの？）\")\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"data from: `psych::bfi`; CFA with `lavaan::cfa()`; table by `gt()`; font: Times New Roman/ Noto Serif JP\"\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = md(\"因子間相関は`dplyr::bind_rows()`で無理やりくっつけた\"),\n    locations = cells_row_groups(\n      groups = 2\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = md(\"&alpha;係数の算出は、非grouped dfを`dplyr::group_walk()`に入れることで無理やり解決\"),\n    locations = cells_body(\n      columns = item_label,\n      rows = 26:30\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(\n      align = \"left\"\n    ),\n    locations = cells_body(\n      columns = c(items, item_label)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"bottom\",\n      style = \"hidden\"\n    ),\n    locations = cells_row_groups()\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      font = \"Times New Roman\"\n    ),\n    locations = list(\n      cells_body(\n        columns = c(items, item_label),\n        rows = 1:25\n      ),\n      cells_body(\n        columns = Agreeableness:Opennness,\n        rows = 26:30\n      ),\n      cells_source_notes()\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      align = \"center\"\n    ),\n    locations = list(\n      cells_column_labels(columns = Agreeableness:Opennness),\n      cells_body(columns = Agreeableness:Opennness)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      size = \"small\"\n    ),\n    locations = list(\n      cells_footnotes(),\n      cells_source_notes()\n    )\n  ) |&gt;\n  sub_zero(\n    zero_text = \"\"\n  ) |&gt;\n  sub_missing(\n    missing_text = \"\"\n  ) |&gt;\n  text_transform(\n    fn = \\(x) {\n      str_remove(\n        x,\n        pattern = \"0(?=\\\\.)\"\n      )\n    }\n  ) |&gt;\n  text_replace(\n    pattern = \"rev\",\n    replacement = \"(逆)\",\n    locations = cells_body(\n      columns = is_rev\n    )\n  ) |&gt;\n  text_case_match(\n    \"1\" ~ \"\",\n    \"2\" ~ \"因子間相関\",\n    .locations = cells_row_groups()\n  ) |&gt;\n  text_case_match(\n    \"Agreeableness\" ~ \"I\",\n    \"Conscientiousness\" ~ \"II\",\n    \"Extraversion\" ~ \"III\",\n    \"Neuroticism\" ~ \"IV\",\n    \"Opennness\" ~ \"V\",\n    .locations = cells_column_labels()\n  ) |&gt;\n  text_case_match(\n    \"Agreeableness\" ~ paste0(\n      \"協調性（&alpha; = \", res_alpha$A, \")\"\n    ) |&gt;\n      md(),\n    \"Conscientiousness\" ~ paste0(\n      \"勤勉性（&alpha; = \", res_alpha$C, \")\"\n    ) |&gt;\n      md(),\n    \"Extraversion\" ~ paste0(\n      \"外向性（&alpha; = \", res_alpha$E, \"）\"\n    ) |&gt;\n      md(),\n    \"Neuroticism\" ~ paste0(\n      \"情緒安定性（&alpha; = \", res_alpha$N, \"）\"\n    ) |&gt;\n      md(),\n    \"Opennness\" ~ paste0(\n      \"開放性（&alpha; = \", res_alpha$O, \")\"\n    ) |&gt;\n      md(),\n    .locations = cells_body(columns = item_label)\n  ) |&gt;\n  cols_label(\n    items = \"番号\",\n    item_label = \"項目\",\n    is_rev = \"\"\n  ) |&gt;\n  cols_width(\n    item_label ~ px(270)\n  ) |&gt; \n  tab_options(\n    table.font.names = \"Noto Serif JP\",\n    heading.border.bottom.color = \"black\",\n    heading.border.bottom.width = px(2),\n    table.border.top.style = \"hidden\",\n    table.border.bottom.width = px(2),\n    table.border.bottom.color = \"black\",\n    table_body.hlines.style = \"hidden\",\n    table_body.border.top.color = \"black\",\n    table_body.border.top.width = px(1),\n    table_body.border.bottom.color = \"black\",\n    table_body.border.bottom.width = px(1),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(1),\n    row_group.border.top.color = \"black\",\n    row_group.border.top.width = px(1),\n    data_row.padding = px(3),\n    heading.align = \"left\",\n    table.margin.left = px(5),\n    table.margin.right = px(5)\n  )\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      表X. 確認的因子分析の表をRだけで作りたい\n    \n    \n      なんか前処理から表まで１パイプラインで完結できたかもしれない……（これ実用性あるの？）\n    \n    \n      番号\n      項目\n      \n      I\n      II\n      III\n      IV\n      V\n    \n  \n  \n    \n      \n    \n    A3\nKnow how to comfort others.\n\n.749\n\n\n\n\n    A5\nMake people feel at ease.\n\n.687\n\n\n\n\n    A2\nInquire about others' well-being.\n\n.648\n\n\n\n\n    A4\nLove children.\n\n.510\n\n\n\n\n    A1\nAm indifferent to the feelings of others.\n(逆)\n.344\n\n\n\n\n    C4\nDo things in a half-way manner.\n(逆)\n\n.702\n\n\n\n    C5\nWaste my time.\n(逆)\n\n.620\n\n\n\n    C2\nContinue until everything is perfect.\n\n\n.592\n\n\n\n    C1\nAm exacting in my work.\n\n\n.551\n\n\n\n    C3\nDo things according to a plan.\n\n\n.546\n\n\n\n    E4\nMake friends easily.\n\n\n\n.703\n\n\n    E2\nFind it difficult to approach others.\n(逆)\n\n\n.699\n\n\n    E3\nKnow how to captivate people.\n\n\n\n.627\n\n\n    E1\nDon't talk a lot.\n(逆)\n\n\n.564\n\n\n    E5\nTake charge.\n\n\n\n.553\n\n\n    N1\nGet angry easily.\n(逆)\n\n\n\n.825\n\n    N2\nGet irritated easily.\n(逆)\n\n\n\n.803\n\n    N3\nHave frequent mood swings.\n(逆)\n\n\n\n.721\n\n    N4\nOften feel blue.\n(逆)\n\n\n\n.573\n\n    N5\nPanic easily.\n(逆)\n\n\n\n.503\n\n    O3\nCarry the conversation to a higher level.\n\n\n\n\n\n.724\n    O1\nAm full of ideas.\n\n\n\n\n\n.564\n    O5\nWill not probe deeply into a subject.\n(逆)\n\n\n\n\n.461\n    O2\nAvoid difficult reading material.\n(逆)\n\n\n\n\n.418\n    O4\nSpend time reflecting on things.\n\n\n\n\n\n.233\n    \n      因子間相関1\n    \n    I\n協調性（α = .703)2\n\n\n.334\n.683\n.223\n.303\n    II\n勤勉性（α = .727)2\n\n\n\n.357\n.283\n.301\n    III\n外向性（α = .762）2\n\n\n\n\n.244\n.453\n    IV\n情緒安定性（α = .814）2\n\n\n\n\n\n.112\n    V\n開放性（α = .600)2\n\n\n\n\n\n\n  \n  \n    \n      1 因子間相関はdplyr::bind_rows()で無理やりくっつけた\n    \n    \n      2 α係数の算出は、非grouped dfをdplyr::group_walk()に入れることで無理やり解決\n    \n    \n      data from: psych::bfi; CFA with lavaan::cfa(); table by gt(); font: Times New Roman/ Noto Serif JP\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote処理の過程を見たい人向け（長いよ）\n\n\n\n\n\nやってること\n\n元データの前処理（逆転処理）\ndplyr::group_walk()に非grouped dfを突っ込んで、中でα係数の計算をこなして.GrobalEnvに結果を保存しつつ、元のdfをそのまま返してもらう\nlavaan::cfa()で確認的因子分析をして、因子負荷量を抽出する\n\n後で使うので、CFAの結果をassign()で.GrobalEnvに保存しつつ、そのまま処理を続ける\n\ndplyr::left_join()で、psych::bfi.dictionaryから質問項目と逆転項目かどうかの列をくっつける\ndplyr::bind_rows()で、因子間相関のdfを無理やりくっつける\n\nCFAの結果はすでに保存してあるので、そこから抽出\n\ngt::gt()で表づくり\n\n保存したα係数をくっつける\n\n\ngt::gt()とViewPipeSteps::prit_pipe_step()の相性が悪いので、gt()の前まで。\n\n\nCode\noptions(pillar.print_max = 30) # for better tibble displaying\n\nbfi %&gt;%\n  relocate(\n    age, gender, education\n  ) %&gt;%\n  mutate(\n    across(\n      .cols = bfi.dictionary %&gt;%\n        subset(\n          Keying == -1\n        ) %&gt;%\n        rownames(),\n      .fns = \\(x) {\n        7 - x\n      }\n    )\n  ) %&gt;% \n  rownames_to_column(var = \"id\") %&gt;%\n  group_walk(\n    .f = \\(x, idx) {\n      x %&gt;%\n        pivot_longer(\n          cols = matches(\"\\\\w\\\\d\"),\n          names_to = \"items\"\n        ) %&gt;%\n        mutate(\n          nest_item = str_extract(\n            items,\n            pattern = \"^\\\\w\"\n          )\n        ) %&gt;%\n        nest(.by = nest_item) %&gt;%\n        mutate(\n          res = map(\n            .x = data,\n            .f = \\(x) {\n              x %&gt;%\n                pivot_wider(\n                  names_from = items,\n                  values_from = value\n                ) %&gt;%\n                select(matches(\"\\\\w\\\\d\")) %&gt;%\n                psych::alpha() %&gt;%\n                .$total %&gt;%\n                .$raw_alpha %&gt;%\n                round(digits = 3) %&gt;%\n                sprintf(\"%.3f\", x = .) %&gt;%\n                str_remove(pattern = \"^0\")\n            }\n          ) %&gt;%\n            set_names(nest_item),\n          .by = nest_item\n        ) %&gt;%\n        pull(res) %&gt;%\n        assign(\n          \"res_alpha\",\n          value = .,\n          envir = .GlobalEnv\n        )\n    }\n  ) %&gt;%\n  lavaan::cfa(\n    model = '\n      Agreeableness =~ A1 + A2 + A3 + A4 + A5\n      Conscientiousness =~ C1 + C2 + C3 + C4 + C5\n      Extraversion =~ E1 + E2 + E3 + E4 + E5\n      Neuroticism =~ N1 + N2 + N3 + N4 + N5\n      Opennness =~ O1 + O2 + O3 + O4 + O5\n    ',\n    data = .,\n    std.lv = TRUE\n  ) %&gt;%\n  assign(\n    \"fit_bfi\",\n    value = .,\n    envir = .GlobalEnv\n  ) %&gt;%\n  inspect(\n    what = \"std.all\"\n  ) %&gt;%\n  .$lambda %&gt;%\n  unclass() %&gt;%\n  as_tibble(\n    rownames = \"items\"\n  ) %&gt;%\n  arrange(\n    across(\n      .cols = c(Agreeableness, Conscientiousness, Extraversion, Neuroticism, Opennness),\n      .fns = ~desc(.x)\n    )\n  ) %&gt;%\n  left_join(\n    y = bfi.dictionary %&gt;%\n      as_tibble(rownames = \"items\") %&gt;%\n      select(items, Item, Keying),\n    by = join_by(\"items\")\n  ) %&gt;%\n  relocate(\n    item_label = Item,\n    is_rev = Keying,\n    .after = items\n  ) %&gt;%\n  mutate(\n    is_rev = if_else(\n      is_rev &gt; 0,\n      \"\",\n      \"rev\"\n    )\n  ) %&gt;%\n  bind_rows(\n    y = fit_bfi %&gt;%\n      inspect(what = \"std.all\") %&gt;%\n      .$psi %&gt;%\n      unclass() %&gt;%\n      (\\(x) {\n        `[&lt;-`(x, lower.tri(x, diag = TRUE), NA)\n      }) () %&gt;%\n      as_tibble(\n        rownames = \"item_label\"\n      ) %&gt;%\n      mutate(\n        items = c(\"I\", \"II\", \"III\", \"IV\", \"V\")\n      ),\n    .id = \"data_from\"\n  ) %&gt;%\n  group_by(data_from) %&gt;% \n  ViewPipeSteps::print_pipe_steps()\n\n\n1. bfi\n\n\n# A tibble: 2,800 × 28\n      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     2     4     3     4     4     2     3     3     4     4     3     3     3     4     4     3\n 2     2     4     5     2     5     5     4     4     3     4     1     1     6     4     3     3\n 3     5     4     5     4     4     4     5     4     2     5     2     4     4     4     5     4\n 4     4     4     6     5     5     4     4     3     5     5     5     3     4     4     4     2\n 5     2     3     3     4     5     4     4     5     3     2     2     2     5     4     5     2\n 6     6     6     5     6     5     6     6     6     1     3     2     1     6     5     6     3\n 7     2     5     5     3     5     5     4     4     2     3     4     3     4     5     5     1\n 8     4     3     1     5     1     3     2     4     2     4     3     6     4     2     1     6\n 9     4     3     6     3     3     6     6     3     4     5     5     3    NA     4     3     5\n10     2     5     6     6     5     6     5     6     2     1     2     2     4     5     5     5\n# ℹ 2,790 more rows\n# ℹ 12 more variables: N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;int&gt;, O3 &lt;int&gt;,\n#   O4 &lt;int&gt;, O5 &lt;int&gt;, gender &lt;int&gt;, education &lt;int&gt;, age &lt;int&gt;\n\n\n2. relocate(age, gender, education)\n\n\n# A tibble: 2,800 × 28\n     age gender education    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n   &lt;int&gt;  &lt;int&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    16      1        NA     2     4     3     4     4     2     3     3     4     4     3     3\n 2    18      2        NA     2     4     5     2     5     5     4     4     3     4     1     1\n 3    17      2        NA     5     4     5     4     4     4     5     4     2     5     2     4\n 4    17      2        NA     4     4     6     5     5     4     4     3     5     5     5     3\n 5    17      1        NA     2     3     3     4     5     4     4     5     3     2     2     2\n 6    21      2         3     6     6     5     6     5     6     6     6     1     3     2     1\n 7    18      1        NA     2     5     5     3     5     5     4     4     2     3     4     3\n 8    19      1         2     4     3     1     5     1     3     2     4     2     4     3     6\n 9    19      1         1     4     3     6     3     3     6     6     3     4     5     5     3\n10    17      2        NA     2     5     6     6     5     6     5     6     2     1     2     2\n# ℹ 2,790 more rows\n# ℹ 13 more variables: E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;,\n#   N5 &lt;int&gt;, O1 &lt;int&gt;, O2 &lt;int&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;int&gt;\n\n\n3. mutate(across(.cols = bfi.dictionary %&gt;% subset(Keying == -1) %&gt;% rownames(), .fns = function(x) {\n    7 - x\n}))\n\n\n# A tibble: 2,800 × 28\n     age gender education    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2\n   &lt;int&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    16      1        NA     5     4     3     4     4     2     3     3     3     3     4     4\n 2    18      2        NA     5     4     5     2     5     5     4     4     4     3     6     6\n 3    17      2        NA     2     4     5     4     4     4     5     4     5     2     5     3\n 4    17      2        NA     3     4     6     5     5     4     4     3     2     2     2     4\n 5    17      1        NA     5     3     3     4     5     4     4     5     4     5     5     5\n 6    21      2         3     1     6     5     6     5     6     6     6     6     4     5     6\n 7    18      1        NA     5     5     5     3     5     5     4     4     5     4     3     4\n 8    19      1         2     3     3     1     5     1     3     2     4     5     3     4     1\n 9    19      1         1     3     3     6     3     3     6     6     3     3     2     2     4\n10    17      2        NA     5     5     6     6     5     6     5     6     5     6     5     5\n# ℹ 2,790 more rows\n# ℹ 13 more variables: E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;dbl&gt;, N2 &lt;dbl&gt;, N3 &lt;dbl&gt;, N4 &lt;dbl&gt;,\n#   N5 &lt;dbl&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\n4. rownames_to_column(var = \"id\")\n\n\n# A tibble: 2,800 × 29\n   id      age gender education    A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1\n   &lt;chr&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 61617    16      1        NA     5     4     3     4     4     2     3     3     3     3     4\n 2 61618    18      2        NA     5     4     5     2     5     5     4     4     4     3     6\n 3 61620    17      2        NA     2     4     5     4     4     4     5     4     5     2     5\n 4 61621    17      2        NA     3     4     6     5     5     4     4     3     2     2     2\n 5 61622    17      1        NA     5     3     3     4     5     4     4     5     4     5     5\n 6 61623    21      2         3     1     6     5     6     5     6     6     6     6     4     5\n 7 61624    18      1        NA     5     5     5     3     5     5     4     4     5     4     3\n 8 61629    19      1         2     3     3     1     5     1     3     2     4     5     3     4\n 9 61630    19      1         1     3     3     6     3     3     6     6     3     3     2     2\n10 61633    17      2        NA     5     5     6     6     5     6     5     6     5     6     5\n# ℹ 2,790 more rows\n# ℹ 14 more variables: E2 &lt;dbl&gt;, E3 &lt;int&gt;, E4 &lt;int&gt;, E5 &lt;int&gt;, N1 &lt;dbl&gt;, N2 &lt;dbl&gt;, N3 &lt;dbl&gt;,\n#   N4 &lt;dbl&gt;, N5 &lt;dbl&gt;, O1 &lt;int&gt;, O2 &lt;dbl&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;dbl&gt;\n\n\n6. lavaan::cfa(model = \"\\n      Agreeableness =~ A1 + A2 + A3 + A4 + A5\\n      Conscientiousness =~ C1 + C2 + C3 + C4 + C5\\n      Extraversion =~ E1 + E2 + E3 + E4 + E5\\n      Neuroticism =~ N1 + N2 + N3 + N4 + N5\\n      Opennness =~ O1 + O2 + O3 + O4 + O5\\n    \", data = ., std.lv = TRUE)\n\n\nlavaan 0.6-19 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        60\n\n                                                  Used       Total\n  Number of observations                          2436        2800\n\nModel Test User Model:\n                                                      \n  Test statistic                              4165.467\n  Degrees of freedom                               265\n  P-value (Chi-square)                           0.000\n\n\n8. inspect(what = \"std.all\")\n\n\n$lambda\n   Agrbln Cnscnt Extrvr Nrtcsm Opnnns\nA1  0.344  0.000  0.000  0.000  0.000\nA2  0.648  0.000  0.000  0.000  0.000\nA3  0.749  0.000  0.000  0.000  0.000\nA4  0.510  0.000  0.000  0.000  0.000\nA5  0.687  0.000  0.000  0.000  0.000\nC1  0.000  0.551  0.000  0.000  0.000\nC2  0.000  0.592  0.000  0.000  0.000\nC3  0.000  0.546  0.000  0.000  0.000\nC4  0.000  0.702  0.000  0.000  0.000\nC5  0.000  0.620  0.000  0.000  0.000\nE1  0.000  0.000  0.564  0.000  0.000\nE2  0.000  0.000  0.699  0.000  0.000\nE3  0.000  0.000  0.627  0.000  0.000\nE4  0.000  0.000  0.703  0.000  0.000\nE5  0.000  0.000  0.553  0.000  0.000\nN1  0.000  0.000  0.000  0.825  0.000\nN2  0.000  0.000  0.000  0.803  0.000\nN3  0.000  0.000  0.000  0.721  0.000\nN4  0.000  0.000  0.000  0.573  0.000\nN5  0.000  0.000  0.000  0.503  0.000\nO1  0.000  0.000  0.000  0.000  0.564\nO2  0.000  0.000  0.000  0.000  0.418\nO3  0.000  0.000  0.000  0.000  0.724\nO4  0.000  0.000  0.000  0.000  0.233\nO5  0.000  0.000  0.000  0.000  0.461\n\n$theta\n      A1    A2    A3    A4    A5    C1    C2    C3    C4    C5    E1    E2    E3    E4    E5    N1\nA1 0.882                                                                                          \nA2 0.000 0.580                                                                                    \nA3 0.000 0.000 0.438                                                                              \nA4 0.000 0.000 0.000 0.740                                                                        \nA5 0.000 0.000 0.000 0.000 0.528                                                                  \nC1 0.000 0.000 0.000 0.000 0.000 0.697                                                            \nC2 0.000 0.000 0.000 0.000 0.000 0.000 0.650                                                      \nC3 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.702                                                \nC4 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.507                                          \nC5 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.615                                    \nE1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.682                              \nE2 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.512                        \nE3 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.607                  \nE4 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.506            \nE5 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.694      \nN1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.320\nN2 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nN3 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nN4 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nN5 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nO1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nO2 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nO3 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nO4 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nO5 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n      N2    N3    N4    N5    O1    O2    O3    O4    O5\nA1                                                      \nA2                                                      \nA3                                                      \nA4                                                      \nA5                                                      \nC1                                                      \nC2                                                      \nC3                                                      \nC4                                                      \nC5                                                      \nE1                                                      \nE2                                                      \nE3                                                      \nE4                                                      \nE5                                                      \nN1                                                      \nN2 0.356                                                \nN3 0.000 0.481                                          \nN4 0.000 0.000 0.672                                    \nN5 0.000 0.000 0.000 0.747                              \nO1 0.000 0.000 0.000 0.000 0.682                        \nO2 0.000 0.000 0.000 0.000 0.000 0.826                  \nO3 0.000 0.000 0.000 0.000 0.000 0.000 0.476            \nO4 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.946      \nO5 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.788\n\n$psi\n                  Agrbln Cnscnt Extrvr Nrtcsm Opnnns\nAgreeableness      1.000                            \nConscientiousness  0.334  1.000                     \nExtraversion       0.683  0.357  1.000              \nNeuroticism        0.223  0.283  0.244  1.000       \nOpennness          0.303  0.301  0.453  0.112  1.000\n\n\n9. .$lambda\n\n\n   Agrbln Cnscnt Extrvr Nrtcsm Opnnns\nA1  0.344  0.000  0.000  0.000  0.000\nA2  0.648  0.000  0.000  0.000  0.000\nA3  0.749  0.000  0.000  0.000  0.000\nA4  0.510  0.000  0.000  0.000  0.000\nA5  0.687  0.000  0.000  0.000  0.000\nC1  0.000  0.551  0.000  0.000  0.000\nC2  0.000  0.592  0.000  0.000  0.000\nC3  0.000  0.546  0.000  0.000  0.000\nC4  0.000  0.702  0.000  0.000  0.000\nC5  0.000  0.620  0.000  0.000  0.000\nE1  0.000  0.000  0.564  0.000  0.000\nE2  0.000  0.000  0.699  0.000  0.000\nE3  0.000  0.000  0.627  0.000  0.000\nE4  0.000  0.000  0.703  0.000  0.000\nE5  0.000  0.000  0.553  0.000  0.000\nN1  0.000  0.000  0.000  0.825  0.000\nN2  0.000  0.000  0.000  0.803  0.000\nN3  0.000  0.000  0.000  0.721  0.000\nN4  0.000  0.000  0.000  0.573  0.000\nN5  0.000  0.000  0.000  0.503  0.000\nO1  0.000  0.000  0.000  0.000  0.564\nO2  0.000  0.000  0.000  0.000  0.418\nO3  0.000  0.000  0.000  0.000  0.724\nO4  0.000  0.000  0.000  0.000  0.233\nO5  0.000  0.000  0.000  0.000  0.461\n\n\n10. unclass()\n\n\n   Agreeableness Conscientiousness Extraversion Neuroticism Opennness\nA1     0.3440969         0.0000000    0.0000000   0.0000000 0.0000000\nA2     0.6480625         0.0000000    0.0000000   0.0000000 0.0000000\nA3     0.7494317         0.0000000    0.0000000   0.0000000 0.0000000\nA4     0.5099514         0.0000000    0.0000000   0.0000000 0.0000000\nA5     0.6873613         0.0000000    0.0000000   0.0000000 0.0000000\nC1     0.0000000         0.5507480    0.0000000   0.0000000 0.0000000\nC2     0.0000000         0.5919409    0.0000000   0.0000000 0.0000000\nC3     0.0000000         0.5459679    0.0000000   0.0000000 0.0000000\nC4     0.0000000         0.7022903    0.0000000   0.0000000 0.0000000\nC5     0.0000000         0.6202606    0.0000000   0.0000000 0.0000000\nE1     0.0000000         0.0000000    0.5640632   0.0000000 0.0000000\nE2     0.0000000         0.0000000    0.6988480   0.0000000 0.0000000\nE3     0.0000000         0.0000000    0.6270618   0.0000000 0.0000000\nE4     0.0000000         0.0000000    0.7031669   0.0000000 0.0000000\nE5     0.0000000         0.0000000    0.5533894   0.0000000 0.0000000\nN1     0.0000000         0.0000000    0.0000000   0.8249066 0.0000000\nN2     0.0000000         0.0000000    0.0000000   0.8027075 0.0000000\nN3     0.0000000         0.0000000    0.0000000   0.7205144 0.0000000\nN4     0.0000000         0.0000000    0.0000000   0.5729313 0.0000000\nN5     0.0000000         0.0000000    0.0000000   0.5027198 0.0000000\nO1     0.0000000         0.0000000    0.0000000   0.0000000 0.5641192\nO2     0.0000000         0.0000000    0.0000000   0.0000000 0.4175189\nO3     0.0000000         0.0000000    0.0000000   0.0000000 0.7239194\nO4     0.0000000         0.0000000    0.0000000   0.0000000 0.2325609\nO5     0.0000000         0.0000000    0.0000000   0.0000000 0.4606394\n\n\n11. as_tibble(rownames = \"items\")\n\n\n# A tibble: 25 × 6\n   items Agreeableness Conscientiousness Extraversion Neuroticism Opennness\n   &lt;chr&gt;         &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 A1            0.344             0            0           0         0    \n 2 A2            0.648             0            0           0         0    \n 3 A3            0.749             0            0           0         0    \n 4 A4            0.510             0            0           0         0    \n 5 A5            0.687             0            0           0         0    \n 6 C1            0                 0.551        0           0         0    \n 7 C2            0                 0.592        0           0         0    \n 8 C3            0                 0.546        0           0         0    \n 9 C4            0                 0.702        0           0         0    \n10 C5            0                 0.620        0           0         0    \n11 E1            0                 0            0.564       0         0    \n12 E2            0                 0            0.699       0         0    \n13 E3            0                 0            0.627       0         0    \n14 E4            0                 0            0.703       0         0    \n15 E5            0                 0            0.553       0         0    \n16 N1            0                 0            0           0.825     0    \n17 N2            0                 0            0           0.803     0    \n18 N3            0                 0            0           0.721     0    \n19 N4            0                 0            0           0.573     0    \n20 N5            0                 0            0           0.503     0    \n21 O1            0                 0            0           0         0.564\n22 O2            0                 0            0           0         0.418\n23 O3            0                 0            0           0         0.724\n24 O4            0                 0            0           0         0.233\n25 O5            0                 0            0           0         0.461\n\n\n12. arrange(across(.cols = c(Agreeableness, Conscientiousness, Extraversion, Neuroticism, Opennness), .fns = ~desc(.x)))\n\n\n# A tibble: 25 × 6\n   items Agreeableness Conscientiousness Extraversion Neuroticism Opennness\n   &lt;chr&gt;         &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 A3            0.749             0            0           0         0    \n 2 A5            0.687             0            0           0         0    \n 3 A2            0.648             0            0           0         0    \n 4 A4            0.510             0            0           0         0    \n 5 A1            0.344             0            0           0         0    \n 6 C4            0                 0.702        0           0         0    \n 7 C5            0                 0.620        0           0         0    \n 8 C2            0                 0.592        0           0         0    \n 9 C1            0                 0.551        0           0         0    \n10 C3            0                 0.546        0           0         0    \n11 E4            0                 0            0.703       0         0    \n12 E2            0                 0            0.699       0         0    \n13 E3            0                 0            0.627       0         0    \n14 E1            0                 0            0.564       0         0    \n15 E5            0                 0            0.553       0         0    \n16 N1            0                 0            0           0.825     0    \n17 N2            0                 0            0           0.803     0    \n18 N3            0                 0            0           0.721     0    \n19 N4            0                 0            0           0.573     0    \n20 N5            0                 0            0           0.503     0    \n21 O3            0                 0            0           0         0.724\n22 O1            0                 0            0           0         0.564\n23 O5            0                 0            0           0         0.461\n24 O2            0                 0            0           0         0.418\n25 O4            0                 0            0           0         0.233\n\n\n13. left_join(y = bfi.dictionary %&gt;% as_tibble(rownames = \"items\") %&gt;% select(items, Item, Keying), by = join_by(\"items\"))\n\n\n# A tibble: 25 × 8\n   items Agreeableness Conscientiousness Extraversion Neuroticism Opennness Item              Keying\n   &lt;chr&gt;         &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;              &lt;int&gt;\n 1 A3            0.749             0            0           0         0     Know how to comf…      1\n 2 A5            0.687             0            0           0         0     Make people feel…      1\n 3 A2            0.648             0            0           0         0     Inquire about ot…      1\n 4 A4            0.510             0            0           0         0     Love children.         1\n 5 A1            0.344             0            0           0         0     Am indifferent t…     -1\n 6 C4            0                 0.702        0           0         0     Do things in a h…     -1\n 7 C5            0                 0.620        0           0         0     Waste my time.        -1\n 8 C2            0                 0.592        0           0         0     Continue until e…      1\n 9 C1            0                 0.551        0           0         0     Am exacting in m…      1\n10 C3            0                 0.546        0           0         0     Do things accord…      1\n11 E4            0                 0            0.703       0         0     Make friends eas…      1\n12 E2            0                 0            0.699       0         0     Find it difficul…     -1\n13 E3            0                 0            0.627       0         0     Know how to capt…      1\n14 E1            0                 0            0.564       0         0     Don't talk a lot.     -1\n15 E5            0                 0            0.553       0         0     Take charge.           1\n16 N1            0                 0            0           0.825     0     Get angry easily.     -1\n17 N2            0                 0            0           0.803     0     Get irritated ea…     -1\n18 N3            0                 0            0           0.721     0     Have frequent mo…     -1\n19 N4            0                 0            0           0.573     0     Often feel blue.      -1\n20 N5            0                 0            0           0.503     0     Panic easily.         -1\n21 O3            0                 0            0           0         0.724 Carry the conver…      1\n22 O1            0                 0            0           0         0.564 Am full of ideas.      1\n23 O5            0                 0            0           0         0.461 Will not probe d…     -1\n24 O2            0                 0            0           0         0.418 Avoid difficult …     -1\n25 O4            0                 0            0           0         0.233 Spend time refle…      1\n\n\n14. relocate(item_label = Item, is_rev = Keying, .after = items)\n\n\n# A tibble: 25 × 8\n   items item_label        is_rev Agreeableness Conscientiousness Extraversion Neuroticism Opennness\n   &lt;chr&gt; &lt;fct&gt;              &lt;int&gt;         &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 A3    Know how to comf…      1         0.749             0            0           0         0    \n 2 A5    Make people feel…      1         0.687             0            0           0         0    \n 3 A2    Inquire about ot…      1         0.648             0            0           0         0    \n 4 A4    Love children.         1         0.510             0            0           0         0    \n 5 A1    Am indifferent t…     -1         0.344             0            0           0         0    \n 6 C4    Do things in a h…     -1         0                 0.702        0           0         0    \n 7 C5    Waste my time.        -1         0                 0.620        0           0         0    \n 8 C2    Continue until e…      1         0                 0.592        0           0         0    \n 9 C1    Am exacting in m…      1         0                 0.551        0           0         0    \n10 C3    Do things accord…      1         0                 0.546        0           0         0    \n11 E4    Make friends eas…      1         0                 0            0.703       0         0    \n12 E2    Find it difficul…     -1         0                 0            0.699       0         0    \n13 E3    Know how to capt…      1         0                 0            0.627       0         0    \n14 E1    Don't talk a lot.     -1         0                 0            0.564       0         0    \n15 E5    Take charge.           1         0                 0            0.553       0         0    \n16 N1    Get angry easily.     -1         0                 0            0           0.825     0    \n17 N2    Get irritated ea…     -1         0                 0            0           0.803     0    \n18 N3    Have frequent mo…     -1         0                 0            0           0.721     0    \n19 N4    Often feel blue.      -1         0                 0            0           0.573     0    \n20 N5    Panic easily.         -1         0                 0            0           0.503     0    \n21 O3    Carry the conver…      1         0                 0            0           0         0.724\n22 O1    Am full of ideas.      1         0                 0            0           0         0.564\n23 O5    Will not probe d…     -1         0                 0            0           0         0.461\n24 O2    Avoid difficult …     -1         0                 0            0           0         0.418\n25 O4    Spend time refle…      1         0                 0            0           0         0.233\n\n\n15. mutate(is_rev = if_else(is_rev &gt; 0, \"\", \"rev\"))\n\n\n# A tibble: 25 × 8\n   items item_label        is_rev Agreeableness Conscientiousness Extraversion Neuroticism Opennness\n   &lt;chr&gt; &lt;fct&gt;             &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 A3    Know how to comf… \"\"             0.749             0            0           0         0    \n 2 A5    Make people feel… \"\"             0.687             0            0           0         0    \n 3 A2    Inquire about ot… \"\"             0.648             0            0           0         0    \n 4 A4    Love children.    \"\"             0.510             0            0           0         0    \n 5 A1    Am indifferent t… \"rev\"          0.344             0            0           0         0    \n 6 C4    Do things in a h… \"rev\"          0                 0.702        0           0         0    \n 7 C5    Waste my time.    \"rev\"          0                 0.620        0           0         0    \n 8 C2    Continue until e… \"\"             0                 0.592        0           0         0    \n 9 C1    Am exacting in m… \"\"             0                 0.551        0           0         0    \n10 C3    Do things accord… \"\"             0                 0.546        0           0         0    \n11 E4    Make friends eas… \"\"             0                 0            0.703       0         0    \n12 E2    Find it difficul… \"rev\"          0                 0            0.699       0         0    \n13 E3    Know how to capt… \"\"             0                 0            0.627       0         0    \n14 E1    Don't talk a lot. \"rev\"          0                 0            0.564       0         0    \n15 E5    Take charge.      \"\"             0                 0            0.553       0         0    \n16 N1    Get angry easily. \"rev\"          0                 0            0           0.825     0    \n17 N2    Get irritated ea… \"rev\"          0                 0            0           0.803     0    \n18 N3    Have frequent mo… \"rev\"          0                 0            0           0.721     0    \n19 N4    Often feel blue.  \"rev\"          0                 0            0           0.573     0    \n20 N5    Panic easily.     \"rev\"          0                 0            0           0.503     0    \n21 O3    Carry the conver… \"\"             0                 0            0           0         0.724\n22 O1    Am full of ideas. \"\"             0                 0            0           0         0.564\n23 O5    Will not probe d… \"rev\"          0                 0            0           0         0.461\n24 O2    Avoid difficult … \"rev\"          0                 0            0           0         0.418\n25 O4    Spend time refle… \"\"             0                 0            0           0         0.233\n\n\n16. bind_rows(y = fit_bfi %&gt;% inspect(what = \"std.all\") %&gt;% .$psi %&gt;% unclass() %&gt;% (function(x) {\n    `[&lt;-`(x, lower.tri(x, diag = TRUE), NA)\n})() %&gt;% as_tibble(rownames = \"item_label\") %&gt;% mutate(items = c(\"I\", \"II\", \"III\", \"IV\", \"V\")), .id = \"data_from\")\n\n\n# A tibble: 30 × 9\n   data_from items item_label        is_rev Agreeableness Conscientiousness Extraversion Neuroticism\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1         A3    Know how to comf… \"\"             0.749             0            0           0    \n 2 1         A5    Make people feel… \"\"             0.687             0            0           0    \n 3 1         A2    Inquire about ot… \"\"             0.648             0            0           0    \n 4 1         A4    Love children.    \"\"             0.510             0            0           0    \n 5 1         A1    Am indifferent t… \"rev\"          0.344             0            0           0    \n 6 1         C4    Do things in a h… \"rev\"          0                 0.702        0           0    \n 7 1         C5    Waste my time.    \"rev\"          0                 0.620        0           0    \n 8 1         C2    Continue until e… \"\"             0                 0.592        0           0    \n 9 1         C1    Am exacting in m… \"\"             0                 0.551        0           0    \n10 1         C3    Do things accord… \"\"             0                 0.546        0           0    \n11 1         E4    Make friends eas… \"\"             0                 0            0.703       0    \n12 1         E2    Find it difficul… \"rev\"          0                 0            0.699       0    \n13 1         E3    Know how to capt… \"\"             0                 0            0.627       0    \n14 1         E1    Don't talk a lot. \"rev\"          0                 0            0.564       0    \n15 1         E5    Take charge.      \"\"             0                 0            0.553       0    \n16 1         N1    Get angry easily. \"rev\"          0                 0            0           0.825\n17 1         N2    Get irritated ea… \"rev\"          0                 0            0           0.803\n18 1         N3    Have frequent mo… \"rev\"          0                 0            0           0.721\n19 1         N4    Often feel blue.  \"rev\"          0                 0            0           0.573\n20 1         N5    Panic easily.     \"rev\"          0                 0            0           0.503\n21 1         O3    Carry the conver… \"\"             0                 0            0           0    \n22 1         O1    Am full of ideas. \"\"             0                 0            0           0    \n23 1         O5    Will not probe d… \"rev\"          0                 0            0           0    \n24 1         O2    Avoid difficult … \"rev\"          0                 0            0           0    \n25 1         O4    Spend time refle… \"\"             0                 0            0           0    \n26 2         I     Agreeableness      &lt;NA&gt;         NA                 0.334        0.683       0.223\n27 2         II    Conscientiousness  &lt;NA&gt;         NA                NA            0.357       0.283\n28 2         III   Extraversion       &lt;NA&gt;         NA                NA           NA           0.244\n29 2         IV    Neuroticism        &lt;NA&gt;         NA                NA           NA          NA    \n30 2         V     Opennness          &lt;NA&gt;         NA                NA           NA          NA    \n# ℹ 1 more variable: Opennness &lt;dbl&gt;\n\n\n# A tibble: 30 × 9\n# Groups:   data_from [2]\n   data_from items item_label        is_rev Agreeableness Conscientiousness Extraversion Neuroticism\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1 1         A3    Know how to comf… \"\"             0.749             0            0           0    \n 2 1         A5    Make people feel… \"\"             0.687             0            0           0    \n 3 1         A2    Inquire about ot… \"\"             0.648             0            0           0    \n 4 1         A4    Love children.    \"\"             0.510             0            0           0    \n 5 1         A1    Am indifferent t… \"rev\"          0.344             0            0           0    \n 6 1         C4    Do things in a h… \"rev\"          0                 0.702        0           0    \n 7 1         C5    Waste my time.    \"rev\"          0                 0.620        0           0    \n 8 1         C2    Continue until e… \"\"             0                 0.592        0           0    \n 9 1         C1    Am exacting in m… \"\"             0                 0.551        0           0    \n10 1         C3    Do things accord… \"\"             0                 0.546        0           0    \n11 1         E4    Make friends eas… \"\"             0                 0            0.703       0    \n12 1         E2    Find it difficul… \"rev\"          0                 0            0.699       0    \n13 1         E3    Know how to capt… \"\"             0                 0            0.627       0    \n14 1         E1    Don't talk a lot. \"rev\"          0                 0            0.564       0    \n15 1         E5    Take charge.      \"\"             0                 0            0.553       0    \n16 1         N1    Get angry easily. \"rev\"          0                 0            0           0.825\n17 1         N2    Get irritated ea… \"rev\"          0                 0            0           0.803\n18 1         N3    Have frequent mo… \"rev\"          0                 0            0           0.721\n19 1         N4    Often feel blue.  \"rev\"          0                 0            0           0.573\n20 1         N5    Panic easily.     \"rev\"          0                 0            0           0.503\n21 1         O3    Carry the conver… \"\"             0                 0            0           0    \n22 1         O1    Am full of ideas. \"\"             0                 0            0           0    \n23 1         O5    Will not probe d… \"rev\"          0                 0            0           0    \n24 1         O2    Avoid difficult … \"rev\"          0                 0            0           0    \n25 1         O4    Spend time refle… \"\"             0                 0            0           0    \n26 2         I     Agreeableness      &lt;NA&gt;         NA                 0.334        0.683       0.223\n27 2         II    Conscientiousness  &lt;NA&gt;         NA                NA            0.357       0.283\n28 2         III   Extraversion       &lt;NA&gt;         NA                NA           NA           0.244\n29 2         IV    Neuroticism        &lt;NA&gt;         NA                NA           NA          NA    \n30 2         V     Opennness          &lt;NA&gt;         NA                NA           NA          NA    \n# ℹ 1 more variable: Opennness &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20260118_CFA_table_in_one_pipeline/index.html#conclusion",
    "href": "posts/20260118_CFA_table_in_one_pipeline/index.html#conclusion",
    "title": "R 確認的因子分析の表を作ってみた、1パイプラインで",
    "section": "Conclusion",
    "text": "Conclusion\nデータとコードブックがあれば技術的には可能です。 そのうちちゃんとした因子分析のやり方のメモを残そうと思います。"
  },
  {
    "objectID": "posts/20260118_CFA_table_in_one_pipeline/index.html#session-infomation",
    "href": "posts/20260118_CFA_table_in_one_pipeline/index.html#session-infomation",
    "title": "R 確認的因子分析の表を作ってみた、1パイプラインで",
    "section": "Session Infomation",
    "text": "Session Infomation\n\n\n\n\n\n\nNotesessioninfo\n\n\n\n\n\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Japanese_Japan.utf8  LC_CTYPE=Japanese_Japan.utf8    LC_MONETARY=Japanese_Japan.utf8\n[4] LC_NUMERIC=C                    LC_TIME=Japanese_Japan.utf8    \n\ntime zone: Asia/Tokyo\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gt_1.2.0        psych_2.5.6     lavaan_0.6-19   lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n [7] dplyr_1.1.4     purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0    ggplot2_3.5.2  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6          sass_0.4.10         generics_0.1.4      xml2_1.4.0         \n [5] stringi_1.8.7       lattice_0.22-7      hms_1.1.3           digest_0.6.37      \n [9] magrittr_2.0.3      evaluate_1.0.5      grid_4.4.2          timechange_0.3.0   \n[13] RColorBrewer_1.1-3  fastmap_1.2.0       jsonlite_2.0.0      scales_1.4.0       \n[17] ViewPipeSteps_0.1.0 pbivnorm_0.6.0      textshaping_1.0.1   mnormt_2.1.1       \n[21] cli_3.6.5           rlang_1.1.6         litedown_0.7        commonmark_2.0.0   \n[25] base64enc_0.1-3     withr_3.0.2         yaml_2.3.10         tools_4.4.2        \n[29] parallel_4.4.2      tzdb_0.5.0          pacman_0.5.1        vctrs_0.6.5        \n[33] R6_2.6.1            stats4_4.4.2        lifecycle_1.0.4     fs_1.6.6           \n[37] htmlwidgets_1.6.4   ragg_1.4.0          pkgconfig_2.0.3     pillar_1.11.0      \n[41] gtable_0.3.6        glue_1.8.0          systemfonts_1.2.3   xfun_0.52          \n[45] tidyselect_1.2.1    rstudioapi_0.17.1   knitr_1.50          farver_2.1.2       \n[49] htmltools_0.5.8.1   nlme_3.1-168        rmarkdown_2.29      labeling_0.4.3     \n[53] compiler_4.4.2      quadprog_1.5-8      markdown_2.0"
  }
]